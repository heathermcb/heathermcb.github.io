[
  {
    "path": "posts/2021-12-17-james_webb_space_telescope/",
    "title": "What data science can learn from the James Webb Space Telescope",
    "description": "A talk at the Statistics and MachIne LEarning (SMILE) Journal Club.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nWhat is data science?\nAstronomical origins\nOn balance between data and science\nExamples of my work\nEffect of elections and changed prime ministers\nExplaining Why Text is Sexist or Racist with GPT-3\nReproducibility of COVID-19 pre-prints\n\nOpen questions\nHow do we write unit tests for data science?\nWhat happened to the revolution?\nHow do we think about power?\n\nThank you\nAcknowledgments\n\nThe slides are here.\nIntroduction\nHi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto in the Faculty of Information and the Department of Statistical Sciences. I’m also the assistant director of CANSSI Ontario, and I’d encourage you all to go to our website - https://canssiontario.utoronto.ca/ - and apply for funding from our programs.\nI’d like to thank Josh and Kartheik for the opportunity to talk today.\nI know that it’s an auspicious time for all you astronomy-inclined folks with the imminent launch of the James Webb Space Telescope, so I appreciate you coming to listen to me. Hopefully in a year, I can come back and you can tell me about all the wonderful datasets James Webb has provided all of you.\nToday I’d like to talk a little about what I see data science as, and why I think astronomers are so good at it; and then talk about a few examples of my work, focusing more on sharing my process and what I learnt, rather than the work itself; and finally close with some open questions.\nNone of what I’m about to say is cannon, this talk is more my way of trying to work out what I think, so I’d appreciate your reactions and comments.\nWhat is data science?\nWhen we think about data science, I think that we all have different things in mind.\nThe only thing that is certain, is that there is no agreed definition of data science, but a lot of people have tried. For instance, Wickham and Grolemund (2016) say it is ‘…an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.’ Similarly, Leek and Peng (2020) say it is ‘…the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.’ Benjamin S. Baumer and Horton (2021) say it is ‘…the science of extracting meaningful information from data.’ And Timbers, Campbell, and Lee (2021) say they define ‘data science as the process of generating insight from data through reproducible and auditable processes.’\nCraiu (2019), who is one of Josh’s bosses and also one of mine, argues that the lack of certainty as to what data science is, might not matter because ‘…who can really say what makes someone a poet or a scientist?’ He goes on to broadly say that a data scientist is ‘…someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.’\nRegardless of who is right, alongside those specific, more-technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. For instance, probability is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, data science can be defined as something like: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict.’\nThat may sound a touch cute, but Francis Edgeworth, the nineteenth century statistician and economist considered statistics to be the science ‘of those Means which are presented by social phenomena,’ so it is in good company (Edgeworth 1885).\nIn any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.\nData must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nIn my experience, this fact is ingrained into astronomers. My guess is that it’s because most of you get into astronomy being the actual data collectors - that is, you loved looking at the sky through telescopes when you were a kid. Eventually you got more sophisticated and layered on physics and math, but I think that astronomers make such great data scientists because you’re fundamentally lashed to the data collection process.\nMore broadly, I think that much of data science focuses on the ‘science,’ but it is important that we also focus on ‘data.’ And that is another feature of my cutesy definition of data science which I posited before. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that require the most time, that update most often, and that are worthy of our most full attention. Unfortunately, it’s not typically the type of thing that is professionally rewarded.\nAstronomical origins\nAt this point I’d like to look back, for a moment, at the origins of data science. As astronomers, I know that you all are used to looking back in time. My PhD is in economic history, so I’m also very keen on looking back in time also, although usually only decades or centuries, rather than the billions of years that you all tend to look back! Anyway, if we look at the history of statistics, we very quickly find ourselves in astronomy.\nFor instance, speaking about the development of least squares in the 1700s, Stigler (1986, 16) describes how it was associated with the following problems:\nDetermining the motion of the moon in a way that takes into account minor perturbations.\nReconciling the non periodic motion of Jupiter and Saturn.\nDetermining the shape of the earth.\nThe very stuff of astronomy!\nIn the work associated with least squares, we see statistical names that we still speak of today including Euler, Gauss, and Laplace. But it may be of interest to you all as astronomers that Stigler (1986) describes the papers at the time as ‘a story of statistical success (by a major astronomer, Mayer (Figure 1) and statistical failure (by a leading mathematician, Euler)!’\n\n\n\nFigure 1: Example of Mayer from Stigler, 1986\n\n\n\nThe fundamental issue at the time with least squares was that of hesitancy to combine different observations. These days, we just take this for granted and do it without thinking much about it. But if one steps back for a moment, one quickly sees why this was such a mental leap. And again, here we have astronomers, as you are so immersed in your data, as bringing a unique perspective.\nComparing Euler and Mayer, Stigler (1986, 28) says:\n\nThe two men brought absolutely first-rate intellects to bear on their respective problems, and both problems were in astronomy. Yet there was an essential conceptual difference in their approaches that made it impossible for Euler’…. The differences were these: Mayer approached his problem as a practical astronomer, dealing with observations that he himself had made under what he considered essentially similar observational conditions, despite the differing astronomical conditions. Euler, on the other hand, approached his problem as a mathematician, dealing with observations made by others over several centuries under unknown observational conditions. Mayer could regard errors, or variation, in his observations as random’… and he could take the step of aggregating equations without fear that bad observations would contaminate good ones’…. Euler could not bring himself to accept such a view with respect to his data. He ‘… [took] the mathematician’s view that errors actually increase with aggregation rather than taking the statistician’s view [and here Stigler should actually also add astronomers] that random errors tend to cancel one another.’…\n\nOn the other hand, Stigler (1986) criticism of Mayer is that ‘…He made no attempt to describe his calculation as a method that would be useful in other problems. He did not do what Legendre did, namely, abstract the method from the application where it first appeared.’ Namely astronomy!\nIn any case, by around 1800, least squares became more widely appreciated. But, oddly, not in my own area of interest—amely social sciences, where we would take another 70 or 80 years, a veritable lifetime, before we widely applied it.\nBefore we return to the present, I’d be remiss if I didn’t mention one final historical astronomer, Quetelet. In this talk I’m arguing that it is your foundation in data gathering that makes you such great data scientists. And this is going to be an example where that feature held back social sciences.\nQuetelet was a Belgium astronomer, but he also had social sciences interests and made foundational contributions in terms of the ‘average man’ and ‘fitting distributions.’ I’d like to touch on a circumstance where his being deeply involved in the data was a handicap.\nStigler (1986, 163) describes how by 1826 Quetelet had become involved in the statistical bureau, and they were planning for a census. As some of you who have taken courses with my wife, Monica Alexander, know, the foundation of demography is births, deaths, and migration. Quetelet argued that births and deaths had been taken care of, but migration had not. He proposed a ratio estimator based on counts in specific geographies—think provinces—which could then be scaled up to the whole country. There was various criticism of this plan, mostly focused on how one could possibly pick appropriate geographies, and Stigler describes how Quetelet then changed his mind.\nStigler (1986) continues\n\nHe [Quetelet] was acutely aware of the infinite number of factors that could affect the quantities he wished to measure, and he lacked the information that could tell him which were indeed important. He, like the generation of social scientists to follow, was reluctant to group together as homogenous, data that he had reason to believe was not’…. In some respects, Quetelet’s view of his social data was like Euler’s view of astronomical data. To be aware of a myriad of potentially important factors, without knowing which are truly important and how their effect may be felt, is often to fear the worst’…. He [Quetelet] could not bring himself to treat large regions as homogeneous, [and so] he could not think of a single rate as applying to a large area’… Astronomers had overcome a similar case of intellectual cold feet in the previous century by comparing observations with fact, by comparing prediction with realization. Success had bolstered confidence in the combination of observations. Social scientists were to require massive empirical data gathered under a wide variety of circumstances before they gained the astronomers’ confidence that the quantities they measured were of sufficient stability that the uncertainty of the estimates was itself susceptible to measurement.\n\nOn balance between data and science\nReturning to the present, as I’ve been saying, one of the reasons that I think astronomers make such great data scientists, is that you are used to knowing that your data is a biased sample that comes to you via instruments—in your case telescopes.\nWe always use various instruments to turn the world into data. In astronomy, the development of better telescopes, and eventually satellites and probes, and soon, hopefully, James Webb, enabled new understanding of other worlds. For instance, returning to history again for just a moment, Stigler (1986, p.25) describes how knowledge of that second problem that I mentioned—Jupiter/Saturn non-periodic motion—was ‘due to improved accuracy of astronomical observations.’ Returning to the present day, we similarly have new instruments for turning our own world into data being developed each day.\nIn the social sciences, a census was once a generational-defining event. And it’s appropriate that this talk happens near the Christmas holidays given the role that the census played in that event. But now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories to be told with data.\nThe telescope that we all hope is launching soon—James Webb—has a unique feature, compared with its predecessor in popular imagination - Hubble. And that feature is that it will be sent to L2. My understanding, and please correct me after the talk if I’m wrong, is that the defining feature of L2 is that the Earth’s gravity and the Sun’s gravity and the moon to a certain extent, effectively hold objects there without much in the way of rocket thrust being needed. So the side of the telescope with the instrumentation that needs to be kept cold and dark, can more easily always be kept cold and dark. And so, in this case James Webb can stay there for about 10 years, focusing on answering big questions, without much in the way of needing to worry about pointing the wrong way. We should all be so lucky!\nIn a way, for James Webb, being at L2 is all about getting the balance right so that it can focus on answering big questions. And I think for the past ten years or so, we’ve had the balance wrong in data science. There has been too much focus on the ‘science’ bit, without sufficient focus on the ‘data’ bit. And we all know what happens when you go too close to the sun.\nIt is not just the ‘science’ bit that is hard, it is the ‘data’ bit as well. I feel that Foster would have known this, and we are just reinventing things. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specialises in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. But this means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset, or in a question, as we are.\nI’m picking on CS a little here, but my home of the social sciences is just as bad and researchers like me are trying to refocus us on data a little more than we have been in the past.\nSo what are some examples of things that I work on?\nExamples of my work\nI mentioned that my PhD is in economic history, but I spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, with regular weekly meetings. It’s not a topic that traditionally would have been appropriate in economics, and I’m grateful they gave me the space because traditional economics is not for me, but data science is.\nEffect of elections and changed prime ministers\nOne result of this work was the paper—The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018—(Alexander and Alexander 2021) which was co-authored with my wife Monica. In Australia there is a written record of what is said in parliament called Hansard. We grabbed the text of everything said between 1901 and 2018 and then built a statistical model to look at whether the topics of discussion changed when there was an election or when there was a change in the prime minister. We found that changes in prime minister tend to be associated with topic changes even when the party in power does not change; and the effect of elections has been increasing since the 1980s, regardless of whether the election results in a change of prime minister.\nLesson: Always start small and then iterate.\nI came to that paper hopelessly naive and lost. I didn’t know that text analysis, and natural language processing more generally, was something that was incredibly popular. I didn’t have the skills that I needed, and really all I had was an all-consuming interest in the broad area—I didn’t even have a question. The dataset has over a billion words in it. So the main lesson for me was in dealing with big datasets. And here I don’t mean in terms of technical skills, I mean in terms of approach. These days when I have students and they want to use the Hansard or any large dataset, I start by insisting that they just use one month of data. After they analyse and write that up, then they can go to a year, and then they can continue to build up slowly.\nThe reason that I know now that’s the best way to go is that I somehow convinced myself that I couldn’t possibly answer any interesting questions until I had a century worth of data. And because I was teaching myself everything as I went, it took me more than a year to just get the data into a usable format.\nIf you take nothing away from this talk, please I beg you take this away: the most important, vital thing, is that you create a minimal viable product of any research. And that minimal viable product needs to be something that you can finish within a week. If you can’t do that then adjust the scope and the question until you can. Then you achieve that MVP and then you start to scale up to the question that you’re interested in, which is hopefully something that can be published.\nExplaining Why Text is Sexist or Racist with GPT-3\nAs anyone who has cared for young children knows, the response to almost any statement can be ‘why?’ One can quickly find oneself trying to explain the finer details of photosynthesis to a 3-year-old, and the extent to which one struggles with this tends to put in sharp relief the extent of one’s knowledge. Large language models such as OpenAI’s GPT-3 can generate text that is indistinguishable from that created by humans. They can also classify whether some given text is sexist or racist (Chiu and Alexander 2021).\nIn the paper—Explaining Why Text is Sexist or Racist with GPT-3—which was co-authored by Ke-Li Chiu we assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We prompt GPT-3 to generate explanations in a question-and-answer manner: ‘Is the following statement in quotes sexist? Answer yes or no and explain why.’ We then assess the adequacy of the explanations generated by GPT-3. We are interested in firstly, the extent to which it correctly classifies whether the statements are sexist/racist; and secondly, the reasonableness of the explanation that accompanies that classification.\nWe find that GPT-3 does poorly in the open-ended approach. When we add more structure to guide its responses the model performs better. But even when it correctly classifies racism or sexism, the accompanying explanations are often inaccurate. At times they even contradict the classification. On a technical level, we find a clear relationship between the hyper-parameter temperature and the number of correctly matched attributes, with substantial decreases as temperature increases.\nLesson: Always teach\nI just used a bunch of terms there and you probably assume that I know what they mean. In writing the paper that preceded this one, I actually fooled myself into thinking that I knew what they meant. It wasn’t until I tried to teach the material that I realised that I didn’t have a clue what was going on.\nA lot of my colleagues try to get out of teaching, and I can understand why they would think that, but I’ve found that having to teach has made me a better researcher. For one reason, I found that it’s too easy to fool yourself into thinking you know something until you have to explain it. And I also kind of think that a lot of us in academia need pressure, constraints, and a weekly structure, in order to do our best work.\nReproducibility of COVID-19 pre-prints\nThe final paper that I’d like to touch on is—Reproducibility of COVID-19 pre-prints—by Annie Collins and me (Collins and Alexander 2021). In that paper we are interested in the reproducibility of COVID-19 research. We create a dataset of pre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv. We conclude that there may be value in having authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, there is a need to better integrate open science training into a wide range of fields.\nLesson: You always need the code more than once\nI went into that paper thinking that I’d just quickly write some code to download some files and then be done with it. But then Annie wanted to broaden the scope of the paper, so the code needed to be re-written because she couldn’t understand what I’d done. And then we had to re-run the code because we wanted to update everything before she presented the paper. And that meant re-writing things. And then we to re-run the code before we submitted the paper. Again that meant re-writing things.\nWe eventually just turned the code into an R package, which is on CRAN as heapsofpapers (Alexander and Mahfouz 2021). Which is what I should have done from the start. The lesson that I learnt from this paper is that regardless of how certain you are that you’ll never use some particular code again, you’ll always need it.\nOpen questions\nHow do we write unit tests for data science?\nOne thing that working with real computer scientists has taught me is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it’s unlikely that it’s a character, and it’s unlikely that it’s an integer larger than 2500, and it’s unlikely that it’s a negative integer. We know all this, but writing unit tests has us write this all down.\nIn this case it’s obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they’re running well. The approach that I’ve taken is to add simulation—so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.\nWhat happened to the revolution?\nI don’t understand what happened to the promised machine learning revolution in social sciences. Specifically, I’m yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can’t happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.\nHow do we think about power?\nAs someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn’t anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I’ve had in other departments.\nI think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to ‘power’—it turns out that there were a bunch of instructions that no one bothered to check—they turned the oven on to some temperature without checking that it was 180C, and that’s fine because whatever mess came out was accepted because the people evaluating the cake didn’t know that they needed to check the temperature had been appropriately set. (I’m ditching this analogy right now).\nAs you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e. moving away from the recipe approach.\nAnd so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that’s how they are trained especially in social sciences like political science and economics, and that’s what is rewarded. But that’s not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one’s shirt.\nThank you\nRight. So I think I’ll leave it there. Thank you again to Josh and Kartheik for inviting me.\nLet’s cross our fingers for a successful launch of James Webb next week and its subsequent deployment. I do hope that you invite me back when you start getting data from it to analyse!\nThis is also my last talk for this year, so I’ll take this opportunity to say Merry Christmas, Happy Holidays, and I hope everyone has a wonderful start to 2022.\nI’d be happy to take any questions.\nAcknowledgments\nThanks very much to Josh for the invitation to speak, and to Monica Alexander for her suggestions and comments.\n\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” http://arxiv.org/abs/2111.09299.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. Heapsofpapers: Easily Download Heaps of PDF and CSV Files. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” http://arxiv.org/abs/2105.05241.\n\n\nBenjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton. 2021. Modern Data Science with r. 2nd ed. https://mdsr-book.github.io/mdsr2e/.\n\n\nChiu, Ke-Li, and Rohan Alexander. 2021. “Detecting Hate Speech with GPT-3.” http://arxiv.org/abs/2103.12407.\n\n\nCollins, Annie, and Rohan Alexander. 2021. “Reproducibility of COVID-19 Pre-Prints.” http://arxiv.org/abs/2107.10724.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.” Journal of the Statistical Society of London, 181–217.\n\n\nLeek, Jeff, and Roger D. Peng. 2020. Advanced Data Science 2020. http://jtleek.com/ads2020/index.html.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed.\n\n\nStigler, Stephen. 1986. The History of Statistics. Harvard University Press.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data Science: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz.\n\n\n\n\n",
    "preview": "posts/2021-12-17-james_webb_space_telescope/stigler.png",
    "last_modified": "2021-12-17T17:35:25-05:00",
    "input_file": {},
    "preview_width": 1044,
    "preview_height": 496
  },
  {
    "path": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/",
    "title": "On the privilege of turning our world into data",
    "description": "A talk at the 'Young Irish Statisticians' group.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nWhat is data science?\nWhat we can learn from a historical Dublin census?\nOn balance between data and science\nExamples of my work\nEffect of elections and changed prime ministers\nExplaining Why Text is Sexist or Racist with GPT-3\nReproducibility of COVID-19 pre-prints\n\nOpen questions\nHow do we write unit tests for data science?\nWhat happened to the revolution?\nHow do we think about power?\n\nThank you\nAcknowledgments\n\nThe slides are here.\nIntroduction\nHi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto in the Faculty of Information and the Department of Statistical Sciences. I’m also one of the co-leads of the University of Toronto Data Sciences Institute Thematic Program in Reproducibility. We’d love to develop ties with other institutions who are similarly interested in these issues, so please do get in touch.\nI’d like to thank Niamh Cahill for the opportunity to talk today. I always feel a little awed by everything that she’s able to accomplish and all her publications. In preparing for this talk I was looking for imminent historical Irish statisticians, and of course arguably one of the most imminent present-day Irish statisticians is Adrian Raftery, who had a PhD student, Leontine Alkema, who was Niamh’s postdoc advisor. And Niamh certainly lives up to her pedigree.\nI must admit that I feel a bit of a fraud talking today at this Young Irish Statisticians group. I’m no longer young, I’m certainly not Irish, and I’m not a statistician. But I do adore your country, and I spend a lot of time with statisticians, and I was once young. That said, I do hope that in a few years you can invite me back and we can do this in person, perhaps at the middle-aged Irish-fans pseudo-statisticians’ group!\nToday I’d like to talk a little about the origins of data science, especially mentioning the contributions of some Irish statisticians. I will turn to what I see data science as, and what I see are some of our roles and responsibilities as quantitatively interested folks. I will then go through various applications including: understanding the effect of elections; hate speech detection; and the reproducibility of COVID-19 pre-prints. While going through those applications I will focus more on what I learned and how I developed, rather than specific results. I will close with a few open issues.\nNone of what I’m about to say is cannon, this talk is more my way of trying to work out what I think, so I’d appreciate your reactions and comments.\nWhat is data science?\nWhen we think about data science, I think that we all have different things in mind.\nThe only thing that is certain, is that there is no agreed definition of data science, but a lot of people have tried. For instance, Wickham and Grolemund (2016) say it is ‘…an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.’ Similarly, Leek and Peng (2020) say it is ‘…the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.’ Benjamin S. Baumer and Horton (2021) say it is ‘…the science of extracting meaningful information from data.’ And Timbers, Campbell, and Lee (2021) say they define ‘data science as the process of generating insight from data through reproducible and auditable processes.’\nCraiu (2019) argues that the lack of certainty as to what data science is, might not matter because ‘…who can really say what makes someone a poet or a scientist?’ He goes on to broadly say that a data scientist is ‘…someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.’\nRegardless of who is right, alongside those specific, more-technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. For instance, probability is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, data science can be defined as something like: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict.’\nThat may sound a touch cute, but Francis Edgeworth, the Irish nineteenth century statistician and economist, who went to Trinity College Dublin, considered statistics to be the science ‘of those Means which are presented by social phenomena,’ so it is in good company (Edgeworth 1885).\nIn any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.\nData must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nMore broadly, I think that much of data science focuses on the ‘science,’ but it is important that we also focus on ‘data.’ And that is another feature of my cutesy definition of data science which I posited before. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that require the most time, that update most often, and that are worthy of our most full attention. Unfortunately, it’s not typically the type of thing that is professionally rewarded.\nWhat we can learn from a historical Dublin census?\nAt this point I’d like to look back, for a moment, at the origins of data science. If we look at the history of statistics, we very quickly find ourselves surrounded by Irish contributions.\nIn preparing for this talk I was thrilled to learn about the book by the Reverend James Whitelaw, and this is its actual title, ‘An essay on the population of Dublin. Being the result of an actual survey taken in 1798, with great care and precision, and arranged in a manner entirely new’ (Whitelaw 1905). I really do think that we all maybe need to adopt a little more of the Whitelaw’s detail in our paper titles!\nIn any case, Whitelaw (1905) says that he is fed up with the bad estimates of the size of the population in various capitals. And he particularly singled out London, which he says has estimates that range anywhere between 128,570 and 300,000. Instead, he says that he intends to make an accurate count of the number of inhabitants of Dublin.\nHe describes how:\n\nWhen I first entered on the business, I conceived that I should have little more to do than to transcribe carefully the list of inhabitants affixed to the door of each house by order of the Lord Mayor.\n\nLike many of us who spend our days in data, he found it was not that simple. Instead, he found:\n\n(t)he lists on the doors… presented generally to view a confused chaos of names, frequently illegible, and generally short of the actual number by a third, or even one-half’. He goes onto say that instead, he and his assistants braved ’the dread of infectious diseases,… filth, stench, and darkness,… to explore… every room of those wretched habitations, from the cellar to the garret, and on the spot ascertained their population.\n\nThe resulting tables looked something like this (Figure 1):\n\n\n\nFigure 1: Extract of the survey\n\n\n\nAnd in case you’re interested, his estimate of the total population of Dublin in 1798 was estimated at 182,370.\nI was surprised by how confident Whitelaw was in his numbers. My PhD is in economic history, where I was interested in Australian economic history, especially political history. And the thing about economic historians is that we have to create, clean, and prepare our own datasets from whatever traces we can find before we can analyse it. So that training focuses one’s mind on measurement, and sampling.\nAnd there’s an odd phenomenon that I’ve observed where the further one is away from decisions to do with the counting, measurement, organisation, and categorisation, the more one tends to trust the resulting datasets. That is, the person that actually does the hard work of constructing our datasets is usually the one that trusts them the least.\nI’ve never understood this confidence that people have in datasets that they don’t construct themselves.\nIt’s an absolute privilege to get to work in data science. To be able to feel, like Reverend Whitelaw did, that one is bringing some order to things. But it’s important to realise the perspective that one brings as one is doing that.\nNow my point here isn’t to attack Whitelaw, as he is long dead and incapable of defending himself, but, I skipped over a bunch of the actual quote in Whitelaw. In particular, after talking about that bit about just looking at the lists on the doors:\n\nAs the families of the middle and upper classes always contained some individual who was competent to the task, and as few had any motive to conceal or misrepresent, I found their lists, in general, extremely correct; but among the lower class, which forms the great mass of the population of this city, the case was very different…. This I at first imputed to design, but was afterwards convinced that it proceeded from ignorance and incapacity.\n\nBut how could he possibly know?\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could perfectly forecast a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex, world from which they were generated.\nThere are different approximations of ‘plausibly measurable.’ Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data.\nMuch of statistics is focused on considering, thoroughly, the data that we have. And that was appropriate for when our data were predominately agricultural, astronomical, or from the physical sciences. But with the rise of data science, mostly because of the value of its application to datasets generated by humans, we must also actively consider what is not in our dataset. Who is systematically missing from our dataset? Whose data does not fit nicely into the approach that we are using and are hence is being inappropriately simplified? If the process of the world becoming data necessarily involves abstraction and simplification, then we need to be clear about the points at which we can reasonably simplify, and those which would be inappropriate, recognising that this will be application specific.\nData science needs diversity. And it’s one reason that I’m excited by all the initiatives to increase participation by women and other underrepresented groups. We need our labs, our classrooms, and of course, eventually, our startups and businesses to be representative of the broader society.\nThe process of our world becoming data necessarily involves measurement. As I mentioned before, paradoxically, often those that do the measurement and are deeply immersed in the details have less trust in the data than those that are removed from it. Even seemingly clear tasks, such as measuring distance, defining boundaries, and counting populations, are surprisingly difficult in practice. Turning our world into data requires many decisions and imposes much error. Among many other considerations, we need to decide what will be measured, how accurately we will do this, and who will be doing the measurement.\nAn important example of how something seemingly simple quickly becomes difficult is maternal mortality. That refers to the number of women who die while pregnant, or soon after a termination, from a cause related to the pregnancy or its management (WHO 2019). It is difficult but critical to turn the tragedy of such a death into cause-specific data because that helps mitigate future deaths. Some countries have well-developed civil registration and vital statistics (CRVS). These collect data about every death. But many countries do not have a CRVS and so not every death is recorded. Even if a death is recorded, defining a cause of death may be difficult, especially when there is a lack of qualified medical personal or equipment. Maternal mortality is especially difficult because there are typically many causes. Some CRVS have a checkbox on the form to specify whether the death should be counted as maternal mortality. But even some developed countries have only recently adopted this. For instance, it was only introduced in the US in 2003, and even in 2015 Alabama, California, and West Virginia had not adopted the standard question (MacDorman and Declercq 2018).\nOn balance between data and science\nI mentioned at the start of the talk that I’m appointed in both the Faculty of Information and the Department of Statistical Sciences. For those who don’t know, faculties of information were created around the turn of the century to train librarians. Library science of course has a long and distinguished history of archiving, categorising, storing, and retrieving information, and for a long time when computers were first developed, a lot of the initial work was done by librarians, or at libraries. I suspect there is a gendered aspect to this, with librarians often being female, but a lot of that work was systematically overlooked and was re-invented later. This continues today, with our library at the University of Toronto doing excellent work that is consistently overlooked. I used to feel a bit funny about being jointly appointed, but it turns out that the first chair of the Department of Statistics at Trinity College Dublin, Gordon Foster, was not only a statistician, but he also created the International Book Numbering System (ISBN), which we still use today.\nIn 1968 Foster gave the Geary Lecture at the Economic and Social Research Institute in Dublin (Foster 1968). And I think it is worth revisiting that talk because he was clearly, yet another, Irish statistician who was well ahead of his time.\nFoster (1968) first describes how:\n\n[j]ust prior to the time when my own career in statistics was commencing, an important development took place in which I was therefore able to participate straight away, and which has since had a great effect on the development of the subject. I mean of course the invention of the computer.\n\nIt’s funny, but fifty years on, I actually feel the same way about my career!\nFoster (1968) goes on to say that\n\nit was something of an achievement to get a programme actually working. It was useful to have handy a screwdriver and scotch-tape, and to know where to kick the machine if it stuck.\n\nAnd as someone who gets the privilege to write code for living, I do wonder if much has changed!\nIt’s interesting that Foster (1968) points very clearly to data science in this talk, where he says:\n\n(s)tatistics are concerned with the processing and analysis of masses of data and with the development of mathematical methods of extracting information from data. Combine all this activity with computer methods and you have something more than the sum of its parts.\n\nTalking about computers at the time, Foster (1968) says:\n\n(t)hey are capable of performing any structured task, from planning a hospital diet, retrieving a legal precedent, or controlling stocks in a warehouse, to playing a reasonably good game of chess… I think, we begin to realise that the endeavour which I have referred to as information technology is no longer something just affecting specialists, but is bringing about changes in society affecting us all.\n\nHow easy it would be to change ‘information technology’ to ‘data science’ in those quotes for them to ring true almost a lifetime after they were spoken.\nWe always use various instruments to turn the world into data. For instance, in astronomy, the development of better telescopes, and eventually satellites and probes, enabled new understanding of other worlds. We similarly have new instruments for turning our own world into data being developed each day.\nIn the social sciences, a census was once a generational-defining event. And it’s appropriate that this talk happens near the Christmas holidays given the role that the census played in that event. But now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories to be told with data.\nI think for the past ten years or so, we’ve had the balance wrong in data science. There has been too much focus on the ‘science’ bit, without sufficient focus on the ‘data’ bit.\nIt is not just the ‘science’ bit that is hard, it is the ‘data’ bit as well. I feel that Foster would have known this, and we are just reinventing things. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specialises in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. But this means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset, or in a question, as we are.\nI’m picking on CS a little here, but my home of the social sciences is just as bad and researchers like me are trying to refocus us on data a little more than we have been in the past.\nSo what are some examples of things that I work on?\nExamples of my work\nI mentioned that my PhD is in economic history, but I spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, with regular weekly meetings. It’s not a topic that traditionally would have been appropriate in economics, and I’m grateful they gave me the space because traditional economics is not for me, but data science is.\nEffect of elections and changed prime ministers\nOne result of this work was the paper—The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018—(Alexander and Alexander 2021) which was co-authored with my wife Monica. In Australia there is a written record of what is said in parliament called Hansard. We grabbed the text of everything said between 1901 and 2018 and then built a statistical model to look at whether the topics of discussion changed when there was an election or when there was a change in the prime minister. We found that changes in prime minister tend to be associated with topic changes even when the party in power does not change; and the effect of elections has been increasing since the 1980s, regardless of whether the election results in a change of prime minister.\nLesson: Always start small and then iterate.\nI came to that paper hopelessly naive and lost. I didn’t know that text analysis, and natural language processing more generally, was something that was incredibly popular. I didn’t have the skills that I needed, and really all I had was an all-consuming interest in the broad area - I didn’t even have a question. The dataset has over a billion words in it. So the main lesson for me was in dealing with big datasets. And here I don’t mean in terms of technical skills, I mean in terms of approach. These days when I have students and they want to use the Hansard or any large dataset, I start by insisting that they just use one month of data. After they analyse and write that up, then they can go to a year, and then they can continue to build up slowly.\nThe reason that I know now that’s the best way to go is that I somehow convinced myself that I couldn’t possibly answer any interesting questions until I had a century worth of data. And because I was teaching myself everything as I went, it took me more than a year to just get the data into a usable format.\nIf you take nothing away from this talk, please I beg you take this away: the most important, vital thing, is that you create a minimal viable product of any research. And that minimal viable product needs to be something that you can finish within a week. If you can’t do that then adjust the scope and the question until you can. Then you achieve that MVP and then you start to scale up to the question that you’re interested in, which is hopefully something that can be published.\nExplaining Why Text is Sexist or Racist with GPT-3\nAs anyone who has cared for young children knows, the response to almost any statement can be ‘why?’ One can quickly find oneself trying to explain the finer details of photosynthesis to a 3-year-old, and the extent to which one struggles with this tends to put in sharp relief the extent of one’s knowledge. Large language models such as OpenAI’s GPT-3 can generate text that is indistinguishable from that created by humans. They can also classify whether some given text is sexist or racist (Chiu and Alexander 2021).\nIn the paper—Explaining Why Text is Sexist or Racist with GPT-3—which was co-authored by Ke-Li Chiu we assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We prompt GPT-3 to generate explanations in a question-and-answer manner: ‘Is the following statement in quotes sexist? Answer yes or no and explain why.’ We then assess the adequacy of the explanations generated by GPT-3. We are interested in firstly, the extent to which it correctly classifies whether the statements are sexist/racist; and secondly, the reasonableness of the explanation that accompanies that classification.\nWe find that GPT-3 does poorly in the open-ended approach. When we add more structure to guide its responses the model performs better. But even when it correctly classifies racism or sexism, the accompanying explanations are often inaccurate. At times they even contradict the classification. On a technical level, we find a clear relationship between the hyper-parameter temperature and the number of correctly matched attributes, with substantial decreases as temperature increases.\nLesson: Always teach\nI just used a bunch of terms there and you probably assume that I know what they mean. In writing the paper that preceded this one, I actually fooled myself into thinking that I knew what they meant. It wasn’t until I tried to teach the material that I realised that I didn’t have a clue what was going on.\nA lot of my colleagues try to get out of teaching, and I can understand why they would think that, but I’ve found that having to teach has made me a better researcher. For one reason, I found that it’s too easy to fool yourself into thinking you know something until you have to explain it. And I also kind of think that a lot of us in academia need pressure, constraints, and a weekly structure, in order to do our best work.\nReproducibility of COVID-19 pre-prints\nThe final paper that I’d like to touch on is—Reproducibility of COVID-19 pre-prints—by Annie Collins and me (Collins and Alexander 2021). In that paper we are interested in the reproducibility of COVID-19 research. We create a dataset of pre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv. We conclude that there may be value in having authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, there is a need to better integrate open science training into a wide range of fields.\nLesson: You always need the code more than once\nI went into that paper thinking that I’d just quickly write some code to download some files and then be done with it. But then Annie wanted to broaden the scope of the paper, so the code needed to be re-written because she couldn’t understand what I’d done. And then we had to re-run the code because we wanted to update everything before she presented the paper. And that meant re-writing things. And then we to re-run the code before we submitted the paper. Again that meant re-writing things.\nWe eventually just turned the code into an R package, which is on CRAN as heapsofpapers (Alexander and Mahfouz 2021). Which is what I should have done from the start. The lesson that I learnt from this paper is that regardless of how certain you are that you’ll never use some particular code again, you’ll always need it.\nOpen questions\nHow do we write unit tests for data science?\nOne thing that working with real computer scientists has taught me is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it’s unlikely that it’s a character, and it’s unlikely that it’s an integer larger than 2500, and it’s unlikely that it’s a negative integer. We know all this, but writing unit tests has us write this all down.\nIn this case it’s obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they’re running well. The approach that I’ve taken is to add simulation - so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.\nWhat happened to the revolution?\nI don’t understand what happened to the promised machine learning revolution in social sciences. Specifically, I’m yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can’t happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.\nHow do we think about power?\nAs someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn’t anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I’ve had in other departments.\nI think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to ‘power’—it turns out that there were a bunch of instructions that no one bothered to check—they turned the oven on to some temperature without checking that it was 180C, and that’s fine because whatever mess came out was accepted because the people evaluating the cake didn’t know that they needed to check the temperature had been appropriately set. (I’m ditching this analogy right now).\nAs you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e. moving away from the recipe approach.\nAnd so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that’s how they are trained especially in social sciences like political science and economics, and that’s what is rewarded. But that’s not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one’s shirt.\nThank you\nRight. So I think I’ll leave it there. Thank you again to Niamh for inviting me. And I hope that I haven’t bored you all too much.Merry Christmas, happy holidays, and I hope everyone has a wonderful start to 2022.\nI’d be happy to take any questions.\nAcknowledgments\nThanks very much to Niamh Cahill for the invitation to speak, and to Monica Alexander for her suggestions and comments.\n\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” http://arxiv.org/abs/2111.09299.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. Heapsofpapers: Easily Download Heaps of PDF and CSV Files. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” http://arxiv.org/abs/2105.05241.\n\n\nBenjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton. 2021. Modern Data Science with r. 2nd ed. https://mdsr-book.github.io/mdsr2e/.\n\n\nChiu, Ke-Li, and Rohan Alexander. 2021. “Detecting Hate Speech with GPT-3.” http://arxiv.org/abs/2103.12407.\n\n\nCollins, Annie, and Rohan Alexander. 2021. “Reproducibility of COVID-19 Pre-Prints.” http://arxiv.org/abs/2107.10724.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.” Journal of the Statistical Society of London, 181–217.\n\n\nFoster, Gordon. 1968. “Computers, Statistics and Planning: Systems or Chaos?” Geary Lecture. https://www.esri.ie/system/files/media/file-uploads/2016-03/GLS2.pdf.\n\n\nLeek, Jeff, and Roger D. Peng. 2020. Advanced Data Science 2020. http://jtleek.com/ads2020/index.html.\n\n\nMacDorman, Marian F, and Eugene Declercq. 2018. “The Failure of United States Maternal Mortality Reporting and Its Impact on Women’s Lives.” Birth (Berkeley, Calif.) 45 (2): 105.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data Science: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/.\n\n\nWhitelaw, James. 1905. An Essay on the Population of Dublin. Being the Result of an Actual Survey Taken in 1798, with Great Care and Precision, and Arranged in a Manner Entirely New. Graisberry; Campbell.\n\n\nWHO. 2019. “Trends in Maternal Mortality 2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population Division.” https://www.who.int/reproductivehealth/publications/maternal-mortality-2000-2017/en/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz.\n\n\n\n\n",
    "preview": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/extract.png",
    "last_modified": "2021-12-16T10:56:17-05:00",
    "input_file": {},
    "preview_width": 2108,
    "preview_height": 1052
  },
  {
    "path": "posts/2021-12-02-average_age_of_australian_politicians/",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "description": "I examine the average age of politicians in the Australian Federal Parliament on a daily basis. Using a publicly available dataset I find that generally the Senate is older than the House of Representatives. The average age increased from Federation in 1901 through to 1949, when an expansion of the parliament's size likely brought many new politicians. I am unable to explain a sustained decline that occurred during the 1970s. From the 1980s onward there has been a gradual aging of both houses.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-12-02",
    "categories": [],
    "contents": "\n\nContents\nOverview\nData preparation\nResults\nOver time\nGroup by election period\nCompare with average age\n\nConcluding remarks\nAcknowledgments\n\nOverview\nLast week I had the opportunity to attend my first thesis defence at a French-language institution (just to clarify - the actual defence was nonetheless conducted in English). Congratulations to Florence Vallée-Dubois on a successful defence. Her thesis looks at population ageing and democratic representation in Canada and is absolutely brilliant.\nHer work, and some earlier comments by Ben Readshaw, got me thinking about what the average age of a politician in Australia looks like. Thankfully, this is an example of a question that my new R package with Paul Hodgetts AustralianPoliticians makes easy to answer (Alexander and Hodgetts 2021).\nThe average age of politicians in a parliament may have implications for the types of issues that are emphasised and the policies that are put in place. I examine the average age of Australian politicians in the Senate and the House of Representatives. In general, the Senate tends to be slightly older than the House of Representatives. I find a gradual increase in the average age from Federation through to the 1940s. In 1949 there was an expansion of the parliament and the average age noticeably declined. After a period of relative stability in the 1950s and 1960s, there was another noticeable decrease in the 1970s, followed by a gradual aging.\nData preparation\nThe dataset that I used in this note was primarily collected from records from the Australian Parliamentary Library, Wikipedia, and the Australian Dictionary of Biography. The dataset comprises biographical, political, and other information about every person elected to either the House of Representatives or the Senate. You can get the dataset in a series of CSVs on GitHub or as an R Package AustralianPoliticians available on CRAN.1\n\n\n# install.packages(\"AustralianPoliticians\")\nlibrary(AustralianPoliticians)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n\n\nThe other packages that are required for the analysis are lubridate (Grolemund and Wickham 2011), the tidyverse (Wickham et al. 2019). Additionally this note draws on the knitr (Xie 2020) and distill (Allaire et al. 2021) packages. All analysis is conducted in the statistical programming language R (R Core Team 2020).\nI’m not going to spend a lot of time on descriptives about the dataset here because that’s needed for a paper, and this is meant to be fun.\n\n\nall <- AustralianPoliticians::get_auspol(\"all\")\nhead(all)\n\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames      firstName commonName\n  <chr>      <chr>   <chr>              <chr>     <chr>     \n1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n3 Abbott1877 Abbott  Macartney          Macartney Mac       \n4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n# … with 15 more variables: displayName <chr>,\n#   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#   deathDate <date>, member <dbl>, senator <dbl>,\n#   wasPrimeMinister <dbl>, wikidataID <chr>,\n#   wikipedia <chr>, adb <chr>, comments <chr>\n\nall$member %>% table()\n\n\n.\n   0    1 \n 578 1205 \n\nall$senator %>% table()\n\n\n.\n   0    1 \n1155  628 \n\nBut briefly, there are 1,783 politicians in the dataset, with 1,205 having sat in the House of Representatives, which is the lower house, and 628 having sat in the Senate, which is the upper house. There is some overlap because there are people who sat in both houses.2\nThe AustralianPoliticians package contains a number of datasets that are related by the uniqueID variable. This note requires the main dataset - ‘all’ - as well as two supporting datasets that provide more detailed information about the members - ‘mps’ - and the senators - ‘senators.’\n\n\n# Get the members and the dates they were in the house\nmps <- AustralianPoliticians::get_auspol(\"mps\")\n\naustralian_mps <- \n  all %>% \n  filter(member == 1) %>% \n  left_join(mps, \n            by = \"uniqueID\") %>% \n  select(uniqueID, mpFrom, mpTo) %>% \n  mutate(house = \"reps\") %>% \n  rename(from = mpFrom,\n         to = mpTo)\n\n\n\n\n\n# Get the senators and the dates they were in the senate\nsenators <- AustralianPoliticians::get_auspol(\"senators\")\n\naustralian_senators <- \n  all %>% \n  filter(senator == 1) %>% \n  left_join(senators, \n            by = \"uniqueID\") %>% \n  select(uniqueID, senatorFrom, senatorTo) %>% \n  mutate(house = \"senate\") %>% \n  rename(from = senatorFrom,\n         to = senatorTo)\n\naustralian_politicians <- rbind(australian_mps, australian_senators)\nrm(australian_senators, australian_mps, mps, senators)\n\n# Change the names so that they print nicely in graphs/tables\naustralian_politicians <- \n  australian_politicians %>% \n  mutate(house =\n           case_when(\n             house == \"senate\" ~ \"Senate\",\n             house == \"reps\" ~ \"HoR\",\n             TRUE ~ \"OH NO\")\n         )\nhead(australian_politicians)\n\n\n# A tibble: 6 × 4\n  uniqueID   from       to         house\n  <chr>      <date>     <date>     <chr>\n1 Abbott1869 1913-05-31 1919-11-03 HoR  \n2 Abbott1886 1925-11-14 1929-10-12 HoR  \n3 Abbott1886 1931-12-19 1937-03-28 HoR  \n4 Abbott1891 1940-09-21 1949-10-31 HoR  \n5 Abbott1957 1994-03-26 2019-05-18 HoR  \n6 Abel1939   1975-12-13 1977-11-10 HoR  \n\n# australian_politicians$house %>% table()\n\n\n\nFor each day, I would like to know the average age of everyone sitting in the parliament. There are a variety of ways to do this, but one is to create a dataset of two columns: date and uniqueID. Both of these are repeated, so that for every date there is every uniqueID.\n\n\nstart_date <- ymd(\"1901-01-01\")\nend_date <- ymd(\"2021-12-31\")\n\npoliticians_by_date <- \n  tibble(\n    uniqueID = rep(australian_politicians$uniqueID %>% unique(),\n                   end_date - start_date + 1),\n    date = rep(seq.Date(start_date, end_date, by = \"day\"),\n               australian_politicians$uniqueID %>% unique() %>% length()\n               )\n    )\n\nhead(politicians_by_date)\n\n\n# A tibble: 6 × 2\n  uniqueID   date      \n  <chr>      <date>    \n1 Abbott1869 1901-01-01\n2 Abbott1886 1901-01-02\n3 Abbott1891 1901-01-03\n4 Abbott1957 1901-01-04\n5 Abel1939   1901-01-05\n6 Adams1951  1901-01-06\n\nAlthough the dataset is long at this point, it will be quite sparse as there are many combinations of date and uniqueID that are irrelevant. In the next step I check if each uniqueID was in parliament on each date, and filter away those that were not.\n\n\n# Add an explicit end date to the uniqueIDs that are still in parliament and\n# hence have NA in the date they left parliament.\naustralian_politicians$to[is.na(australian_politicians$to)] <- end_date\n\npoliticians_by_date <- \n  politicians_by_date %>% \n  left_join(australian_politicians,\n            by = \"uniqueID\"\n            )\n\npoliticians_by_date <- \n  politicians_by_date %>% \n  mutate(in_parliament_interval = interval(from, to),\n         in_parliament_at_date = if_else(date %within% in_parliament_interval, \n                                         1, \n                                         0)\n         ) %>% \n  filter(in_parliament_at_date == 1) %>% \n  select(-in_parliament_interval,\n         -in_parliament_at_date,\n         -from,\n         -to)\n\nhead(politicians_by_date)\n\n\n# A tibble: 6 × 3\n  uniqueID     date       house\n  <chr>        <date>     <chr>\n1 Bonython1848 1901-04-13 HoR  \n2 Braddon1829  1901-04-25 HoR  \n3 Brown1861    1901-05-11 HoR  \n4 Cameron1851  1901-06-13 HoR  \n5 Chanter1845  1901-07-13 HoR  \n6 Chapman1864  1901-07-14 HoR  \n\nNow that the dataset is a bit more tractable, I add the birthday of every uniqueID and then calculate their age, in days, for every date.\n\n\npoliticians_by_house_and_birthday <- \n  all %>% \n  select(uniqueID, birthDate, member, senator) %>% \n  pivot_longer(cols = c(member, senator), \n               names_to = \"house\", \n               values_to = \"in_it\"\n               ) %>% \n  filter(in_it == 1) %>% \n  select(-in_it) %>% \n  mutate(house = \n           case_when(\n             house == \"senator\" ~ \"Senate\",\n             house == \"member\" ~ \"HoR\",\n             TRUE ~ \"OH NO\")\n         )\n\n# Check if the catch-all has been invoked\n# politicians_by_house_and_birthday[politicians_by_house_and_birthday$house == \"OH NO\",]\n\npoliticians_by_date <- \n  politicians_by_date %>% \n  left_join(politicians_by_house_and_birthday,\n            by = c(\"uniqueID\", \"house\")\n            )\n\npoliticians_by_date <- \n  politicians_by_date %>% \n  filter(!is.na(birthDate)) %>% # I can't find Trish Wortley's birthday and also\n  # we don't know the birthdates of some of the politicians from around Federation.\n  mutate(age_as_at_that_date = date - birthDate)\n\nhead(politicians_by_date)\n\n\n# A tibble: 6 × 5\n  uniqueID     date       house birthDate  age_as_at_that_d…\n  <chr>        <date>     <chr> <date>     <drtn>           \n1 Bonython1848 1901-04-13 HoR   1848-10-15 19172 days       \n2 Braddon1829  1901-04-25 HoR   1829-06-11 26250 days       \n3 Brown1861    1901-05-11 HoR   1861-10-06 14461 days       \n4 Cameron1851  1901-06-13 HoR   1851-11-03 18119 days       \n5 Chanter1845  1901-07-13 HoR   1845-02-11 20605 days       \n6 Chapman1864  1901-07-14 HoR   1864-07-10 13517 days       \n\nI need to work out the average age for each day. There are some politicians for whom we do not know their exact birthdate. Those have been removed in this calculation.\n\n\naverage_age_by_date <- \n  politicians_by_date %>%\n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %>% # This just converts it into a days count\n  group_by(date, house) %>% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE)) \n\n\naverage_age_by_date <- \n  average_age_by_date %>% \n  mutate(average_age_in_years = average_age/365) \n\nhead(average_age_by_date)\n\n\n# A tibble: 6 × 4\n# Groups:   date [3]\n  date       house  average_age average_age_in_years\n  <date>     <chr>        <dbl>                <dbl>\n1 1901-03-29 HoR         16765                  45.9\n2 1901-03-29 Senate      16086.                 44.1\n3 1901-03-30 HoR         17235                  47.2\n4 1901-03-30 Senate      18878.                 51.7\n5 1901-03-31 HoR         17236                  47.2\n6 1901-03-31 Senate      18878.                 51.7\n\nI’ll do the same thing except to work out the average by election period. This requires grabbing the election periods (there’s an R package coming soon about this)!\n\n\nelections <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/australian_federal_elections/master/outputs/elections.csv\",\n                      col_types = \"Dicc\")\n\naverage_age_by_election <- \n  politicians_by_date %>%\n  left_join(elections %>% \n              rename(date = electionDate) %>% \n              select(-comment) %>% \n              mutate(house = \"HoR\"),\n            by = c(\"date\", \"house\")) %>% \n  ungroup() %>% \n  filter(house == \"HoR\") %>% \n  arrange(date, uniqueID) %>% \n  mutate(is_election = if_else(is.na(electionWinner), 0, 1),\n         is_election = if_else(is_election == lag(is_election, default = 0), 0, is_election),\n         election_counter = cumsum(is_election))\n\naverage_age_by_election <- \n  average_age_by_election %>%\n  ungroup() %>% \n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %>% # This just \n  # converts it into a days count\n  group_by(election_counter) %>% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE),\n            first_date = min(date)) %>% \n  mutate(average_age_in_years = average_age/365) %>% \n  ungroup() \n\n\n\nResults\nOver time\nThere are considerable changes over time (Figure 1).\n\n\naverage_age_by_date %>% \n  # filter(year(date) == 1904) %>%\n  # filter(month(date) == 2) %>%\n  # filter(house == \"HoR\") %>%\n  ggplot(aes(x = date, y = average_age_in_years, colour = house)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Date\",\n       y = \"Average age (years)\",\n       color = \"House\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 1: Average age in the Australian Federal Parliament on a daily basis\n\n\n\nI’ll just separate each of the houses because there’s a lot going on there (Figures 2 and 3).\n\n\naverage_age_by_date %>% \n  filter(house == \"HoR\") %>% \n  ggplot(aes(x = date, y = average_age_in_years)) +\n  geom_point(alpha = 0.5, color = \"#F90000\") +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 2: Average age in the lower house on a daily basis\n\n\n\n\n\naverage_age_by_date %>% \n  filter(house == \"Senate\") %>% \n  ggplot(aes(x = date, y = average_age_in_years)) +\n  geom_point(alpha = 0.5, color = \"#0080BD\") +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 3: Average age in the upper house on a daily basis\n\n\n\nGroup by election period\nWe can also look at a summary of the results, averaged across days, on the basis of each election period (Table 1 and Figure 4).\n\n\naverage_age_by_election %>% \n  select(-average_age) %>% \n  rename(`Election number` = election_counter,\n         `Period begins` = first_date,\n         `Average age (years)` = average_age_in_years) %>% \n  kable(caption = \"Average age by for each period between lower house elections\",\n        digits = 1,\n        booktabs = TRUE)\n\n\nTable 1: Average age by for each period between lower house elections\nElection number\nPeriod begins\nAverage age (years)\n1\n1901-03-29\n48.2\n2\n1903-12-16\n48.8\n3\n1906-12-12\n49.0\n4\n1910-04-13\n50.1\n5\n1913-05-31\n49.8\n6\n1914-09-05\n51.9\n7\n1917-05-05\n53.0\n8\n1919-12-13\n52.2\n9\n1922-12-16\n51.7\n10\n1925-11-14\n52.3\n11\n1928-11-17\n50.9\n12\n1929-10-12\n49.4\n13\n1931-12-19\n52.3\n14\n1934-09-15\n51.1\n15\n1937-10-23\n51.9\n16\n1940-09-21\n52.2\n17\n1943-08-21\n51.6\n18\n1946-09-28\n53.3\n19\n1949-12-10\n50.9\n20\n1951-04-28\n52.4\n21\n1954-05-29\n53.7\n22\n1955-12-10\n52.4\n23\n1958-11-22\n52.5\n24\n1961-12-09\n53.5\n25\n1963-11-30\n53.5\n26\n1966-11-26\n52.5\n27\n1969-10-25\n51.3\n28\n1972-12-02\n49.5\n29\n1974-05-18\n48.6\n30\n1975-12-13\n47.1\n31\n1977-12-10\n47.3\n32\n1980-10-18\n48.3\n33\n1983-03-05\n47.7\n34\n1984-12-01\n48.2\n35\n1987-07-11\n49.4\n36\n1990-03-24\n48.5\n37\n1993-03-13\n48.9\n38\n1996-03-02\n48.9\n39\n1998-10-03\n49.7\n40\n2001-11-10\n50.7\n41\n2004-10-09\n51.7\n42\n2007-11-24\n51.2\n43\n2010-08-21\n51.5\n44\n2013-09-07\n50.0\n45\n2016-07-02\n50.2\n46\n2019-05-18\n51.7\n\n\n\naverage_age_by_election %>% \n  ggplot(aes(x = first_date, y = average_age_in_years)) +\n  geom_point() +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 4: Average age in the Australian Federal Parliament, by election period\n\n\n\nCompare with average age\nTo get a sense of whether the parliament is unusual we need know what is happening the broader population over this time. Fortunately, I know someone who is fairly handy when it comes to demography who can help.3\nAlthough it’s going to average over elections, which we’ve seen is a big source of variation, we can create an average age for each year, and then compare that against data from the Australian Bureau of Statistics (ABS). The ABS provides an estimate of historical population statistics in 3105.0.65.001 - Australian Historical Population Statistics, 2016, which was released in 2019. I want the second data cube - 2 - Population Age and Sex Structure - and within that I want Table 2.18 - Median age by sex, states and territories, 30 June, 1861 onwards.\n\n\nlibrary(readxl)\nlibrary(janitor)\nABS_data <- read_excel(\"3105065001ds0002_2019.xls\", \n                       sheet = \"Table 2.18\",\n                       skip = 4) %>%\n  janitor::clean_names() %>% \n  rename(area_and_type = x1,) %>% \n  select(-x1861, -x1870, -x1871, -x1881, -x1891)\n\nABS_data <- \n  ABS_data %>% \n  mutate(type = ifelse(area_and_type %in% c(\"Males\", \"Females\", \"Persons\"), area_and_type, NA),\n         area_and_type = ifelse(is.na(type), area_and_type, NA)) %>% \n  select(area_and_type, type, everything()) %>% \n  fill(area_and_type, .direction = \"down\") %>% \n  mutate(area_and_type = str_remove(area_and_type, \"\\\\(f\\\\)\\\\(g\\\\)\"))\n\nABS_data <- \n  ABS_data %>%  \n  filter(area_and_type == \"Australia\",\n         type == \"Persons\")\n\nABS_data <- \n  ABS_data %>%\n  pivot_longer(cols = starts_with(\"x\"),\n               names_to = \"year\",\n               values_to = \"median\") %>% \n  mutate(year = str_remove(year, \"x\"),\n         year = as.integer(year)) %>%\n  rename(area = area_and_type)\n\n\n\n\n\naverage_age_by_year <- \n  politicians_by_date %>%\n  ungroup() %>% \n  mutate(year = year(date)) %>% \n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %>% # This just \n  # converts it into a days count\n  group_by(year) %>% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE)) %>% \n  mutate(average_age_in_years = average_age/365) %>% \n  ungroup() \n\n\n\n\n\naverage_age_by_year %>% \n  left_join(ABS_data, by = \"year\") %>% \n  select(year, average_age_in_years, median) %>% \n  rename(Politicians = average_age_in_years,\n         Overall = median) %>% \n  pivot_longer(cols = c(Politicians, Overall), \n               names_to = \"population\",\n               values_to = \"median\") %>% \n  ggplot(aes(x = year, y = median, color = population)) +\n  geom_point() +\n  # geom_smooth() +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(x = \"Year\",\n         y = \"Median age (years)\",\n         color = \"Population\")\n\n\n\n\nConcluding remarks\nI’ve had a brief look at the average age of politicians in the Australian Parliament. In general, I’ve found that they’re older than the general population, and that there’s a lot of variation.\nMore generally, I’ve shown one of the advantages of putting data onto GitHub, and wrapping that in an R Package where possible. If you don’t like my analysis then you can grab the data yourself and play around with it!\nAs I mentioned at the top, Florence Vallée-Dubois does a lot more of this type of work in her thesis ‘Growing old : Population ageing and democratic representation in Canada’ where she looks at ‘whether the social changes brought about by population ageing also have implications for electoral politics and democratic representation.’ There’s probably a lot of similar work that could be done for Australia.\nAcknowledgments\nThanks very much to Ben Readshaw and Florence Vallée-Dubois for motivating this note, and to Monica Alexander for her thoughtful comments.\n\n\n\nAlexander, Rohan, and Paul A. Hodgetts. 2021. AustralianPoliticians: Provides Datasets about Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAllaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2021. Distill: ’R Markdown’ Format for Scientific and Technical Writing. https://CRAN.R-project.org/package=distill.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2020. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nMonica Alexander uses an early version of this dataset to look at the average age of death of Australian politicians: https://www.monicaalexander.com/posts/2019-08-09-australian_politicians/.↩︎\nThere are a handful of politicians who sat in the parliament, but were never elected - when the first parliament sat, there were initially some who were just appointed, and never went on to seek election. Conversely, there is one politician - Heather Hill - who was elected, but whose election was reversed before she could take her seat. While she never served in either house, she is included in the dataset for purposes of being able to link it to the elections dataset. However she is not included in the number of Australian politicians.↩︎\nIf you’re interested in this sort of thing, then a PhD with Monica Alexander is probably the way to go.↩︎\n",
    "preview": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments_files/figure-html5/maingraph-1.png",
    "last_modified": "2021-12-03T11:23:15-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-30-review_of_timbers/",
    "title": "Review of 'Data Science: A First Introduction'",
    "description": "A brief review of 'Data Science: A First Introduction' by Tiffany-Anne Timbers, Trevor Campbell, and Melissa Lee.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-11-30",
    "categories": [],
    "contents": "\n\nContents\nAcknowledgments\n\n“Data Science: A First Introduction” by Tiffany-Anne Timbers, Trevor Campbell, and Melissa Lee from University of British Columbia’s Department of Statistics is the first-year data science textbook by which all others will be judged. It is a kind, yet rigorous, textbook that provides a foundation on which students can quickly answer exciting questions. One can imagine the authors saying to a new student ‘I’ve spent my life learning all this exciting stuff, and I can’t wait to share it with you’.\nThe book is effectively divided into three parts: the first four chapters focus on data; the next six chapters go through statistical methods; and the final three brief chapters introduce tools such as Jupyter, version control, and local installation.\nChapter 1 focuses on R and the tidyverse and is based around a dataset of languages spoken in Canada. It introduces key verbs, such as filter, select, arrange, and slice, as well as ggplot2. Chapter 2 covers how to get data into R; initially using CSV and TSV files, but then turning to SQL. This early focus on SQL (starting at p. 41) is a key feature of this book, and one that will pay dividends for students as they progress with subsequent courses. This chapter again uses the Canadian languages dataset. Chapter 3 focuses on cleaning and wrangling data within the tidy data framework. Key verbs including mutate, summarize, map, pivot_wider, and pivot_longer are introduced in this chapter, as is the base pipe operator |>. This example of the authors’ decision to use the latest innovation, is reflected throughout the book in many other choices. The chapter uses a dataset of Canadian city populations and the Canadian languages dataset. Chapter 4 focuses on data visualization with ggplot2, especially scatter plots, line plots, bar plots, histograms, and how to improve on the default plots. The chapter uses the Old Faithful eruptions dataset and the Michelson’s speed of light dataset. The pages in this chapter that tell students how to explain a visualization (pp. 168-169) are an absolute treat.\nChapters 5 through to 10 focus on statistical methods. Chapters 5 and 6 are focused on classification, firstly introducing K-nearest neighbors, and then using this to introduce tidymodels and data preprocessing, which Chapter 6 then builds on to focus on test and training datasets, evaluation, and tuning, including lovely discussions of pre-processing and cross-validation. These chapters use the breast cancer dataset. There is a very nice discussion of what it means to be ‘good’ when it comes to accuracy on p. 224. Chapter 7 introduces regression using K-nearest neighbors and discusses underfitting and overfitting. It uses a real estate dataset from Sacramento. Chapter 8 turns to classical linear regression including simple and multivariable. There is some discussion of multicollinearity and outliers as well as prediction. This chapter again uses the Sacramento real estate dataset. Chapter 9 focuses on clustering within the context of K-means, including thorough discussions of how it all works, and some of the limitations. It uses the Palmer Penguins dataset. Finally, Chapter 10 discusses statistical inference, and sampling, including a lovely treatment of bootstrapping. It uses a dataset from Airbnb.\nChapters 11 through to 13 focus on tools. Chapter 11 introduces Jupyter including execution and markdown. Chapter 12 introduces version control and GitHub, covering not just commit, push, and pull, but also dealing with GitHub PATs, cloning, and collaboration including merge conflicts. Finally, Chapter 13 briefly covers installing all this locally, on the assumption that the reader to this point has been able to use a cloud solution that was already set-up for them.\nTo a certain extent data science textbooks are playing catch-up: we’ve long had methods texts in discipline-specific areas, but data sciences today is largely following demand from research/industry to move these methods into other areas – what discipline at a university covers real estate pricing? Into that void, Timbers, Campbell, and Lee, is the standard by which all other introductory data science textbooks will be judged. It is a rare immediate addition to the pantheon of introductory data science textbooks released (or updated) in the past five years, taking its place alongside R4DS, ISLR, and Statistical Rethinking. It introduces data science in a way that specifies exactly what a student needs and anticipates many of their questions. Its content will represent table stakes in terms of what we expect of students after they take first-year classes.\nI made it 10 per cent of the way through Timbers et al before I learnt something new. Frankly I was surprised I made it so far. Data science pedagogy has been so disjoint and so many of us are self-taught that it is refreshing to have a class-room-tested textbook that is focused on workflows and reproducibility. The approaches are rigorous and opinionated, and the text is filled with kindness and warmth. It is the book that I wish I had when I first came to learn this material. The book is unashamedly focused on the newest innovations including tidymodels and the native pipe operator, and I soon found myself learning things, on average, at roughly one-thing-per-page, which was an exciting experience for someone who spends his days doing and teaching data science in R. This is a text that I can see myself coming back to regularly, not just in my teaching, but as a reference. I am hopeful that the authors will go on to write “Data Science”, and “Advanced Data Science”, without too much delay!\nI mentioned kindness and warmth earlier, but it permeates the book. And there are many sections that proactively address questions that new students often have. For instance, p. 11 when a dataset is assigned to a name, the authors acknowledge how perplexing it can be to a student that nothing happens. Pleasingly the authors place reproducibility front-and-center of data science. For instance, the use of seeds is emphasised throughout the text, as is the importance of code that others can run.\nThere is extensive use of ‘Notes’ to separate more reflective content. One can imagine many of these are the result of the countless hours the authors have spent teaching this material. More generally, all aspects are clearly explained and built-up slowly. Chapter 3, which focuses on tidy data, is a particularly strong example of this. The text places potential research questions throughout, which may help retain student interest throughout.\nWhile it’s clear that I think the book is great, there are a few areas there I wish they had expanded on a little. For instance, on p. 3 Timbers et al say ‘when you work with data, it is essential to think about how the data were collected, which affects the conclusions you can draw. If your data are biased, then you results will be biased.’ But there is not much coverage of this throughout the book. I wonder if subsequent editions of this book could consider more explicit coverage of ethics especially around data collection, following say, D’Ignazio and Klein Data Feminism, and also around applications of these methods, following say, O’Neil Weapons of Math Destruction? Similarly, it may make sense to introduce sampling concepts alongside data collection.\nA free version of the textbook is available here: https://ubc-dsci.github.io/introduction-to-datascience/ and it is forthcoming from CRC Press. When it’s available I’d recommend that everyone buy it and assign it in their classes.\nAcknowledgments\nI was provided with a PDF version of the book for the purposes of writing a blurb. There are no other financial disclosures. I printed and bound it at my own expense. At my invitation Timbers has previously spoken at an event—Toronto Workshop on Reproducibility—that I organized earlier this year.\n\n\n\n",
    "preview": "posts/2021-11-30-review_of_timbers/IMG_9803.png",
    "last_modified": "2021-12-02T09:09:26-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 1280
  },
  {
    "path": "posts/2021-10-19-trinity_remarks/",
    "title": "Remarks at Trinity College",
    "description": "Introductory remarks delivered at Trinity College High Table panel on data science at the University of Toronto on 19 October 2021.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nWhat I do in data science\nSome steps to get started in data science\nConcluding remarks\n\nThe slides are here.\nIntroduction\nHi, my name is Rohan Alexander. I’m an assistant professor in the Faculty of Information and the Department of Statistical Sciences at the University of Toronto. I’d like to thank Academic Don, Statistics, Manmeet Sangha for the invitation to talk today at Trinity College.\nWhen I asked the undergraduate students that I work with who was associated with ‘Trin’, it was no surprise to me that the college was well represented among the strongest ones. ‘Trinnies’ have a well-deserved reputation for academic excellence and the number of people that have shown up for this event speaks to how that tradition continues even under trying circumstances.\nI’ve only got 5 minutes, so I’d like to touch on just two aspects:\nAn example of what I do in data science.\nSome steps to get started in data science.\nWhat I do in data science\nThere is no agreed definition of data science, but an informal one is: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict’.\nI like that definition because it does not treat data as terra nullius, or nobody’s land. Data must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself.\nA lot of what my research does is try to understand the specific nuances of specific datasets, especially in terms of who is included in them, and who is not, and understand how this affects inference.\nOne example of this is natural language processing. There we use really big models, built on a lot of data, to generate text. To make sure everyone knows what that means, here’s a model that has two parameters: \\(y = x1 + x2\\). And is trained on 5 rows of data.\nOpenAI’s GPT-3 has 175 billion parameters and was trained on essentially the whole internet. I’ll do a quick demonstration of asking it: “What is Trinity College, Toronto?”, “Which famous person went to Trinity College, Toronto?”\n[Live demo]\nGPT-3, like any language model, can generate racist/sexist text. With a student I recently used it to see whether its ability to generate racist/sexist text, also meant that it could recognize racist/sexist text and explain what made it racist/sexist. The results were disappointing and lead to a variety of conclusions, including the fact that we need to do a better job of putting together the enormous datasets that underpin these models.\nSome steps to get started in data science\nOne way to contribute in data science is to be at the intersection of two areas. Typically statistics and something else. Then you need to immerse yourself in the data. One of the most successful ‘Trinnies’ - Michael Ignatieff (IG-NAH-TEE-UHF) - says about politics that\n\nThe grind of politics, the endless travel, the meetings, the impossible schedule, the constant being on show are all in search of an authority that can be acquired in no other way. You have to learn the country.\n\nAnd it’s the same in data science - you need to become an expert in some dataset by doing the grinding, endless, impossible schlep to understand it. Only then can you even begin to consider inference and prediction.\nA lot of what you do in class is play house; when what you need to do is actually build something. One thing that helped me in this regard was getting to work with professors.\nSo how do you get to work with a professor? There’s an easy answer, which I expect is relevant for many ‘Trinnies’, and that’s to just to get an A+ in their class. But more generally, you should send an email that demonstrates that you’ve got:\nan interest; and\nsome skills.\nFor instance, these days a lot of papers in the fields that I’m interested in need some type of R package to go alongside them, and it can be hard to find students who can do this. If a student emailed having made an R package, that would demonstrate a lot of interest and skills, and it would be clear how I could involve them. There’s an enormous difference between a student who emails claiming to know R, and a student who emails a link to a GitHub repo that demonstrates it.\nConcluding remarks\nThank you very much to Manmeet for the invitation to be on this panel. It’s a great honour to be here, alongside my colleagues - Sam, Emma, and Morris - and one of my bosses - Helen! Very much looking forward to the discussion.\n\n\n\n",
    "preview": "posts/2021-10-19-trinity_remarks/IMG_9136.png",
    "last_modified": "2021-10-20T06:20:43-04:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 480
  },
  {
    "path": "posts/2021-09-17-andrew-gelman-dsi-intro/",
    "title": "Introduction to Andrew Gelman",
    "description": "Introductory remarks about Andrew Gelman delivered at the University of Toronto Data Sciences Institute (DSI) launch on 17 September 2021.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-09-17",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nAndrew Gelman\nRent\nBooks\nBlog\nReproducibility\nPapers\nStan\nImpact on others\nAcknowledgments\n\nThe slides are here.\nIntroduction\nHi, my name is Rohan Alexander. I’m an assistant professor jointly across Information and Statistical Sciences. I’m also the Assistant Director of the Canadian Statistical Sciences Institute (CANSSI), Ontario, which strengthens research and training in data science. And in that capacity it is a pleasure to introduce our next speaker, Andrew Gelman.\nAndrew Gelman\nAndrew Gelman is Higgins Professor of Statistics and Professor of Political Science at Columbia University. He believes ‘the #1 neglected topic in statistics is measurement’\nRent\nAnd so, with apologies to Rent, how should we measure his career? With blog posts? With papers? With software? With cups of coffee? With books? Or by the others who relied?\nI’ve been unable to verify his coffee consumption, but by any other measure, Andrew Gelman, puts most to shame.\nBooks\nHe has published many books and I’ll just touch on two.\nWith co-authors, his foundational Bayesian statistics textbook—Bayesian Data Analysis—was first published in 1995. It has been cited something like 30,000 times. And now on its 3rd edition, it is the basis for many foundational courses on Bayesian statistics.\nA more applied textbook, co-authored with Jennifer Hill—Data Analysis using Regression and Multilevel Models—was published in 2006. That book has been cited something like 14,000 times, and forms the foundation for many applied statistics courses.\nMy wife, Monica, is an assistant professor jointly in statistical sciences and sociology, and in my household these are known, respectively, as the Old and New Testament.\nBlog\nGelman’s blog—Statistical Modeling, Causal Inference, and Social Science—launched in 2004—is the go-to place for a fun mix of somewhat-nerdy statistics-focused content. The very first post promised to ‘…report on recent research and ongoing half-baked ideas, including … Bayesian statistics, multilevel modeling, causal inference, and political science.’ 17 years on, the site has very much kept its promise.\nReproducibility\nOne thing the blog does is enable Gelman to make contributions and influence debate in a way that isn’t possible in papers or books. For instance, it enables him to share short snippets of code, explain how he approaches problems, and sketch solutions.\nGelman’s blog is where he’s done a lot of writing about reproducibility—one of the DSI’s Thematic Programs. Arguably Andrew’s blog posts brought the replication crisis that enveloped social sciences to the fore. A lot of my colleagues now push reproducibility; both in their teaching and in their research. And that’s in large part because we intellectually grew up reading Gelman’s blog posts about it.\nPapers\nBy any measure, Andrew Gelman is a prolific author of papers, with something like 400 based on a recent count. My favourite of these is affectionately known as ‘the XBox paper’ which was published in 2015.\nThese days the statistical approach that underpinned that paper—multilevel regression with post-stratification or MRP—is used by almost every major polling company to some extent. And teams of undergraduates at the UofT used MRP to correctly forecast a Biden victory last year. If you’re interested in learning more about MRP, Andrew Gelman, Lauren Kennedy and I are putting together a book about MRP that is forthcoming with Cambridge University Press next year.\nStan\nAndrew was foundational in the development of Stan - a probabilistic programming language, which has become the predominant way to implement Bayesian models.\nFunnily enough the origin of Stan is that Gelman was trying to expand on some of the models in his book with Jennifer Hill that I mentioned earlier. He found that the software was struggling to do it. With co-authors he found that a change in the way random samples were obtained worked well, especially when combined with a few other innovations. One thing led to another, and a revolution in scientific computing ensued.\nImpact on others\nAndrew Gelman is someone who has made incredible contributions in a range of data science and perfectly characterises what we’re trying to do with this Data Sciences Institute. To this point, I’ve stuck with things that are relatively easy to measure. Following the spirit of our guest, I’d like to close with something less easy to measure; and that is, the impact on others.\nIn my own case, I wouldn’t be speaking here today if it weren’t for Andrew: I learnt statistics from his books, his XBox paper gave my career purpose, I copy-paste his code most days, his blog provided me with a community, and from his example I learnt the type of academic I wanted to be.\nAnd when I asked a few others it seems that feeling was unanimous.\n\nHis blog made me realise that there is a place for people like me, that is, people who want to do good applied stats on interesting social problems. This and reading Gelman Hill influenced my decision to go to grad school.\nMonica Alexander\n\n\nAndrew provided a voice that cut through the often confusing and rule based interpretation of statistics to instead provide an interpretation that is reason based. This represented a turning point for me in the way I understood existing statistical problems and rationalised about new challenges.\nLauren Kennedy\n\n\nHis class (and writing) showed me the connections between statistics, science, and philosophy and gave me a language through which to express my frustrations with the ways that data is tortured. I particularly appreciated his emphasis that “stats is hard” and it’s OK to make mistakes as long as you learn from them!\nJames Doss-Gollin\n\n\nWorking in government and politics, I channel Andy, both as a statistician and as a teammate. I quote insights from his books, papers, and blog posts. I learn from his humor, openness about being confused, and commitment to doing the right thing.\nShira Mitchell\n\n\nAndrew’s my Obi-Wan-Kenobi.\nBob Carpenter\n\n\nAndrew’s blog made me aware of the shortcomings in my training and research, and gave me motivation to improve. It also gave me the motivation to get out of projects that could become posts in the Zombies category.\nMarta Kołczyńska\n\n\nHere’s something that I really appreciate about Andrew and isn’t common enough in academia: if you do good work Andrew will want to work with you (even hire you) regardless of your credentials.\nJonah S Gabry\n\n\nMy conception of what good research was - what kind of research I wanted to be doing - completely changed when I read Andrew’s paper on the garden of forking paths. It was one of the main reasons I decided to go back to school for a degree in statistics, and it’s deeply influenced the way I try to do research and the research practices I advocate for at my workplace. Since that paper came out in 2013 I’ve been a dedicated reader of his blog, and his posts have made me fundamentally rethink the way I do research an embarrassing number of times - and they continue to motivate me to try to do careful and thoughtful work.\nIsaac Maddow-Zimet\n\nSo it’s a very warm virtual welcome to Toronto, Professor Andrew Gelman. Thank you for everything you’ve done for data science, and thank you for helping us to launch our Data Sciences Institute.\nAcknowledgments\nThank you very much to Monica Alexander, and members of my research group, for reading a draft of this. And to Jonah Gabry for the Stan historical background. And thank you to Isaac, Jonah, Marta, Bob, Shira, James, Lauren, and Monica for providing quotes.\n\n\n\n",
    "preview": "posts/2021-09-17-andrew-gelman-dsi-intro/dsi.png",
    "last_modified": "2021-09-17T13:18:00-04:00",
    "input_file": {},
    "preview_width": 1020,
    "preview_height": 514
  },
  {
    "path": "posts/2021-07-06-turning-our-world-into-data/",
    "title": "Turning our world into data",
    "description": "A talk delivered at the Harvard Biostatistics Data Science in Action Summer Camp, 7 July 2021, organised by Jesse Gronsbell.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-07-06",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nMy path to data science\nWhy data science is exciting\nWhat data science means to me\nTips for students\nAcknowledgments\n\nIntroduction\nHi, I’d like to thank Jesse Gronsbell for letting me talk today. My name is Rohan Alexander, and I’m an assistant professor in the Faculty of Information and the Department of Statistical Sciences at the University of Toronto.\nProfessor Gronsbell is also a professor in Statistical Sciences, but that’s about where my comparison with Jesse ends. Jesse works on a wide range of biostatistics problems, using classical statistical techniques to solve new problems, especially with large observational health data sets such as electronic health records. It’s a privilege to get to call her a colleague in data science.\nI’m just someone who likes to play with data using the statistical programming language R (R Core Team 2020). And the nice thing about what we now call data science is that there’s space for both of us. And there’s space for you as well.\nMy path to data science\nI’m 34 and so I’m in the ‘data science didn’t exist when I was an undergrad’ generation. Most of you are about 17, or about half my age. When I was 17, I wasn’t thinking about data science summer camps nor self-driving cars, so it’s commendable that you’re already so advanced.\nWhen I was 17 I just went to uni and studied economics, because that’s what my friends were doing. Luckily, I had some terrific mentors including Rabee Tourkey, Flavio Menezes, and Leo Yanes, and what they did was give broad instructions then get out of the way and check in weekly. While I don’t think I delivered anything of value to them, that experience of working directly with professors was formative for me.\nOne day I was in the library with a friend, and I wanted to go to the pub, but my friend insisted I wait until he’d applied for an internship at the Reserve Bank of Australia (RBA). I didn’t want to go to the pub without him, so I applied for that internship as well while I was waiting.\nBe careful who you make friends with because the result was that after uni I worked for a bit at the RBA, which is Australia’s central bank (think: Fed Reserve) and that was great because I learnt about just how vital communication—both written and speaking—is. (My friend didn’t get the internship, but he ended up at Goldman Sachs, so he’s doing just fine.) And also, about navigating a traditional workplace.\nAfter that I started a business with some friends because that was what I was doing when I wasn’t at work, and it seemed to make sense to make that my actual job. And that start-up was great because I learnt about strategy, tactics, execution, teamwork, and leadership; all the stuff that traditional workplaces don’t let junior analysts worry about\nWhy data science is exciting\nI’d like to come back to that earlier point—that data science didn’t exist when I was 17—because it may imply that one should not just be making decisions that optimize for what data science looks like right now, but also what could happen. While that’s a little difficult, that’s also one of the things that makes data science so exciting. As a 17-year-old that might mean choices like:\ntaking courses on fundamentals, not just fashionable applications;\nreading books, not just whatever is trending on Hacker News; and\ntrying to be at the intersection of at least a few different areas, rather than hyper-specialized.\nWhen that start-up finished, I started a PhD. My PhD is actually in economic history, so if I had my way, I’d spend my time in dusty, beautiful libraries reading old books and drinking coffee in the morning, and then writing code and drinking wine in the afternoon. Your library at Harvard is very nice, but not quite dusty enough for me, and you’re not allowed wine in there. The Faculty of Information, which is one of my appointments, originally existed to train librarians, so these days I get an office in a library, which is great, and my role is ‘human centered data science,’ so it’s very much the best of both worlds. I get to turn our world into data, analyse it, and get paid to do it!\nI spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, again with regular weekly meetings. It’s not a topic that traditionally would have been appropriate, and I’m grateful they gave me the space because traditional economics is not for me, but data science is. And I hope that you also consider that it could be for you.\nWhat data science means to me\nWhat is data science? There’s not one agreed on definition, but a lot of people have tried. My other appointment is in the Department of Statistical Sciences, and my boss there says (Craiu 2019):\n\nThis unsureness isn’t necessarily the weakness many consider it. After all, who can really say what makes someone a poet or a scientist? Even so, some things can be said. In broad strokes, … someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.\n\nI like this definition, but I also think there’s value in having a simple definition, even if we lose a bit of specificity in the process. Probability, which is related to statistics, is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, I’m currently playing around with a definition of data science that is something like ‘measuring stuff and averaging it, with purpose.’\nOne reason that I like this definition is that it doesn’t treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some data generating process, which we can never know, but that we try to use data to come to understand. I’m oversimplifying here, and many statisticians care deeply about data, but at the same time, there’s a lot of cases in statistics where the data kind of just appear; they belong to no one. But that’s just never the case.\nData have to be gathered, cleaned, and prepared, and these decisions matter (Huntington-Klein et al. 2021). I’ve come to believe that every dataset is sui generis, or in a class by itself, and so when you come to know one dataset really well, you just know one dataset, not all datasets.\nDuring this data science camp, you’re going to be exposed to an awful lot of ‘science.’ I’d like to spend a little more time in defence of ‘data.’ And that’s another reason that I like my definition. I argue that the most important word in ‘data science’ is ‘data’ and would love to convince you of it.\nI’ll take an example from Jordan (2019) where he talks about being in a medical office and being given some probability, based on prenatal screening, that his child, then a fetus, had Down syndrome. By way of background, you can decide to test to know for sure, but that test comes with the risk of the fetus not surviving, so this initial screening probability matters. Jordan (2019) describes how he found those probabilities were determined by a study done a decade earlier in the UK. The issue was that in the ensuing 10 years, imaging technology had improved so the test wasn’t expecting such high-resolution images and there had been a subsequent (false) increase in Down syndrome diagnoses when the images improved. There was no problem with the science here, it was the data that were the issue.\nTips for students\nIn my opinion, we are realising that it’s not just the ‘science’ bit that’s hard, it’s the ‘data’ bit as well. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There’s a lot of people who could have told the computer scientists that those datasets would have big problems; there’s an entire field—linguistics—that specialises in this stuff. And this is one of the dangers of any one field being hegemonic, and why it’s important that you don’t specialise too early. Instead, pick at least two different areas, learn how the ‘insiders’ speak in each, and then be the person that translates between them.\nData science needs diversity. And it’s one reason that I’m excited to see you all here with your variety of skills and backgrounds—we need you in our research labs. We need your intelligence and enthusiasm. We need you to be in the room making contributions. And I think, just like me, that working directly with professors would be formative and enjoyable for you. I hope you’re lucky enough, like me, to be given the space to find what types of projects you’re intrinsically interested in, because then everything just becomes a lot easier.\nHow do you push down the door and work with professors? I was very lucky and it was a lot easier for me to get opportunities than it is for you, because there were fewer of us. But basically, what I think you should do is show a professor that you’ve got:\nan interest; and\nsome skills.\nOne way to do this is through demonstrating your interest in coding and the data science skills that you’re developing at this camp. For instance, these days a lot of papers in the fields that I’m interested in need some type of R package to go alongside them, and it can be hard to find students who can do this. If a student emailed having made an R package, that would show a lot of interest and skills, and it would be clear how I could involve them. Another thing that is increasingly needed is a datasheet (Gebru et al. 2020). So again, you could put together documentation for a dataset and email that. Again, it shows that you’ve got a genuine interest and that you’ve got a base of skills that would enable you to be useful.\nThanks very much for letting me speak. I know that everyone says this to you, and with apologies to Olivia Rodrigo, but being 17 is just such an exciting time of one’s life, and I’m glad to see that you’re making the most of it. One seems to go straight from being the youngest in the room to being the oldest, without any middle. And, just so that you know, you kind of never work life out— everyone is just making it up as they go.\nI’d be happy to take any questions.\nAcknowledgments\nThank you very much to Monica for reading a draft of this.\n\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” http://arxiv.org/abs/2105.05241.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. 2020. “Datasheets for Datasets.” http://arxiv.org/abs/1803.09010.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey R Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry.\n\n\nJordan, Michael I. 2019. “Artificial Intelligence—the Revolution Hasn’t Happened Yet.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking. CRC Press.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\n\n\n",
    "preview": "posts/2021-07-06-turning-our-world-into-data/ed_draws.jpeg",
    "last_modified": "2021-07-07T11:22:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-22-baby-steps/",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "description": "Some thoughts on getting started with open data and reproducibility. A talk delivered at the University of Toronto, Stellar Stats Workshop, 28 May 2021, organised by Gwen Eadie & Josh Speagle.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-05-23",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nThree benefits\nImproved diversity\nImproving your own (future) life\nBuilding on-ramps\n\nThree baby steps\nBuy a bunch of notebooks\nWrite a datasheet\nMake a software package.\n\nConcluding remarks\nAcknowledgments\n\nIntroduction\nHi, my name is Rohan Alexander. Thank you very much to Gwen and Josh for the opportunity to speak. I’m a baby professor at the University of Toronto, in the Faculty of Information and the Department of Statistical Sciences. Today I would like to talk about taking some baby steps toward open data and reproducibility. And in particular the opportunities provided by open data and reproducibility across applied statistics and astronomy.\nI’d like to talk about three benefits of openness and reproducibility:\nimproved diversity broadly;\nimproving your own (future) life; and\nbuilding on-ramps for your collaborators.\nAnd then three baby steps that you could get started with this afternoon:\nbuy a bunch of notebooks;\nwrite a datasheet; and\nmake a software package.\nThree benefits\nBy way of background, M. Alexander (2019) says ‘research is reproducible if it can be reproduced exactly, given all the materials used in the study.’ She goes on:\n\nReproducibility is not just publishing your analysis code. The entire workflow of a research project––from formulating hypotheses to dissemination of your results––has decisions and steps that ideally should be reproducible.\n\nOpen Data Institute (2017) defines open data as ‘data that’s available to everyone to access, use and share.’ Open data enables reproducibility and is the bedrock of progress.\nImproved diversity\nThe first benefit that I see is improved diversity, broadly, in our fields. By making our work accessible to more people, we help to to make education and research resources more equitable. In Canadian STEM fields diversity at an undergraduate level is not great, but it’s also not too bad—we reflect broader Canadian society to some extent—but at each successive level we significantly worsen (Villemure and Webb 2017). In my own Department of Statistical Sciences, University Professor Nancy Reid was the only tenured research-track woman in the department for decades. And in astronomy, speaking more broadly, Prescord-Weinstein (2021, 132) says ‘I’ve never met a Black woman professor in the field of theoretical cosmology because I am the first—not just the first professor, but also the first Black woman cosmology theory PhD’1.\nImproving diversity is important because diverse teams are associated with higher performance (Hunt et al. 2020). That’s just a correlation, but thinking about my own experiences, I do think that diversity in my own team improved my productivity. For instance, I’ve found that students with different experiences push back and criticize me about things that I haven’t considered. Improving those aspects makes the work better.\nAs an aside, I also think that as Canada’s best university, and a public one at that, we have a duty to be more reflective of Canada. There’s a quote from one of my favourite books that I thought this audience would like, because it features an astronomer is talking to a child, trying to convince him that humanities are important, and the astronomer says:\n\nAny science is expensive and astronomy is more expensive than most…. If you need hundreds of millions of pounds sooner or later you are going to have to talk to people who don’t understand what you’re doing and don’t want to understand because they hated science at school.\nDeWitt (2000, 396)\n\nIt’s entirely correct that these people are in charge because that’s who we voted for. But it would be nice if they didn’t hate astronomy or statistics when they take it at university and possibly part of that is reflecting other experiences than just our own.\nImproving your own (future) life\nThe second benefit is that ‘future-you’ will be helped. I think that most of us who write code to analyse data for a living have had the feeling of coming back to a project after six months and a lot of the time it’s literally like having to start a new project. The variables make no sense, and the code is unintelligible. The main question for me is always ‘why did I do that?’ And then I spend the afternoon re-coding and almost always end up back where I was.\nIf we’re spending eight hours a day writing code to analyse data, then almost anything that adds even one per cent to our productivity is worthwhile, let alone something that saves an afternoon. And that’s particularly the case with open data and reproducibility, because these benefits tend to compound over time and accrue not just to you but other researchers. Talking about knowledge that only you have, doesn’t make you knowledgeable, it makes you a crank, and that includes knowledge that only ‘past-you’ has.\nBuilding on-ramps\nThe third benefit is that adopting open data and reproducibility can allow us to build better on-ramps for our collaborators. I love applied statistics, and I can’t quite believe that I get paid to do this job, and I really want more people to be able to work with me on my projects. These days I get to work with a lot of students, and I think the best way for me to make it easier for them to get up to speed is to adopt open data and reproducibility principles.\nI don’t think of myself as a scary or intimidating person, but I’m regularly told that in anonymous surveys. New, especially undergraduate, collaborators may be hesitant to ask questions because they feel awkward, or they feel they should know the answer or that they worry I’ll think they’re dumb. Some thoughts that may run through their head when they watch me talk through some data analysis: “Why did he remove the 99s?” “Maybe everyone does that?” “Can I just look this up after he finishes talking?” “Is he ever going to finish talking?” “Oh no, now I missed it, what is he saying now?” Open data and reproducibility enable them to not be reliant on me and I hope makes it easier for my collaborators to work with me.\nThree baby steps\nRight, so you’re convinced! You want these great benefits! How can you go about getting them? I’m not asking for anything big from you, and I’m not asking you to advocate, or even change much of anything that you do. (Indeed, if you’re faculty then I’m just asking you to spend some research funds and hire some undergrads!2)\nBuy a bunch of notebooks\nThe first is to buy a notebook. (If you’re faculty, then go and buy a bunch for your team.) And then just write one dot point each time you write some code chunk. Let’s say you’re cleaning some text data, and you want to change all the instances of ‘Rohan’ to ‘Monica.’ Just before you write the code that does that, or perhaps just after, write a simple dot point in that fancy notebook that you bought that explains what you did and why. That’s it. (Don’t worry too much about what you write initially – you’ll get better at it naturally over time.) At the end of the day, you’ll magically have a plain-English list of everything that was done to the dataset. That can easily be added to an appendix or added as comments and documentation alongside the code. Newton and Da Vinci kept notebooks! And if that doesn’t convince you (and it shouldn’t, see: selection bias) the US NIH describes notebooks in science as ‘legal documents’ that support claims to patents, defend against allegations of fraud, and act as your scientific legacy (Ryan 2015).\nWrite a datasheet\nThe second thing is writing a datasheet for your dataset. I had a quick look at some of the astronomy data repos and there’s some amazing things around, NASA came up immediately of course, but also a whole bunch associated with folks speaking at this workshop! I’ve never used astronomical datasets, but when I use political or economic datasets of a similar nature, I sometimes find that because I didn’t collect, clean, or prepare the dataset myself, it can be difficult to trust it. This is where datasheets come in (Gebru et al. 2020). Datasheets are basically nutrition labels for datasets. It’s really important to understand what you’re feeding your model, but plenty of researchers don’t have any idea. For instance, recently researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated (Bandy and Vincent 2021)!\nInstead of telling you how unhealthy various foods are, a datasheet tells you things like:\n‘Who created the dataset and on behalf of which entity?’\n‘Who funded the creation of the dataset?’\n‘Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?’\n‘Is any information missing from individual instances?’\nI’ve integrated datasheets into my teaching and an example of one that a student wrote earlier in Winter 2021 is Rosenthal (2021). I’m not saying that this isn’t helpful for the student who made it—I hope it was—but it’s especially helpful for me when I think about how best to use the dataset that he created.\nThe speaker after me, Renée Hlozek, has a bunch of papers documenting datasets, for instance LSST Dark Energy Science Collaboration et al. (2021), and another about data challenges (Hložek 2019) that I think would be interesting to combine with the idea of backfilling datasheets.\nMake a software package.\nThe third and final thing is to build out internal APIs for your code and then make them external. Now this may sound intimidating, but you can do it! (Or if you’re faculty, again, it’s something that you can have an undergraduate do.) My PhD is in economic history and basically what that means is that I have a PhD in data gathering and cleaning. To my shame, I have literally written code that ‘downloads a PDF from a URL, saves it to my computer, pauses, and goes and gets another PDF from a very similar URL,’ literally hundreds of times. And of course, no one should write the same code hundreds of times.\nLast summer I found myself asking a student to go and write code to do that same task and I finally decided that enough was enough. Instead, I had them put together an R package. Now this was literally only the work of a week for them. So instead of everyone in the lab writing code each time they needed to download a PDF, they could just call this R package and use that instead: we wrote an internal API. I decided that I wanted to make it external facing and there’s now an R package—‘heapsofpapers’—that anyone can use (R. Alexander and Mahfouz 2021).\nIf you want to do this and you use R then just follow ‘Chapter 2 - The Whole Game’ from Wickham and Bryan (2021). Within a few hours you can have a workable solution and within a month or two you can have a great external API.\nAnd for those of you who are more Python focused, the speaker before me, Jo Bovy, has all these great Python packages on his GitHub, including a whole course on writing Python packages (Bovy 2021).\nMaking internal APIs external is one reason that Amazon is worth a trillion dollars (Benzell, Lagarda, and Van Alstyne 2017), and while I can’t promise that it’ll make you a millionaire, I can promise that you’ll get a paper out of it, and more importantly, help your field.\nConcluding remarks\nI think that open data and reproducibility can be intimidating. I have a two-year-old and I imagine that when he first started to walk he was pretty intimidated also. But he did take his first steps and now he runs around the whole day. And I assure you that it’s the same for open data and reproducibility.\nI’m looking forward to diving further into the work of the other speakers at this workshop, and thank Gwen and Josh for bringing these two communities together, and for letting me speak today.\nAcknowledgments\nThank you very much to Monica for reading a draft of this. If you want something to watch after reading this then I recommend M. Alexander (2021)\n\n\n\nAlexander, Monica. 2019. Reproducibility in Demographic Research. Max Planck Institute for Demographic Research: Talk given at Open Science Workshop. https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\n———. 2021. Getting Started with Sharing Code. https://youtu.be/yvM2C6aZ94k.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. “Heapsofpapers.” https://rohanalexander.github.io/heapsofpapers/.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” http://arxiv.org/abs/2105.05241.\n\n\nBenzell, Seth, Guillermo Lagarda, and Marshall W Van Alstyne. 2017. “The Impact of APIs in Firm Performance.” Boston University Questrom School of Business Research Paper, no. 2843326.\n\n\nBovy, Jo. 2021. Python Code Packaging for Scientific Software. https://pythonpackaging.info.\n\n\nDeWitt, Helen. 2000. The Last Samurai. Talk Miramax Books.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. 2020. “Datasheets for Datasets.” http://arxiv.org/abs/1803.09010.\n\n\nHložek, Renée. 2019. “Data Challenges as a Tool for Time-Domain Astronomy.” Publications of the Astronomical Society of the Pacific 131 (1005): 118001. https://doi.org/10.1088/1538-3873/ab311d.\n\n\nHunt, Vivian, Sundiatu Dixon-Fyle, Sara Prince, and Kevin Dolan. 2020. Diversity Wins: How Inclusion Matters. McKinsey & Company. https://www.mckinsey.com/featured-insights/diversity-and-inclusion/diversity-wins-how-inclusion-matters.\n\n\nLSST Dark Energy Science Collaboration, Bela Abolfathi, Robert Armstrong, Humna Awan, Yadu N. Babuji, Franz Erik Bauer, George Beckett, et al. 2021. “DESC Dc2 Data Release Note.” http://arxiv.org/abs/2101.04855.\n\n\nOpen Data Institute. 2017. What Is ’Open Data’ and Why Should We Care? https://theodi.org/article/what-is-open-data-and-why-should-we-care/.\n\n\nPrescord-Weinstein, Chandra. 2021. The Disordered Cosmos. Bold Type Books.\n\n\nRosenthal, Thomas William. 2021. https://github.com/mrpotatocode/COFFEE_COFFEE_COFFEE/blob/main/journal/Week8/DataSheet-0.1.md.\n\n\nRyan, Philip. 2015. Keeping a Lab Notebook. NIH. https://youtu.be/-MAIuaOL64I.\n\n\nVillemure, Serge, and Anne Webb. 2017. Strengthening Research Excellence Through Equity, Diversity and Inclusion. NSERC Presentation. https://www.nserc-crsng.gc.ca/_doc/EDI/EDIpresentation_EN.pdf.\n\n\nWickham, Hadley, and Jenny Bryan. 2021. R Packages. https://r-pkgs.org/.\n\n\nThanks very much to Vianey Leos Barajas for gifting me this book.↩︎\nStatistical Sciences has something in the order of 4,000 undergrads, which is quite a lot, but the great thing about it is that the top 5 per cent are just incredibly strong. You could hire a few of them for literally $25 an hour, and then just get them to do these three things! Possibly that hiring would even add to the diversity of your team.↩︎\n",
    "preview": "posts/2021-05-22-baby-steps/if_the_shoes_fit.png",
    "last_modified": "2021-05-23T08:36:42-04:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-05-04-work-life-balance/",
    "title": "On work-life balance",
    "description": "Some thoughts on work-life balance as a PhD student and junior faculty and how that changes over time. A talk delivered to the University of Toronto Faculty of Information 'How to get a PhD' session on work-life balance.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nLessons\nLesson 1: You’re smart enough to work out what works for you\nLesson 2: Ignore what everyone else says and does\nLesson 3: Doing a PhD is hard\nLesson 4: An extra hour of work is almost always worthwhile\nLesson 5: Use positive procrastination\nLesson 6: Take complete, week/s-long breaks\n\nWe get to do what we want\nBetween the ideal and the reality falls the shadow\nConcluding remarks\nAcknowledgments\n\nIntroduction\nHi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto, in the Faculty of Information, and the Department of Statistical Sciences. I’d like to thank Tony Tang for the invitation to talk about work-life balance.\nThe work-life balance that I’m going to talk about today is just something for me, right now. So maybe some of what I’ll talk about will be relevant for you, but maybe my balance won’t be, and that’s okay. My main take-away, in any case, is that you’re smart enough to work out what works for you.\nI don’t think there’s only one optimal work-life balance, and if there is one, then it certainly changes over time. While doing my PhD, and also my wife doing her PhD, and us both now in faculty jobs, we’ve had a variety of opportunities to experience a bunch of different approaches to all this. We’ve also now got a two-year-old and another on the way, so I’ve reflected on all this to come up with a few lessons that I’d like to share.\nLessons\nLesson 1: You’re smart enough to work out what works for you\nThe first lesson is that you’re smart enough to work out what works for you. When my first child was born, it was all a bit challenging. As folks who have done or are doing a PhD, we’re trained in, and selected for, certain attributes, and none of those have any relation to your ability to care for an infant when it’s 3am and you’ve not slept, and the child won’t eat. I did what I was trained to do, which was look at the literature, synthesize it, plan, and execute. And of course, that didn’t help.\nOne of my PhD supervisors had also had a challenging time around the birth of her children, and she emailed me, and she said, ‘you’re smart enough to work out what works for you’. And that’s the key bit. You’re here. The admissions committee doesn’t make mistakes.\nToday is all about us giving you advice, but in the end you’re smart enough to work out what work-life balance works for you.\nLesson 2: Ignore what everyone else says and does\nThe second lesson is to ignore what everyone else is saying. Our friend once said on Twitter there are two types of people: one type says they work all the time, and the other type say they never work. The takeaway is that both types of people are probably exaggerating, and you probably shouldn’t believe either of them. Instead, you need to think about your own circumstances and decide what balance makes sense. And you need to recognize that will change as your circumstances change.\nI’ve always been a morning person, so during my PhD I used to get up at 6am to start work and then stop in mid-afternoon and go for a run and then cook dinner early and then maybe do some lighter work in the evening. And I got good at working for long chunks of time. Then the baby arrived. I became a night person for a while, then I became a bursts-of-work person for a while, now I’m back to being a morning person, but it’s more of a 4am start to get a chunk of work done, and then various bursts-of-work, followed by a short morning chunk, and then very little in the afternoon, when I may need to look after him, depending on sleep, and then maybe another chunk at night if he goes down easily.\nIgnore what others are doing, and react to your own circumstances.\nLesson 3: Doing a PhD is hard\nThe third lesson is that doing a PhD is really, really hard (and it’s meant to be). The fundamental purpose of doing a PhD is to create new knowledge. Knowledge that literally no one in the world has.\nOne way to think about it is that you’re going to become the world champion in one tiny little aspect of the world. The days of Oxbridge gentlemen athletes winning Olympic medals are gone, and that’s true also of easy PhDs. What’s left for us is the stuff that takes a huge amount of effort. Recognizing that fact, and the implication that working really hard at this is what it’s going to take, is crucial.\nThe work-life balance that I experience in a university isn’t, and cannot be, the work-life balance of those that I know outside of universities. This job is just really different. You have to be prepared for things to be hard, and to work hard, and try not to take it personally when things don’t work out straight away. It’s part of the process\nLesson 4: An extra hour of work is almost always worthwhile\nThe fourth lesson builds on this and it’s that putting an extra hour into your work is always worthwhile. The nature of our job—creating new research—means that there is always something to do. This is not a job where you lay 500 bricks in a day and then call it a day. I had a job like that once, briefly, and that job was hard. But when it was done, it was done. Our job—doing research—is not like that. There’s always something else that you can do. And it’s beneficial to do that thing today, because that way you can build on it tomorrow.\nThere’s enormous compounding in academia. Awards, conference and journal acceptances tend to go to those who already have plenty of them. So how do you get that first one? It’s just hard, but putting an extra hour in will almost always compound over time and be useful. The thing about compounding interest is that initially you don’t notice any benefit, but after a few years there’s an enormous difference.\nLesson 4.5: Until it isn’t\nHowever, there is a corollary to that fourth lesson, and that is: Until it isn’t. The fourth and a half lesson, as it were. The temptation is to just spend more and more time working. And that’s fine for a while, and is important to be able to do, from time to time. But it doesn’t work for more than a month or two.\nBut now we have a contradiction. On the one hand, I’ve just told you that there’s enormous benefits to compounding, and we all know about the importance of flow and staying in that state when you can achieve it. And on the other hand, if you take that to its natural conclusion and work all the time, you won’t last in academia for long. So, what do we do?\nLesson 5: Use positive procrastination\nThis leads to the fifth lesson, which is that you use positive procrastination to your advantage. I wrote this talk when I was tired of writing my book, which I write when I’m tired of trying to write a paper. None of this is wasting time, but the task switching ensures a freshness and enthusiasm. Another advantage of positive procrastination when you’re a PhD student is that it also helps you identify what you’re passionate about.\nNow you don’t have to love what you do—plenty of professors don’t—but I do and I’m going to speak now assuming this to be true, because it’s gone part way to resolving this contradiction.\nA friend once said that a way to think about passion is that it’s your brain’s way of tricking you into doing things that are boring or are hard. And if you can identify passion then it’ll make your life a lot easier. When I started my PhD, I was in a bit of a malaise. I’d go from subject to subject, and certainly work at them, but not enjoy anything and there were a lot of long coffees with friends. I’d cycle home, and then at night and on the weekends, I’d spend my whole-time using R to do statistical analysis of political data in a reproducible way. Then during the day I’d go to university and go back to my malaise of not knowing what to do. Of course, with some support from various advisors and mentors, I eventually worked out that what I should be doing is using R to do statistical analysis of political data in a reproducible way for my PhD and after that everything got a lot easier! I loved what I was doing and work no longer felt like work.\nThere’s a limit, of course, to positive procrastination. Because you need to make sure that you’re shooting, and not just setting up shots, as it were. As a PhD student, you probably want to have about three projects, and that’s about it. But I do think that cycling through, especially early in your PhD when it’s easy to just email a professor and take a reading course to write a short paper, is something that more students should do and would help resolve work-life concerns.\nAnd again, the viability of this changes over your career. Now I have about 10-15 different projects because I supervise students, and I have some longer-term gambles that may or may not work out. However, because of how I have to work in fits and starts, I can’t just do what I was trained to do, which is to throw more time into things. I need to try to be smarter about things and look for efficiencies and relationships between them.\nLesson 6: Take complete, week/s-long breaks\nAnd so we reach the sixth and final lesson, which is about having complete breaks, and I learnt this one from someone that I’ll name, and that’s Professor Kelly Lyons. Because once you’ve identified your passion and you understand the power of compounding and flow, it can be easy to just stay caught up in things. But to have an academic career or to finish a PhD without burning out, and if your work-life trick is going to be that you’d be doing what you get paid to do, even if you didn’t get paid to do it, then you need to make sure that remains the case.\nMonica and I have started insisting on complete week-long breaks from time to time. Minimal computers, and no GitHub. And this is one aspect where having children certainly helps. Because they want all of your attention.\nWe get to do what we want\nOur work-life balance in academia is different to that of doctors, lawyers, and tech bros. We have the best job in the world. But our job is not like others. One of the great things about academia is the flexibility. We get to do whatever we want.\nThat point bears repeating. We get to do whatever we want. And that means that whatever we’re doing is something that we’re choosing.\nHence, for me, it’s become wrong to think that work and life are separate and that they need to be siloed. People outside of universities talk about work-life balance and the implication is that you need to make time for each. But in academia, that’s not the way that I think of things.\nI think it’s a spectrum. For some people, work is work, and life is life and they’re completely separated, while for others all they do is work and that’s their life. For most of us, we’re probably somewhere in between. And I think that’s especially the case in academia. Because you’ve got so much flexibility and ownership over what we do compared with, say, working at a bank. And so, in terms of work-life balance, it’s about trying to work out what works for you. Thinking about what’s important in your life and how that fits into your work. And also, being aware that this balance is going to change especially around life events such as having a baby, which will affect how you can work, as well as the volume. And that this change is good, appropriate, and normal, even if that’s not recognized in the current academic structures.\nIf you enjoy coding, then taking a break from work might mean doing a fun coding project. If you enjoy hiking, then taking a break from work might mean going hiking. When my wife was doing her PhD, we went on road trips and worked in different environments. We did this because we loved our work, not because anyone was forcing us. We went to different cafes, and ate different foods, and ran in different parks, and went to different pubs, but for most of the day we sat at tables in random Airbnbs and worked. We have great memories of that, but not everyone wants to do that and that’s okay.\nInitially, your supervisor will put a huge amount of pressure on you, but I’ve found that as I’ve gotten older the person putting the most pressure on me is myself. Once you’re at that stage, if you think ‘oh I want a break, but I should get this done because the code isn’t working, or I need to get it to my collaborator, or whatever’, then chances are that it’s okay and you should probably just email the collaborator to say that it’ll be a week late and then take the week off. From time to time, I’ll just declare a day off and not open the computer. If you think that maybe you don’t have time for something, then you are probably right and you need to get out of it.\nIn grad school I’ve found that many people lose their hobbies. But maybe there is value in trying to make time, particularly for exercise. So, things like going for a run or being part of a team might be important for you. During my PhD a lot of my friends continued with rock-climbing, or cycling, or took up walking.\nBetween the ideal and the reality falls the shadow\nIncreasingly we see that the composition of academia is changing, for instance, in terms of the proportion of women in masters and PhD programs and junior faculty. However, institutional structures at all stages of academia are still fundamentally designed for people—particularly men—without caring responsibilities; and as such those who do not fit this mould often face enormous challenges\nWhile things have changed, this structural issue has been increasingly brought to light during the pandemic. NSERC submission deadlines remained the same. NSERC grant application requirements remained the same. I said earlier that you should take a week off, but the amount of work to be done remains the same and NSERC doesn’t change its deadlines just because you took a week off. Similarly, departments here didn’t change their teaching, or service requirements.\nA lot of people with young children have not done any research for a year, and applied for few grants. Generalising from the literature on the effect of long-term job losses, chances are that, on average, our careers will not recover. And that’s okay. We still get to do what we love. We might not become full professors, but being an associate would be great. It’s still the best job in the world.\nWhile I still think that it is correct to say, as I did above, that one shouldn’t compare oneself to others, the fact remains that the academic structures do exactly that. If you are not of the type that academia is set-up for, then every day you will fail to measure up to those expectations. We are not in a normal job, and that’s okay, but you need to recognize that the structures that job exists within may not be set-up for your circumstances. And it may be that these circumstances will dictate what can be accomplished, rather than your abilities.\nConcluding remarks\nI don’t think there’s a one-size-fits-all for work-life balance. You need to work it out for yourself and part of that is just worrying about yourself, ignoring what others do or say they do. You’re smart enough to work out what works for you, so just think. And once you work out what works for you, then set-out to make it easier for others.\nI think that there’s value in the struggle. Even if my cohort of academics with young children doesn’t achieve what it may have, there’s value in sticking with it, while that’s possible. Next time, maybe some of us will be the ones who are chairs and NSERC reviewers. Even now, we have opportunities to make things easier for the next cohort, some of whom may be the undergrads that have spent the past year in Zoom classrooms. We have the best job in the world, and sometimes taking a step back and seeing it as bigger than oneself is important.\nThanks very much to Tony for giving me the opportunity to reflect on work-life balance at universities and how that changes over the life course, and I hope that he doesn’t regret providing me with the platform too much. I’d be happy to take any questions.\nAcknowledgments\nThank you very much to Monica for reading a draft of this. Thank you also to John, Zach, Martine, Tim, and Jill at the ANU, and Kelly at U of T for helpful conversations about this topic at various points in time.\n\n\n\n",
    "preview": "posts/2021-05-04-work-life-balance/were_all_just_fingerpainting.png",
    "last_modified": "2021-05-04T13:32:59-04:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-14-greg-wilson/",
    "title": "In Appreciation of Greg Wilson",
    "description": "As he moves on from his role at R Studio Education, a few words of appreciation for Greg Wilson.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-03-14",
    "categories": [],
    "contents": "\nAs a baby professor there is a lot that causes me to wake up crying during the night. But the main reason is teaching. Having largely spent five years alone in an office working on projects by-myself, the idea that I should stand in front of (or Zoom to) one- to two-hundred students and competently teach is still a tad surprising. Greg Wilson is the only reason that I no longer wake up wracked with concern about at least this aspect of baby professor life.\nGreg established and, until recently, ran instructor training at R Studio. This involved putting together and teaching a two-day course about the Tidyverse (there’s also a Shiny version that I didn’t do). After the course, Greg assessed you on a one-to-one basis and when he was happy he certified you.\nThe training assumed that you already knew R (essentially to the level of R4DS). It’s not about R, it’s about teaching you how to teach R. We covered an awful lot, but some highlights include:\nThe creation of learner personas to help understand where the student is coming from.\nHow to formally structure assessment, and other feedback, in a way that builds a student’s knowledge and confidence.\nPutting together different aspects in a ‘concept map’ that a student can use to navigate these aspects.\nRemaining mindful of cognitive load and active teaching.\nSeeing things from the student perspective.\nAnd how to do all this in the real-world i.e. a time-constrained mess of priorities and incomplete information.\nMy guess is that none of this is pedagogical rocket science, but it’s not something that I knew, or even thought about, before Greg’s course. More important than any specific aspect, Greg provided a foundation for my teaching on which I can iterate and evolve. There’s the famous Cheshire Cat quote in Alice in Wonderland about needing to have a destination before it matters in which direction you go. Greg provided that destination. I don’t know whether it improved outcomes for my students; but it certainly reduced how worried I was about teaching.\nThat all said, I think that the most important thing that I learnt from Greg is ‘Be kind: all else is details.’. Greg taught us this through his example.\nBy my rough count there are something like 200 R Studio certified Tidyverse instructors. A lot of them are similarly professors who will teach large classes of undergrads, smaller classes of grads, and directly supervise PhD students, who themselves will teach… If Greg caused even a one per cent improvement in the quality of the instruction of those folks, then the potential impact that he’s had is enormous.\nI don’t know anything of the circumstances surrounding Greg’s departure from R Studio, and it’s none of my business either way. Greg holds a CS PhD and spent a few years as an assistant professor at the Toronto CS department a few years back. He founded Software Carpentry. In an ideal world we would find space for him at the university. But regardless of what happens, I’ll always be grateful for his teaching, his example, and most importantly, his kindness, and hope to emulate that in my own teaching.\nIf you’d like to learn more about what Greg taught us you should look at his book Teaching Tech Together: http://teachtogether.tech/en/index.html. It’s all good, but even just skimming ‘The Rules’ is worthwhile.\nA month or so ago, Greg stopped by the Faculty of Information at the University of Toronto and taught a class about ‘How to Run a Meeting’. If you’d like to see the recording of that you can do so here: https://youtu.be/qYh6Nzv3RWs.\nA bunch of other folks have written more about the training and you can read some of them here:\nhttps://silvia.rbind.io/2020-10-07-rstudio-instructor-certification-tidyverse/\nhttps://shelkariuki.netlify.app/post/certification/\nhttps://www.yuqiliao.com/blog/rstudiocertification/\n\n\n\n",
    "preview": "posts/2021-03-14-greg-wilson/certificate.png",
    "last_modified": "2021-03-14T14:27:04-04:00",
    "input_file": {},
    "preview_width": 2458,
    "preview_height": 1334
  },
  {
    "path": "posts/2021-03-13-grad-school-applications/",
    "title": "Saturday morning thoughts on grad school applications",
    "description": "There's a huge amount of luck involved. Worry as much, if not more, about letters and your personal statement as you do about your GPA. Have a clear reason for wanting to go to the school/program that you're applying for and communicate that throughout your application.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\n\nContents\nOverview\nCV and personal statement\nLetters\nTranscript\nConcluding remarks\nAcknowledgments\n\nOverview\nGrad school application season is mostly done. I wanted to quickly put together some general thoughts in case they’re helpful for undergraduates applying next year. Please keep in mind that I’m new and interested in applied statistics. This advice probably doesn’t apply for more-experienced, or less-applied, folks.\nThe main takeaways:\nThere’s a huge amount of luck involved, so don’t tie your self-worth to any particular decision either way.\nWorry as much, if not more, about letters and your personal statement as you do about your GPA. There are three bits to the application — transcript, letters, statement/CV. You’re spending four years getting transcripts, so you should also spend a lot of time on the other two.\nCommunicate a clear reason for wanting to go to the school/program that you’re applying for.\nCreate a narrative that runs through your entire application.\nThese thoughts are specific to me. You need to get advice from your own advisors, and in particular the faculty that are going to write you letters, but more on that later.\nCV and personal statement\nBe clear about why you’re applying, show evidence of your work, and weave that into your narrative. When you do sales, the key to everything is identifying a ‘champion’ in the firm that you want to sell to. Your job in sales is then to give that person the information they need to sell the rest of the firm on it. When you’re applying to grad school, it’s a similar situation. You don’t need all the faculty to love your application - you just need at least one of the reviewers to love it and for no one to hate it enough to veto. Your personal statement is a sales opportunity and will be when the faculty reviewers work out who they advocate for. It’s hard to recover from a bad personal statement. You should get your letter writers to review it before you send it.\nUse the combination of your CV and personal statement to craft a narrative (which your letters and transcript support).\nArticulate the reason that you want to attend the department/school that you’re applying for.\nThis is not a statement like ‘[insert school] is a top-ranked school’.\nOne way to do this is to find a faculty member that is of interest, look at their papers and website, and write about how your interests align with their work and you want to be involved in it. That said, don’t fake interest, because it’s usually obvious.\nAnother way is to identify a clear link to your interests, for instance, if you’re into public health then maybe the university is in a state/province that has universal healthcare and so you’d have great data.\nYet another way is to look up the requirements of the program and weave in how you’re really keen to learn about subject X taught by professor Y because of reason Z.\n\nYou don’t need the world’s greatest justification (and no one will hold you to it if you decide you’re actually interested in something else), but the faculty reviewer needs to know why you want to attend their school/department when you could go to any number of other top ranked schools/departments. There’s only a certain number of offers that can be made, so at the very least they need to have some idea of the likelihood that you’ll accept.\nGrades in undergrad tests don’t necessarily correspond to doing well in graduate school. Much of graduate school is about learning how to learn, with the goal to eventually ‘learn things that no one else knows’ (i.e. do research) and so it is helpful for reviewing faculty to see evidence that points to your ability to do that. One way to do this is to create projects that show off what you can do publicly and link to them. Ideally your undergrad professors will integrate projects into their curriculum, but if not then you need to DIY. (BTW don’t use Kaggle – hunt, gather, or farm your own data).\nFor applied quantitative PhDs, it’s great if you are already able to write code in R or Python, and even better if you can include a link to your GitHub account that shows this. Ideally, your GitHub should be in decent order. For instance, have your best projects pinned, and put a lot of effort into making sure these are commented, have READMEs, etc.\nMake a website, even if it’s very minimal. You need to control your online presence and having LinkedIn or whatever come up is not great (for the purposes of grad school applications).\nOne or two years of work experience is great (or even just a summer internship or similar), especially if it links into your overall narrative of why you want to go to that particular grad school.\nDon’t have a long personal statement that begins with you as a child and culminates in your application. Instead, be succinct and clear, and weave your accomplishments into a narrative, rather than just list them (that’s what your CV/transcript are for).\nLetters\nThe best letters are from folks that know you and your work. You need to spend a huge amount of effort getting decent letters. They’re as important, if not more, as your GPA.\nThe strongest letters are from folks who know you well. Usually, this type of letter looks better than someone who doesn’t know you well, even if they’re a famous professor, and even if you got an A in their class.\nIf possible try to work for a professor, ideally a quantitative one, because it can be more difficult to understand the evidence that non-quantitative-professors bring.\nHow do you get a job with a professor so that they will write you a letter? The best way is to excel in a course that they teach and then ask to work for them. Or excel in a course and then ask that professor to recommend you to the professor you want to work for.\nThere’s a language that good letters use. You need to be very thoughtful in your choice of letter writers because you can’t control what is written about you, other than through your choice of letter writer.\n\nThat said, not everyone can work for a professor. The main point is that you need to identify letter writers that really know your work, and you.\nBe sure to ask the letter writer if they’re comfortable recommending you to the programs that you’re applying for before you ask them to write a letter for those programs.\nThere’s a tendency to ask the most-famous-professor to write you a letter. If you’ve worked closely with them and they’ll say nice things about you, then that’s great. But the thing about famous professors is that they have seen a lot of strong students, and so it can be dangerous. Professor Big-Name saying you’re the best student they’ve ever worked with is of course great. Professor Big-Name saying you were fine to work with is still good, but less clear-cut. Professor Big-Name saying you got an A in their class doesn’t say much that isn’t already in your transcript.\nTranscript\nGrades are important, but not everything. Your transcript needs to reflect the narrative that you’re telling in the rest of the application.\nIf you are not a 4.0 student, then it’s not the end of the world. But be aware of:\nGPA minimums, and\nThat your grades at the end of your degree and/or relevant subjects weigh more heavily.\n\nGrades are important, but students seem to over-weight the importance of grades – all the components of the application are important, and grades are just one part of it.\nConcluding remarks\nGetting in anywhere requires a healthy dose of luck. It’s not a reflection of your worth as an academic, and especially not as a person. You can’t control who reviews your application, or who has funding to take students that year, or the other students applying that year. There’s also just incredible competition for these spots. There’s an awful lot of chance involved. There’s nothing you can do about this other than not tie your self-worth to it.\nAcknowledgments\nWithout wanting to implicate her in anything said here, thanks to Monica Alexander for reading many drafts of this.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-13T15:00:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-24-shelter-usage-in-toronto-2017-2021/",
    "title": "Shelter usage in Toronto (2017-2021)",
    "description": "I look at Toronto shelter usage numbers between 2017 and January 2021. I document and adjust for a systematic error in the 2017 data. The data show that homelessness in Toronto is a large problem; essentially all shelters are almost always full. COVID changed the nature of the problem by reducing the number of people using shelters, however they must be sleeping somewhere. I compare January 2021 with January 2020 and estimate that following COVID there are now around an additional 3,500 people sleeping somewhere outside of shelters, possibly outdoors.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {
          "rohanalexander.com": {}
        }
      }
    ],
    "date": "2021-01-24",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nSet up workspace and gather data\nData cleaning and preparation\nMake the data easier to deal with\nCheck content of day, month, and year\nCheck columns agree about the year\nOne last thing - plot raw data\n\nModel\nAcknowledgments\n\nIntroduction\nThe extent of homelessness in Toronto was highlighted for the R community in December 2020 when the dataset was used in TidyTuesday. That dataset contained data for 2017 to 2019, inclusive. In this post I expand the dataset through to January 2021 to see what has happened since COVID. I also document and adjust for a systematic error in the 2017 dataset. I compare the usage of shelters this year with last year. I estimate that on Friday, 22 January 2021, when it was -11C with winds of 17km/h, there were roughly 3,634 additional people sleeping somewhere other than a shelter, compared with this time last year.\nSet up workspace and gather data\nI’ll use the R statistical programming language (R Core Team 2020). The datasets are accessed via the opendatatoronto package (Gelfand 2020). This package wraps around the City of Toronto’s Open Data Portal and allows the direct import of data rather than the need to visit the website. The website is great, but using the package enhances reproducibility. I’ll use the tidyverse package to make data manipulation easier (Wickham et al. 2019).\n\n\nlibrary(opendatatoronto)\nlibrary(tidyverse)\n\n\n\n\n\n# Get the data\n# Based on https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-12-01/readme.md\nall_data <- \n  opendatatoronto::search_packages(\"Daily Shelter Occupancy\") %>% \n  opendatatoronto::list_package_resources() %>% \n  dplyr::filter(name %in% c(\"daily-shelter-occupancy-2017-csv\",\n                     \"daily-shelter-occupancy-2018-csv\", \n                     \"daily-shelter-occupancy-2019-csv\", \n                     \"daily-shelter-occupancy-2020.csv\")) %>% \n  group_split(name) %>% # Don't totally understand how this works\n  map_dfr(get_resource, .id = \"file\")\nwrite_csv(all_data, \"inputs/raw_data.csv\")\n\n\n\nLet’s just have a quick look at the data.\n\n\nall_data <- read_csv(\"inputs/raw_data.csv\", \n                     col_types = c(\"iiccccccccccii\")) \n# The col_types hieroglyphics above stand for integer, integer, character... etc.\nhead(all_data)\n\n\n# A tibble: 6 x 14\n   file `_id` OCCUPANCY_DATE ORGANIZATION_NA… SHELTER_NAME\n  <int> <int> <chr>          <chr>            <chr>       \n1     1     1 2017-01-01T00… COSTI Immigrant… COSTI Recep…\n2     1     2 2017-01-01T00… Christie Ossing… Christie Os…\n3     1     3 2017-01-01T00… Christie Ossing… Christie Os…\n4     1     4 2017-01-01T00… Christie Refuge… Christie Re…\n5     1     5 2017-01-01T00… City of Toronto  Birchmount …\n6     1     6 2017-01-01T00… City of Toronto  Birkdale Re…\n# … with 9 more variables: SHELTER_ADDRESS <chr>,\n#   SHELTER_CITY <chr>, SHELTER_PROVINCE <chr>,\n#   SHELTER_POSTAL_CODE <chr>, FACILITY_NAME <chr>,\n#   PROGRAM_NAME <chr>, SECTOR <chr>, OCCUPANCY <int>,\n#   CAPACITY <int>\n\nData cleaning and preparation\nMake the data easier to deal with\nThe column names aren’t overly nice to type, there are a few columns that we’re not really going to use much, and finally a few of the columns have data that are less obvious than they should be. For instance, the ‘file’ tells us the year of the data, but because of the import settings it’s 1, 2… instead of 2017, 2018….\n\n\ntoronto_shelters <-\n  all_data %>% \n  janitor::clean_names() %>% # Make the column names easier to type. Thanks Sharla!\n  mutate(file_year = \n           case_when(\n             file == \"1\" ~ 2017,\n             file == \"2\" ~ 2018,\n             file == \"3\" ~ 2019,\n             file == \"4\" ~ 2020,\n             TRUE ~ -1)\n  ) %>% # Just make the column easier to deal with\n  select(-id, -file)\n\n\n\nThe main issue with the data is the dates. In 2017-2019 (inclusive) they appear to be year-month-day, but for 2020 it seems like month-day-year. The separator is also inconsistent between ‘-’ and ‘/.’ I’ll first clean that up, check our guesses, and then get to the main issue. I’m going to draw on the lubridate package (Grolemund and Wickham 2011).\n\n\nlibrary(lubridate)\ntoronto_shelters <- \n  toronto_shelters %>% \n  # 1st line removes times (probs don't actually need to do) and 2nd makes the separation consistent\n  mutate(occupancy_date = str_remove(occupancy_date, \"T[:digit:]{2}:[:digit:]{2}:[:digit:]{2}\"),\n         occupancy_date = str_replace_all(occupancy_date, \"/\", \"-\")\n  ) %>% \n  # Parsing differs between 2017-2019 and 2020. Last line is a catch-all - shouldn't get there.\n  mutate(date = case_when(\n    file_year == \"2020\" ~ mdy(occupancy_date, quiet = TRUE), \n    file_year %in% c(\"2017\", \"2018\", \"2019\") ~ ymd(occupancy_date, quiet = TRUE),\n    TRUE ~ NA_Date_\n    )\n    ) %>% \n  select(file_year, date, occupancy_date, organization_name:capacity)\n\n\n\nCheck content of day, month, and year\nLet’s just check that my guess of the date orderings was at least plausible by looking at the distribution of year, month, and day bits.\n\n\ntoronto_shelters <- \n  toronto_shelters %>% \n  separate(occupancy_date, into = c('one', 'two', 'three'), sep = \"-\", remove = FALSE)\n\n\n\n\n\ntoronto_shelters %>% \n  filter(file_year %in% c(2017, 2018, 2019)) %>% \n  count(one) %>% \n  rename(Year = one, Number = n) %>% \n  kableExtra::kbl(caption = \"Count of entries by year for 2017-2019\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 1: Count of entries by year for 2017-2019\n\n\nYear\n\n\nNumber\n\n\n2017\n\n\n38700\n\n\n2018\n\n\n37770\n\n\n2019\n\n\n39446\n\n\n\n\ntoronto_shelters %>% \n  filter(file_year %in% c(2017, 2018, 2019)) %>% \n  count(two) %>% \n  rename(Month = two, Number = n) %>% \n  kableExtra::kbl(caption = \"Count of entries by month for 2017-2019\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 2: Count of entries by month for 2017-2019\n\n\nMonth\n\n\nNumber\n\n\n01\n\n\n9747\n\n\n02\n\n\n8868\n\n\n03\n\n\n9912\n\n\n04\n\n\n9600\n\n\n05\n\n\n9950\n\n\n06\n\n\n9625\n\n\n07\n\n\n9850\n\n\n08\n\n\n9743\n\n\n09\n\n\n9430\n\n\n10\n\n\n9694\n\n\n11\n\n\n9497\n\n\n12\n\n\n10000\n\n\n\n\ntoronto_shelters %>% \n  filter(file_year %in% c(2017, 2018, 2019)) %>% \n  count(three) %>% \n  ggplot(aes(x = three, y = n)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Day\",\n       y = \"Number\")\n\n\n\n\nFigure 1: Distribution of days for 2017-2019\n\n\n\nAnd again, but for 2020.\n\n\ntoronto_shelters %>% \n  filter(file_year == 2020) %>% \n  count(one) %>% \n  rename(Month = one, Number = n) %>% \n  kableExtra::kbl(caption = \"Count of entries by month for 2020\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 3: Count of entries by month for 2020\n\n\nMonth\n\n\nNumber\n\n\n01\n\n\n3503\n\n\n02\n\n\n3277\n\n\n03\n\n\n3555\n\n\n04\n\n\n3562\n\n\n05\n\n\n3671\n\n\n06\n\n\n3534\n\n\n07\n\n\n3601\n\n\n08\n\n\n3355\n\n\n09\n\n\n3210\n\n\n10\n\n\n3317\n\n\n11\n\n\n3195\n\n\n12\n\n\n3281\n\n\n\n\ntoronto_shelters %>% \n  filter(file_year == 2020) %>% \n  count(two) %>% \n  ggplot(aes(x = two, y = n)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Day\",\n       y = \"Number\")\n\n\n\n\nFigure 2: Distribution of days for 2020\n\n\n\n\n\ntoronto_shelters %>% \n  filter(file_year == 2020) %>% \n  count(three) %>% \n  rename(Year = three, Number = n) %>% \n  kableExtra::kbl(caption = \"Count of entries by year for 2020\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 4: Count of entries by year for 2020\n\n\nYear\n\n\nNumber\n\n\n2020\n\n\n41061\n\n\nThat’s all looking fine. We’d know that we have issues if the distribution of the days wasn’t roughly uniform, or if we have values other than [1-12] in the month.\nCheck columns agree about the year\nLet’s now also check that the year implied by the date matches the year implied by the file.\n\n\ntoronto_shelters %>% \n  mutate(check_year = year(date) == file_year) %>% \n  filter(check_year == FALSE)\n\n\n# A tibble: 0 x 18\n# … with 18 variables: file_year <dbl>, date <date>,\n#   occupancy_date <chr>, one <chr>, two <chr>,\n#   three <chr>, organization_name <chr>,\n#   shelter_name <chr>, shelter_address <chr>,\n#   shelter_city <chr>, shelter_province <chr>,\n#   shelter_postal_code <chr>, facility_name <chr>,\n#   program_name <chr>, sector <chr>, occupancy <int>,\n#   capacity <int>, check_year <lgl>\n\ntoronto_shelters <- \n  toronto_shelters %>% \n  select(-occupancy_date, -one, -two, -three, -file_year)\n\n\n\nThat’s also fine. And I’ll clean-up by removing the unnecessary columns.\nOne last thing - plot raw data\nEverything seems fine, but it’s always important to ‘Plot. Your. Raw. Data.’ so before moving on, I should plot the raw data to see if there’s anything else going on. (Here, students seem to get confused what ‘raw’ means; I’m using it to refer to as close to the original dataset as possible, so no sums, or averages, etc, if possible. Not necessarily before any cleaning. Sometimes your data are too disperse for that so there will be an element of manipulation. The main point is that you, at the very least, need to plot the data that you’re going to be modelling.)\nLet’s just plot the order. As this dataset has been put together by a human we’d expect that it’d be in order of date. Let’s just plot the date in the order it appears in the dataset (Figure 3).\n\n\ntoronto_shelters %>% \n  mutate(row_number = c(1:nrow(toronto_shelters))) %>% \n  ggplot(aes(x = row_number, y = date), alpha = 0.1) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Row number\",\n       y = \"Date\")\n\n\n\n\nFigure 3: Comparison of row number with date\n\n\n\n😱😱😱. This is a bit of a ‘hacky’ graph but it illustrates the point which is that the data are not in order in the dataset. If they were in order, then we’d expect them to be along the diagonal.\nIt’s super weird that they’re not in order in the raw data. Above, I checked by splitting them into pieces (day, month, year) and the counts were okay. But the ‘hacky’ graph was pretty hacky, so let’s try to summarise the data a little and then have another look. We’ll get a count by date and the sector of the shelter.\n\n\n# Based on Lisa Lendway: \n# https://github.com/llendway/tidy_tuesday_in_thirty/blob/main/2020_12_01_tidy_tuesday.Rmd\ntoronto_shelters_by_day <- \n  toronto_shelters %>% \n  # We only want rows with both occupancy and capacity  \n  tidyr::drop_na(occupancy, capacity) %>% \n  # We want to know the occupancy by date and sector\n  group_by(date, sector) %>% \n  summarise(occupancy = sum(occupancy),\n            capacity = sum(capacity),\n            usage = occupancy / capacity, .groups = 'drop')\n\nhead(toronto_shelters_by_day)\n\n\n# A tibble: 6 x 5\n  date       sector   occupancy capacity usage\n  <date>     <chr>        <int>    <int> <dbl>\n1 2017-01-01 Co-ed          530      582 0.911\n2 2017-01-01 Families      1030     1150 0.896\n3 2017-01-01 Men           1595     1706 0.935\n4 2017-01-01 Women          641      697 0.920\n5 2017-01-01 Youth          499      518 0.963\n6 2017-01-02 Co-ed          458      582 0.787\n\nWe are interested in availability of shelter spots in Toronto on the basis of sector for each day. Different sectors focus on different folks: Co-ed, Families, Men, Women, Youth. Now for each day for each sector we have a proportion (note: horrifyingly >1 is possible). In the notes to the data we’re told that the capacity in 2020 may not be accurate, so for this chart we’ll just focus on 2017-2019 (inclusive) (Figure 4).\n\n\n# Graph 2017-2019 (inc)\ntoronto_shelters_by_day %>% \n  filter(year(date) != \"2020\") %>% \n  ggplot(aes(x = date, y = usage, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Date\",\n       y = \"Occupancy rate\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 4: Occupancy rate per day in Toronto shelters\n\n\n\nThat one looks kind of okay, but we’ll again see the problem immediately when we plot the raw number occupied (we can bring this through to include 2020 as it’s not to do with capacity) (Figure 5).\n\n\ntoronto_shelters_by_day %>% \n  ggplot(aes(x = date, y = occupancy, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Date\",\n       y = \"Occupancy (number)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 5: Occupancy per day in Toronto shelters\n\n\n\nWe can see that using modified data hides the problem. Let’s focus on 2017, as that’s where the biggest issue is and facet by month (Figure 6).\n\n\ntoronto_shelters_by_day %>% \n  filter(year(date) == 2017) %>% \n  ggplot(aes(x = day(date), y = occupancy, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Day\",\n       y = \"Occupancy (number)\",\n       title = \"Toronto shelters in 2017\",\n       subtitle = \"Occupancy per day\") +\n  facet_wrap(vars(month(date, label = TRUE)),\n             scales = \"free_x\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 6: Occupancy in Toronto shelters in 2017\n\n\n\nJust to check, let’s plot the same for 2018 (Figure 7).\n\n\ntoronto_shelters_by_day %>% \n  filter(year(date) == 2018) %>% \n  ggplot(aes(x = day(date), y = occupancy, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Day\",\n       y = \"Occupancy (number)\") +\n  facet_wrap(vars(month(date, label = TRUE)),\n             scales = \"free_x\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nFigure 7: Daily occupancy in Toronto shelters in 2018\n\n\n\nThis gives us an idea of what we ought to expect in 2017 - why should they be significantly different? To start, focus on January 2017 and see if that makes it clearer what is going on (Figure 8).\n\n\ntoronto_shelters_by_day %>% \n  filter(year(date) == 2017) %>%\n  filter(month(date) == 1) %>% \n  ggplot(aes(x = day(date), y = occupancy, color = sector)) + \n  geom_point(aes(group = sector)) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Day\",\n       y = \"Occupancy (number)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 8: Daily occupancy in Toronto shelters in January 2017\n\n\n\nThis perhaps gives us some idea of what is going on. Let’s just check February and see if it looks similar (Figure 9).\n\n\ntoronto_shelters_by_day %>% \n  filter(year(date) == 2017) %>%\n  filter(month(date) == 2) %>% \n  ggplot(aes(x = day(date), y = occupancy, color = sector)) + \n  geom_point(aes(group = sector)) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Day\",\n       y = \"Occupancy (number)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 9: Daily occupancy in Toronto shelters in February 2017\n\n\n\nWe’ve clearly got a problem with the first twelve days of the month. We noted at the start that when you look at the data it’s a bit odd in that it’s not in order. Let’s take another look at that by going back to the data as it was given to us (as opposed to the data by day that we’ve been using) (Figure 10).\n\n\ntoronto_shelters %>% \n  mutate(counter = 1:nrow(toronto_shelters)) %>% \n  filter(year(date) == 2017) %>% \n  ggplot(aes(x = counter, y = date)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Row in the dataset\",\n       y = \"Date of that row\") +\n  theme_minimal()\n\n\n\n\nFigure 10: Date of each row in order in 2017\n\n\n\nAlthough there’s no rule that says the dataset has to be in order of the date, if it were, then all the points would lie on the diagonal line. We have a lot of deviation from that. To get a sense of what we’re expecting let’s look at all four years (Figure 11).\n\n\ntoronto_shelters %>% \n  mutate(counter = 1:nrow(toronto_shelters)) %>% \n  ggplot(aes(x = counter, y = date)) +\n  geom_point(alpha = 0.3) +\n  facet_wrap(vars(year(date)),\n             scales = \"free\") +\n  labs(x = \"Row in the dataset\",\n       y = \"Date of that row\") +\n  theme_minimal()\n\n\n\n\nFigure 11: Date of each row in order (2017-2020)\n\n\n\nIt looks like 2020 is as we’d expect. 2019 has a few odd situations, but not too many. 2018 has a small cluster early in the dataset and then possibly something systematic toward the middle. But it’s clear that 2017 has a large number of systematic issues.\nIn general, I think that in 2017 the first 12 days are the wrong way around, i.e we think it’s year-month-day, but it’s actually year-day-month, but there are exceptions. As a first pass, let’s just try to flip those first 12 days of each month and see if that helps. It’ll be fairly blunt, but hopefully gets us somewhere.\n\n\ntoronto_shelters <- \n  toronto_shelters %>% \n  mutate(\n    year = year(date),\n    month = month(date),\n    day = day(date),\n    date = as.character(date),\n    changed_date = if_else(\n      date %in% c(\"2017-02-01\", \"2017-03-01\", \"2017-04-01\", \"2017-05-01\", \"2017-06-01\", \n                  \"2017-07-01\", \"2017-08-01\", \"2017-09-01\", \"2017-10-01\", \"2017-11-01\", \n                  \"2017-12-01\", \"2017-01-02\", \"2017-03-02\", \"2017-04-02\", \"2017-05-02\", \n                  \"2017-06-02\", \"2017-07-02\", \"2017-08-02\", \"2017-09-02\", \"2017-10-02\", \n                  \"2017-11-02\", \"2017-12-02\", \"2017-01-03\", \"2017-02-03\", \"2017-04-03\", \n                  \"2017-05-03\", \"2017-06-03\", \"2017-07-03\", \"2017-08-03\", \"2017-09-03\", \n                  \"2017-10-03\", \"2017-11-03\", \"2017-12-03\", \"2017-01-04\", \"2017-02-04\", \n                  \"2017-03-04\", \"2017-05-04\", \"2017-06-04\", \"2017-07-04\", \"2017-08-04\", \n                  \"2017-09-04\", \"2017-10-04\", \"2017-11-04\", \"2017-12-04\", \"2017-01-05\", \n                  \"2017-02-05\", \"2017-03-05\", \"2017-04-05\", \"2017-06-05\", \"2017-07-05\", \n                  \"2017-08-05\", \"2017-09-05\", \"2017-10-05\", \"2017-11-05\", \"2017-12-05\", \n                  \"2017-01-06\", \"2017-02-06\", \"2017-03-06\", \"2017-04-06\", \"2017-05-06\", \n                  \"2017-07-06\", \"2017-08-06\", \"2017-09-06\", \"2017-10-06\", \"2017-11-06\", \n                  \"2017-12-06\", \"2017-01-07\", \"2017-02-07\", \"2017-03-07\", \"2017-04-07\", \n                  \"2017-05-07\", \"2017-06-07\", \"2017-08-07\", \"2017-09-07\", \"2017-10-07\", \n                  \"2017-11-07\", \"2017-12-07\", \"2017-01-08\", \"2017-02-08\", \"2017-03-08\", \n                  \"2017-04-08\", \"2017-05-08\", \"2017-06-08\", \"2017-07-08\", \"2017-09-08\", \n                  \"2017-10-08\", \"2017-11-08\", \"2017-12-08\", \"2017-01-09\", \"2017-02-09\", \n                  \"2017-03-09\", \"2017-04-09\", \"2017-05-09\", \"2017-06-09\", \"2017-07-09\", \n                  \"2017-08-09\", \"2017-10-09\", \"2017-11-09\", \"2017-12-09\", \"2017-01-10\", \n                  \"2017-02-10\", \"2017-03-10\", \"2017-04-10\", \"2017-05-10\", \"2017-06-10\", \n                  \"2017-07-10\", \"2017-08-10\", \"2017-09-10\", \"2017-11-10\", \"2017-12-10\", \n                  \"2017-01-11\", \"2017-02-11\", \"2017-03-11\", \"2017-04-11\", \"2017-05-11\", \n                  \"2017-06-11\", \"2017-07-11\", \"2017-08-11\", \"2017-09-11\", \"2017-10-11\", \n                  \"2017-12-11\", \"2017-01-12\", \"2017-02-12\", \"2017-03-12\", \"2017-04-12\", \n                  \"2017-05-12\", \"2017-06-12\", \"2017-07-12\", \"2017-08-12\", \"2017-09-12\", \n                  \"2017-10-12\", \"2017-11-12\"),\n      paste(year, day, month, sep = \"-\"),\n      paste(year, month, day, sep = \"-\"),\n    ),\n    changed_date = ymd(changed_date)\n    ) %>% \n  select(-year, -month, -day)\n\n\n\nNow let’s take a look (Figure 12).\n\n\ntoronto_shelters %>% \n  mutate(counter = 1:nrow(toronto_shelters)) %>% \n  filter(year(date) == 2017) %>% \n  ggplot(aes(x = counter, y = changed_date)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Row in the dataset\",\n       y = \"Date of that row\") +\n  theme_minimal()\n\n\n\n\nFigure 12: Date of each row in order in 2017 after adjustment\n\n\n\nWe can see that’s almost entirely taken care of the systematic differences. However it’s probably a little blunt. For instance, notice there are now no entries below the diagonal (Figure 13).\n\n\ntoronto_shelters_adjusted <- \n  toronto_shelters %>% \n  # We only want rows with occupancy\n  tidyr::drop_na(occupancy, capacity) %>% \n  # We want to know the occupancy by date and sector\n  group_by(changed_date, sector) %>% \n  summarise(occupancy = sum(occupancy), .groups = 'drop') \n\ntoronto_shelters_adjusted %>% \n  filter(year(changed_date) == 2017) %>% \n  ggplot(aes(x = day(changed_date), y = occupancy, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Changed day\",\n       y = \"Occupancy (number)\") +\n  facet_wrap(vars(month(changed_date, label = TRUE)),\n             scales = \"free_x\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 13: Toronto shelters daily occupancy in 2017 after adjustment\n\n\n\nWe could keep going here to try to get to the bottom of it, but the baby is going to wake up soon and I’ve got a history of wasting too much time on these types of things. One of the great things about the City of Toronto Data Portal is that each dataset has a publisher and a contact email. I’ll email them and will update this when they get back to me.\nModel\n\n…With a laptop, some free software, and a cup of coffee, I can examine what ought to seem like a staggering amount of information. …I sit here at home, surveying the scope of what’s being inflicted on people across the country and around the world as this disease spreads. … … People sometimes think (or complain) that working with quantitative data like this inures you to the reality of the human lives that lie behind the numbers. Numbers and measures are crude; they pick up the wrong things; they strip out the meaning of what’s happening to real people; they make it easy to ignore what can’t be counted. There’s something to those complaints. But it’s mostly a lazy critique. In practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom. …\"\nKieran Healy, 2020, The Kitchen Counter Observatory, 21 May. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/\n\nLet’s start by looking at the effect of COVID on occupancy. On 17 March Ontario declared a state of emergency, so let’s focus on the time around then (Figure 14 includes a dashed line at that point).\n\n\ntoronto_shelters_adjusted %>% \n  filter(year(changed_date) == 2020) %>%\n  ggplot(aes(x = changed_date, y = occupancy, color = sector)) + \n  geom_point(aes(group = sector), alpha = 0.3) +\n  geom_vline(xintercept = ymd(\"2020-03-17\"), linetype = 'dotted')+\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Changed date\",\n       y = \"Occupancy (number)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 14: Shelter usage in 2020\n\n\n\nIt’s clear that soon after the state of emergency was declared the usage of shelters began to decrease. Understandably the homeless, like all of us, want to socially distance to the extent possible and this likely means avoiding shelters. If you live in Toronto one thing that you’ll notice is that there are a lot of homeless people living in parks since COVID started. While this was fine in summer, the issue is that in winter it is well below 0C overnight.\nWe might like to see if we can estimate how many additional people are sleeping outside in January. We’ll do this by comparing the number that slept in the shelters in January 2021, compared with the number in the shelters in January 2020, which is before COVID hit Toronto. As Kieran Healy says, an exercise like this is confronting.\nWhile it’s a stretch to say that if they’re not sleeping in a shelter then they must be sleeping outside, it’s also not clear where else they could be if they’re not in a shelter. It’s possible that they have found permanent housing and so are no longer using shelters, however I don’t know of any substantial change in public policy or resource allocation such that the whole difference is due to people finding permanent homes.\nWe’ll use the data that the city has so far released for 2021. The data provider warns us against using capacity, so we’ll focus on occupancy.\n\n\ncurrent_data <- opendatatoronto::get_resource(\"29852011-c9c2-4b6d-a79a-b718f5ed1ae2\")\n\nwrite_csv(current_data, \"inputs/raw_data-January.csv\")\n\n\n\n\n\ncurrent_data <- read_csv(\"inputs/raw_data-January.csv\", \n                         col_types = c(\"icccicccicccc\")) \n\nhead(current_data)\n\n\n# A tibble: 6 x 13\n   `_id` OCCUPANCY_DATE SECTOR SHELTER_POSTAL_… CAPACITY\n   <int> <chr>          <chr>  <chr>               <int>\n1 1.91e6 2021-01-01T05… Women  M5S 2P1                 8\n2 1.91e6 2021-01-01T05… Famil… M5S 2P1               174\n3 1.91e6 2021-01-01T05… Famil… M5S 2P1               140\n4 1.91e6 2021-01-01T05… Famil… M5S 2P1               360\n5 1.91e6 2021-01-01T05… Co-ed  M5S 2P1                 8\n6 1.91e6 2021-01-01T05… Women  M5S 2P1                 8\n# … with 8 more variables: SHELTER_PROVINCE <chr>,\n#   FACILITY_NAME <chr>, SHELTER_NAME <chr>,\n#   OCCUPANCY <int>, ORGANIZATION_NAME <chr>,\n#   SHELTER_ADDRESS <chr>, SHELTER_CITY <chr>,\n#   PROGRAM_NAME <chr>\n\nJust want to do the same basic clean up of the dates as before.\n\n\ncurrent_data <- \n  current_data %>% \n  janitor::clean_names() %>% # Make the column names easier to type. Thanks Sharla!\n  mutate(occupancy_date = str_remove(occupancy_date, \"T[:digit:]{2}:[:digit:]{2}:[:digit:]{2}\"),\n  ) %>% \n  mutate(date = ymd(occupancy_date, quiet = TRUE))\nhead(current_data)\n\n\n# A tibble: 6 x 14\n      id occupancy_date sector shelter_postal_… capacity\n   <int> <chr>          <chr>  <chr>               <int>\n1 1.91e6 2021-01-01     Women  M5S 2P1                 8\n2 1.91e6 2021-01-01     Famil… M5S 2P1               174\n3 1.91e6 2021-01-01     Famil… M5S 2P1               140\n4 1.91e6 2021-01-01     Famil… M5S 2P1               360\n5 1.91e6 2021-01-01     Co-ed  M5S 2P1                 8\n6 1.91e6 2021-01-01     Women  M5S 2P1                 8\n# … with 9 more variables: shelter_province <chr>,\n#   facility_name <chr>, shelter_name <chr>,\n#   occupancy <int>, organization_name <chr>,\n#   shelter_address <chr>, shelter_city <chr>,\n#   program_name <chr>, date <date>\n\nFor each day we want to know the number in each category. We’re then going to compare this to this time last year.\n\n\ntoronto_shelters_by_day_current <- \n  current_data %>% \n  # We only want rows with occupancy\n  tidyr::drop_na(occupancy) %>% \n  # We want to know the occupancy by date and sector\n  group_by(date, sector) %>% \n  summarise(occupancy = sum(occupancy), .groups = 'drop') %>% \n  mutate(month_day = paste(month(date), day(date), sep = \"-\")) %>% \n  rename(occupancy_2021 = occupancy)\n\nhead(toronto_shelters_by_day_current)\n\n\n# A tibble: 6 x 4\n  date       sector   occupancy_2021 month_day\n  <date>     <chr>             <int> <chr>    \n1 2021-01-01 Co-ed               515 1-1      \n2 2021-01-01 Families           1128 1-1      \n3 2021-01-01 Men                 986 1-1      \n4 2021-01-01 Women               377 1-1      \n5 2021-01-01 Youth               285 1-1      \n6 2021-01-02 Co-ed               511 1-2      \n\n\n\ntoronto_shelters_by_day_2020 <- \n  toronto_shelters %>% \n  # We only want rows with occupancy\n  tidyr::drop_na(occupancy, capacity) %>% \n  # We want to know the occupancy by date and sector\n  group_by(changed_date, sector) %>% \n  summarise(occupancy = sum(occupancy), .groups = 'drop') %>% \n  filter(year(changed_date) == 2020) %>% \n  mutate(month_day = paste(month(changed_date), day(changed_date), sep = \"-\")) %>% \n  # Don't have to rename but it makes the join easier\n  rename(date = changed_date, \n         occupancy_2020 = occupancy)\n\nhead(toronto_shelters_by_day_2020)\n\n\n# A tibble: 6 x 4\n  date       sector   occupancy_2020 month_day\n  <date>     <chr>             <int> <chr>    \n1 2020-01-01 Co-ed               744 1-1      \n2 2020-01-01 Families           2670 1-1      \n3 2020-01-01 Men                1996 1-1      \n4 2020-01-01 Women               845 1-1      \n5 2020-01-01 Youth               534 1-1      \n6 2020-01-02 Co-ed               744 1-2      \n\nNow we’re going to combine the datasets so that for each day, say ‘4 January,’ we know that in 2020 shelter usage was X and on that day in 2021 we know that shelter usage was Y (Table 5).\n\n\ntoronto_shelters_by_day_current <- \n  toronto_shelters_by_day_current %>% \n  left_join(toronto_shelters_by_day_2020, by = c(\"month_day\" = \"month_day\", \"sector\" = \"sector\")) %>% \n  rename(date = date.x) %>% \n  select(date, sector, occupancy_2020, occupancy_2021) %>% \n  mutate(difference = occupancy_2021 - occupancy_2020)\n\n\ntoronto_shelters_by_day_current %>% \n  rename(Date = date,\n         Sector = sector,\n         `Occupancy in 2020` = occupancy_2020,\n         `Occupancy in 2021` = occupancy_2021,  \n         Difference = difference) %>% \n  kableExtra::kbl(caption = \"Comparison of shelter usage in January 2021 with January 2020 by day\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 5: Comparison of shelter usage in January 2021 with January 2020 by day\n\n\nDate\n\n\nSector\n\n\nOccupancy in 2020\n\n\nOccupancy in 2021\n\n\nDifference\n\n\n2021-01-01\n\n\nCo-ed\n\n\n744\n\n\n515\n\n\n-229\n\n\n2021-01-01\n\n\nFamilies\n\n\n2670\n\n\n1128\n\n\n-1542\n\n\n2021-01-01\n\n\nMen\n\n\n1996\n\n\n986\n\n\n-1010\n\n\n2021-01-01\n\n\nWomen\n\n\n845\n\n\n377\n\n\n-468\n\n\n2021-01-01\n\n\nYouth\n\n\n534\n\n\n285\n\n\n-249\n\n\n2021-01-02\n\n\nCo-ed\n\n\n744\n\n\n511\n\n\n-233\n\n\n2021-01-02\n\n\nFamilies\n\n\n2660\n\n\n1122\n\n\n-1538\n\n\n2021-01-02\n\n\nMen\n\n\n1990\n\n\n985\n\n\n-1005\n\n\n2021-01-02\n\n\nWomen\n\n\n838\n\n\n382\n\n\n-456\n\n\n2021-01-02\n\n\nYouth\n\n\n532\n\n\n282\n\n\n-250\n\n\n2021-01-03\n\n\nCo-ed\n\n\n743\n\n\n505\n\n\n-238\n\n\n2021-01-03\n\n\nFamilies\n\n\n2645\n\n\n1125\n\n\n-1520\n\n\n2021-01-03\n\n\nMen\n\n\n1985\n\n\n976\n\n\n-1009\n\n\n2021-01-03\n\n\nWomen\n\n\n834\n\n\n384\n\n\n-450\n\n\n2021-01-03\n\n\nYouth\n\n\n534\n\n\n276\n\n\n-258\n\n\n2021-01-04\n\n\nCo-ed\n\n\n745\n\n\n508\n\n\n-237\n\n\n2021-01-04\n\n\nFamilies\n\n\n2636\n\n\n1138\n\n\n-1498\n\n\n2021-01-04\n\n\nMen\n\n\n1985\n\n\n990\n\n\n-995\n\n\n2021-01-04\n\n\nWomen\n\n\n848\n\n\n387\n\n\n-461\n\n\n2021-01-04\n\n\nYouth\n\n\n535\n\n\n276\n\n\n-259\n\n\n2021-01-05\n\n\nCo-ed\n\n\n744\n\n\n510\n\n\n-234\n\n\n2021-01-05\n\n\nFamilies\n\n\n2621\n\n\n1137\n\n\n-1484\n\n\n2021-01-05\n\n\nMen\n\n\n1991\n\n\n984\n\n\n-1007\n\n\n2021-01-05\n\n\nWomen\n\n\n844\n\n\n383\n\n\n-461\n\n\n2021-01-05\n\n\nYouth\n\n\n529\n\n\n271\n\n\n-258\n\n\n2021-01-06\n\n\nCo-ed\n\n\n742\n\n\n510\n\n\n-232\n\n\n2021-01-06\n\n\nFamilies\n\n\n2607\n\n\n1139\n\n\n-1468\n\n\n2021-01-06\n\n\nMen\n\n\n1991\n\n\n987\n\n\n-1004\n\n\n2021-01-06\n\n\nWomen\n\n\n845\n\n\n388\n\n\n-457\n\n\n2021-01-06\n\n\nYouth\n\n\n534\n\n\n267\n\n\n-267\n\n\n2021-01-07\n\n\nCo-ed\n\n\n742\n\n\n509\n\n\n-233\n\n\n2021-01-07\n\n\nFamilies\n\n\n2611\n\n\n1138\n\n\n-1473\n\n\n2021-01-07\n\n\nMen\n\n\n1986\n\n\n979\n\n\n-1007\n\n\n2021-01-07\n\n\nWomen\n\n\n848\n\n\n389\n\n\n-459\n\n\n2021-01-07\n\n\nYouth\n\n\n535\n\n\n268\n\n\n-267\n\n\n2021-01-08\n\n\nCo-ed\n\n\n742\n\n\n509\n\n\n-233\n\n\n2021-01-08\n\n\nFamilies\n\n\n2618\n\n\n1133\n\n\n-1485\n\n\n2021-01-08\n\n\nMen\n\n\n1998\n\n\n979\n\n\n-1019\n\n\n2021-01-08\n\n\nWomen\n\n\n848\n\n\n386\n\n\n-462\n\n\n2021-01-08\n\n\nYouth\n\n\n536\n\n\n269\n\n\n-267\n\n\n2021-01-09\n\n\nCo-ed\n\n\n743\n\n\n506\n\n\n-237\n\n\n2021-01-09\n\n\nFamilies\n\n\n2638\n\n\n1133\n\n\n-1505\n\n\n2021-01-09\n\n\nMen\n\n\n1999\n\n\n979\n\n\n-1020\n\n\n2021-01-09\n\n\nWomen\n\n\n840\n\n\n386\n\n\n-454\n\n\n2021-01-09\n\n\nYouth\n\n\n533\n\n\n260\n\n\n-273\n\n\n2021-01-10\n\n\nCo-ed\n\n\n745\n\n\n500\n\n\n-245\n\n\n2021-01-10\n\n\nFamilies\n\n\n2627\n\n\n1128\n\n\n-1499\n\n\n2021-01-10\n\n\nMen\n\n\n1999\n\n\n971\n\n\n-1028\n\n\n2021-01-10\n\n\nWomen\n\n\n838\n\n\n383\n\n\n-455\n\n\n2021-01-10\n\n\nYouth\n\n\n535\n\n\n263\n\n\n-272\n\n\n2021-01-11\n\n\nCo-ed\n\n\n741\n\n\n502\n\n\n-239\n\n\n2021-01-11\n\n\nFamilies\n\n\n2616\n\n\n1121\n\n\n-1495\n\n\n2021-01-11\n\n\nMen\n\n\n2003\n\n\n971\n\n\n-1032\n\n\n2021-01-11\n\n\nWomen\n\n\n835\n\n\n383\n\n\n-452\n\n\n2021-01-11\n\n\nYouth\n\n\n538\n\n\n254\n\n\n-284\n\n\n2021-01-12\n\n\nCo-ed\n\n\n740\n\n\n503\n\n\n-237\n\n\n2021-01-12\n\n\nFamilies\n\n\n2634\n\n\n1136\n\n\n-1498\n\n\n2021-01-12\n\n\nMen\n\n\n1996\n\n\n965\n\n\n-1031\n\n\n2021-01-12\n\n\nWomen\n\n\n844\n\n\n383\n\n\n-461\n\n\n2021-01-12\n\n\nYouth\n\n\n536\n\n\n262\n\n\n-274\n\n\n2021-01-13\n\n\nCo-ed\n\n\n741\n\n\n500\n\n\n-241\n\n\n2021-01-13\n\n\nFamilies\n\n\n2629\n\n\n1146\n\n\n-1483\n\n\n2021-01-13\n\n\nMen\n\n\n1993\n\n\n956\n\n\n-1037\n\n\n2021-01-13\n\n\nWomen\n\n\n842\n\n\n378\n\n\n-464\n\n\n2021-01-13\n\n\nYouth\n\n\n532\n\n\n260\n\n\n-272\n\n\n2021-01-14\n\n\nCo-ed\n\n\n739\n\n\n498\n\n\n-241\n\n\n2021-01-14\n\n\nFamilies\n\n\n2635\n\n\n1145\n\n\n-1490\n\n\n2021-01-14\n\n\nMen\n\n\n1994\n\n\n915\n\n\n-1079\n\n\n2021-01-14\n\n\nWomen\n\n\n842\n\n\n372\n\n\n-470\n\n\n2021-01-14\n\n\nYouth\n\n\n537\n\n\n265\n\n\n-272\n\n\n2021-01-15\n\n\nCo-ed\n\n\n741\n\n\n506\n\n\n-235\n\n\n2021-01-15\n\n\nFamilies\n\n\n2655\n\n\n1131\n\n\n-1524\n\n\n2021-01-15\n\n\nMen\n\n\n1983\n\n\n920\n\n\n-1063\n\n\n2021-01-15\n\n\nWomen\n\n\n847\n\n\n371\n\n\n-476\n\n\n2021-01-15\n\n\nYouth\n\n\n539\n\n\n266\n\n\n-273\n\n\n2021-01-16\n\n\nCo-ed\n\n\n744\n\n\n511\n\n\n-233\n\n\n2021-01-16\n\n\nFamilies\n\n\n2654\n\n\n1133\n\n\n-1521\n\n\n2021-01-16\n\n\nMen\n\n\n1988\n\n\n918\n\n\n-1070\n\n\n2021-01-16\n\n\nWomen\n\n\n845\n\n\n370\n\n\n-475\n\n\n2021-01-16\n\n\nYouth\n\n\n533\n\n\n264\n\n\n-269\n\n\n2021-01-17\n\n\nCo-ed\n\n\n742\n\n\n513\n\n\n-229\n\n\n2021-01-17\n\n\nFamilies\n\n\n2649\n\n\n1130\n\n\n-1519\n\n\n2021-01-17\n\n\nMen\n\n\n1992\n\n\n923\n\n\n-1069\n\n\n2021-01-17\n\n\nWomen\n\n\n844\n\n\n366\n\n\n-478\n\n\n2021-01-17\n\n\nYouth\n\n\n532\n\n\n264\n\n\n-268\n\n\n2021-01-18\n\n\nCo-ed\n\n\n743\n\n\n507\n\n\n-236\n\n\n2021-01-18\n\n\nFamilies\n\n\n2639\n\n\n1125\n\n\n-1514\n\n\n2021-01-18\n\n\nMen\n\n\n1999\n\n\n923\n\n\n-1076\n\n\n2021-01-18\n\n\nWomen\n\n\n844\n\n\n371\n\n\n-473\n\n\n2021-01-18\n\n\nYouth\n\n\n535\n\n\n264\n\n\n-271\n\n\n2021-01-19\n\n\nCo-ed\n\n\n744\n\n\n504\n\n\n-240\n\n\n2021-01-19\n\n\nFamilies\n\n\n2631\n\n\n1106\n\n\n-1525\n\n\n2021-01-19\n\n\nMen\n\n\n1995\n\n\n931\n\n\n-1064\n\n\n2021-01-19\n\n\nWomen\n\n\n849\n\n\n379\n\n\n-470\n\n\n2021-01-19\n\n\nYouth\n\n\n536\n\n\n263\n\n\n-273\n\n\n2021-01-20\n\n\nCo-ed\n\n\n742\n\n\n503\n\n\n-239\n\n\n2021-01-20\n\n\nFamilies\n\n\n2629\n\n\n1081\n\n\n-1548\n\n\n2021-01-20\n\n\nMen\n\n\n2001\n\n\n941\n\n\n-1060\n\n\n2021-01-20\n\n\nWomen\n\n\n840\n\n\n371\n\n\n-469\n\n\n2021-01-20\n\n\nYouth\n\n\n536\n\n\n263\n\n\n-273\n\n\n2021-01-21\n\n\nCo-ed\n\n\n745\n\n\n501\n\n\n-244\n\n\n2021-01-21\n\n\nFamilies\n\n\n2647\n\n\n1075\n\n\n-1572\n\n\n2021-01-21\n\n\nMen\n\n\n2007\n\n\n941\n\n\n-1066\n\n\n2021-01-21\n\n\nWomen\n\n\n839\n\n\n367\n\n\n-472\n\n\n2021-01-21\n\n\nYouth\n\n\n534\n\n\n263\n\n\n-271\n\n\n2021-01-22\n\n\nCo-ed\n\n\n744\n\n\n505\n\n\n-239\n\n\n2021-01-22\n\n\nFamilies\n\n\n2648\n\n\n1058\n\n\n-1590\n\n\n2021-01-22\n\n\nMen\n\n\n2014\n\n\n949\n\n\n-1065\n\n\n2021-01-22\n\n\nWomen\n\n\n843\n\n\n367\n\n\n-476\n\n\n2021-01-22\n\n\nYouth\n\n\n531\n\n\n267\n\n\n-264\n\n\n2021-01-23\n\n\nCo-ed\n\n\n740\n\n\n508\n\n\n-232\n\n\n2021-01-23\n\n\nFamilies\n\n\n2653\n\n\n1058\n\n\n-1595\n\n\n2021-01-23\n\n\nMen\n\n\n2008\n\n\n959\n\n\n-1049\n\n\n2021-01-23\n\n\nWomen\n\n\n842\n\n\n362\n\n\n-480\n\n\n2021-01-23\n\n\nYouth\n\n\n530\n\n\n273\n\n\n-257\n\n\nLet’s get some average statistics for January by sector (Table 6).\n\n\njanuary_average <- \n  toronto_shelters_by_day_current %>% \n  group_by(sector) %>% \n  summarise(Difference = mean(difference)) %>% \n  mutate(Difference = as.integer(Difference))\n\njanuary_average %>% \n  rename(Sector = sector,\n         `Difference this January compared with last` = Difference) %>% \n  kableExtra::kbl(caption = \"Overall comparison of shelter usage in January 2021 with January 2020\") %>%\n  kableExtra::kable_styling()\n\n\n\nTable 6: Overall comparison of shelter usage in January 2021 with January 2020\n\n\nSector\n\n\nDifference this January compared with last\n\n\nCo-ed\n\n\n-236\n\n\nFamilies\n\n\n-1516\n\n\nMen\n\n\n-1037\n\n\nWomen\n\n\n-465\n\n\nYouth\n\n\n-267\n\n\nLet’s look at a comparison graph (Figure 15).\n\n\ntoronto_shelters_by_day_current %>% \n  select(-difference) %>% \n  pivot_longer(cols = c(\"occupancy_2020\", \"occupancy_2021\"),\n               names_to = \"year\",\n               values_to = \"number\"\n               ) %>% \n  mutate(year = if_else(year == 'occupancy_2020', '2020', '2021')) %>% \n  ggplot(aes(x = date, y = number, color = year)) +\n  geom_point() +\n  facet_wrap(vars(sector)) +\n  theme_minimal() +\n  labs(x = \"Date\",\n       y = \"Occupancy in shelters (#)\",\n       color = \"Year\")  +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 15: Comparison of shelter usage in January 2021 with January 2020\n\n\n\nI estimate that in January, on average, there are roughly 3,517 additional people sleeping outside in below freezing conditions this year compared with last year. That is to say, shelter usage is that much below what it was, and I’m not sure where else they could go. In particular, on Friday night, when it was horrendously cold, I estimate there were 3,634 additional people likely sleeping outside, by comparing the usage of shelters on 22 January 2020 with 22 January 2021.\nEven if I’m off by an order of magnitude, the city and province clearly should do more.\nAcknowledgments\nThank you to Monica Alexander for helpful comments.\n\n\n\nGelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\n\n\n",
    "preview": "posts/2021-01-24-shelter-usage-in-toronto-2017-2021/toronto-shelter-usage_files/figure-html5/firstthreeyearsdistrib-1.png",
    "last_modified": "2021-01-27T15:11:31-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-04-14-tidying-the-2019-kenyan-census/",
    "title": "Tidying the 2019 Kenyan Census",
    "description": "In this blog post I convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on an introduce a bunch of handy packages including: `janitor`, `pdftools`, `tidyverse`, and `stringi`.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2020-04-14",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nWorkspace set-up\nGet the data into R\nCleaning\nValues\nAreas\nAges\n\nChecks\nCheck gender sum\nCheck rural urban split\nCheck ages sum to age-groups\n\nFinal tidying\nMake Monica’s dataset\n\nIntroduction\nThe distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here: https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units.\nAnd while it is great that they make it easily available, and it is easy to look-up a particular result, it is not overly useful to do larger-scale data analysis, such as building a Bayesian hierarchical model.\nIn this blog post I convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on and introduce a bunch of handy packages including: janitor by Firke (2021), pdftools by Ooms (2020), tidyverse by Wickham et al. (2019), and stringi by Gagolewski (2020).\nIf you just want the cleaned tidied data, then it is here.\nWorkspace set-up\nTo get started I need to load the necessary packages.\n\n\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(tidyverse)\nlibrary(stringi)\n\n\n\nAnd then I need to read in the PDF that I want to convert.\n\n\n# Read in the PDF\nall_content <- pdftools::pdf_text(\"inputs/pdfs/2019_Kenya_census.pdf\")\n\n\n\nThe pdf_text function from pdftools is useful when you have a PDF and you want to read the content into R. For many recently produced PDFs it’ll work pretty well, but there are alternatives. If the PDF is an image, then it won’t work and you’ll need to turn to OCR.\nYou can see a page of the PDF here:\n\n\nknitr::include_graphics(\"images/2020-04-10-screenshot-of-census.png\") \n\n\n\n\nGet the data into R\nThe first challenge is to get the dataset into a format that we can more easily manipulate. The way that I am going to do this is to consider each page of the PDF and extract the relevant parts. To do this, I first write a function that I want to apply to each page.\n\n\n# The function is going to take an input of a page\nget_data <- function(i){\n  # Just look at the page of interest\n  # Based on https://stackoverflow.com/questions/47793326/tabulize-function-in-r\n  just_page_i <- stringi::stri_split_lines(all_content[[i]])[[1]] \n  \n  # Grab the name of the location\n  area <- just_page_i[3] %>% str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] %>% str_squish()\n  \n  # Get rid of the top matter\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] # Just manually for now, but could create some rules if needed\n  \n  # Get rid of the bottom matter\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] # Just manually for now, but could create some rules if needed\n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # # Split columns\n  demography_data <-\n    demography_data %>%\n    mutate(all = str_squish(all)) %>% # Any space more than two spaces is squished down to one\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) %>% \n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) %>% # Any space more than two spaces is squished down to one\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Just looking for a space. Seems to work fine because the tables are pretty nicely laid out\n             remove = TRUE,\n             fill = \"right\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data %>% select(age, male, female, total),\n          demography_data %>%\n            select(age_2, male_2, female_2, total_2) %>%\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long %>% \n    janitor::remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n\n\n\nAt this point, I have a function that does what I need to each page of the PDF. I’m going to use the function map_dfr from the purrr package to apply that function to each page, and then combine all the outputs into one tibble.\n\n\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data, all_content)\n\n\n\nCleaning\nI now need to clean the dataset to make it useful.\nValues\nThe first step is to make the numbers into actual numbers, rather than characters. Before I can convert the type I need to remove anything that is not a number otherwise it’ll be converted into an NA. So I first identify any values that are not numbers so that I can remove them.\n\n\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables %>% \n  select(male, female, total) %>%\n  mutate_all(~str_remove_all(., \"[:digit:]\")) %>% \n  mutate_all(~str_remove_all(., \",\")) %>%\n  mutate_all(~str_remove_all(., \"_\")) %>%\n  mutate_all(~str_remove_all(., \"-\")) %>% \n  distinct()\n\n\n# A tibble: 3 x 3\n  male  female total\n  <chr> <chr>  <chr>\n1 \"\"    \"\"     \"\"   \n2 \"Aug\" \"\"     \"\"   \n3 \"Jun\" \"\"     \"\"   \n\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\n\n\n\nWhile you could use the janitor package here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that you want. In this case, they’ve used Excel or similar and this has converted a couple of their entries into dates. If we just took the numbers from the column then we’d have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively.\nHaving identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers.\n\n\nall_tables <-\n  all_tables %>%\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) %>% # First get rid of commas\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~as.integer(.))\n\n\n\nAreas\nThe next thing to clean is the areas. We know that there are 47 counties in Kenya, and a whole bunch of sub-counties. They give us a list on pages 19 to 22 of the PDF (document pages 7 to 10). However, this list is not complete, and there are a few minor issues that we’ll deal with later.\nIn any case, I first need to fix a few inconsistencies.\n\n\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\n\n\n\nKenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county so we need to first fix that.\nThe PDF is made-up of three tables.\n\n\nall_tables$table %>% table()\n\n\n.\nTable 2.3: Distribution of Population by Age, Sex*, County and Sub- County \n                                                                     48216 \n      Table 2.4a: Distribution of Rural Population by Age, Sex* and County \n                                                                      5535 \n      Table 2.4b: Distribution of Urban Population by Age, Sex* and County \n                                                                      5781 \n\nSo I can first get the names of the counties based on those first two tables and then reconcile them to get a list of the counties.\n\n\n# Get a list of the counties \nlist_counties <- \n  all_tables %>% \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) %>% \n  select(area) %>% \n  distinct()\n\n\n\nAs I hoped, there are 47 of them. But before I can add a flag based on those names, I need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page.\n\n\n# The following have the issue of the name being used for both a county and a sub-county:\nall_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") %>% \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) %>% \n  select(area, page) %>% \n  distinct()\n\n\n# A tibble: 24 x 2\n   area        page\n   <chr>      <int>\n 1 Samburu       42\n 2 Tana River    53\n 3 Tana River    56\n 4 Garissa       65\n 5 Garissa       69\n 6 Isiolo        98\n 7 Isiolo       100\n 8 Machakos     149\n 9 Machakos     154\n10 Makueni      159\n# … with 14 more rows\n\nNow we can add the flag for whether the area is a county and adjust for the ones that are troublesome,\n\n\n# Add flag for whether it is a county or a sub-county\nall_tables <- \n  all_tables %>% \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n# Fix the flag for the ones that have their names used twice\nall_tables <- \n  all_tables %>% \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)\n\n\n\nAges\nNow we can deal with the ages.\nFirst we need to fix some errors.\n\n\n# Clean up ages\ntable(all_tables$age) %>% head()\n\n\n\n    0   0-4     1    10 10-14 10-19 \n  484   484   484   484   482     1 \n\nunique(all_tables$age) %>% head()\n\n\n[1] \"Total\" \"0\"     \"1\"     \"2\"     \"3\"     \"4\"    \n\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\n\n\n\nThe census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such I’ll add a flag as to the type of age it is: an age group, such as ages 0 to 5, or a single age, such as 1.\n\n\n# Add a flag as to whether it's a summary or not\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"Total\")), \"age-group\", all_tables$age_type)\n\n\n\nAt the moment, age is a character variable. We have a decision to make here, because we don’t want it to be a character variable (because it won’t graph properly), but we don’t want it to be a numeric, because there is total and also 100+ in there. So for now, we’ll just make it into a factor, and at least that will be able to be nicely graphed.\n\n\nall_tables$age <- as_factor(all_tables$age)\n\n\n\nChecks\nCheck gender sum\nGiven the format of the data, at this point it is easy to check that total is the sum of male and female.\n\n\n# Check the parts and the sums\nfollow_up <- \n  all_tables %>% \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) %>% \n  filter(totals_match == 0)\n\n\n\nThere is just one that seems wrong.\n\n\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)\n\n\n\nCheck rural urban split\nThe census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we’d like to make sure that the sum of rural and urban counts equals the total count. This requires reshaping the data from a long to wide format.\nFirst, construct different tables for each of the three. I just do it manually, but I could probably do this a nicer way.\n\n\n# Table 2.3\ntable_2_3 <- all_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables %>% \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables %>% \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n\n\n\nHaving constructed the constituent parts, I now join then based on age, area, and whether it is a county.\n\n\nboth_2_4s <- full_join(table_2_4a, table_2_4b, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_rural\", \"_urban\"))\n\nall <- full_join(table_2_3, both_2_4s, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_all\", \"_\"))\n\nall <- \n  all %>% \n  mutate(page = glue::glue('Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}')) %>% \n  select(-page, -page_rural, -page_urban,\n         -table, -table_rural, -table_urban,\n         -age_type_rural, -age_type_urban\n         )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\n\n\n\nWe can now check that the sum of rural and urban is the same as the total.\n\n\n# Check that the urban + rural = total\nfollow_up <- \n  all %>% \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) %>% \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n\n\n# A tibble: 3 x 16\n  age     male female  total area  area_type age_type\n  <fct>  <int>  <int>  <int> <chr> <chr>     <chr>   \n1 Not …     31     10     41 Naku… county    single-…\n2 Total 434287 441379 875666 Bomet county    age-gro…\n3 Not …      3      2      5 Bomet county    single-…\n# … with 9 more variables: male_rural <int>,\n#   female_rural <int>, total_rural <int>,\n#   male_urban <int>, female_urban <int>,\n#   total_urban <int>, total_from_bits <int>,\n#   check_total_is_rural_plus_urban <dbl>, `total_from_bits\n#   - total` <int>\n\nrm(follow_up)\n\n\n\nThere are just a few, but they only have a a difference of 1, so I’ll just move on.\nCheck ages sum to age-groups\nFinally, I want to check that the single age counts sum to the age-groups.\n\n\n# One last thing to check is that the ages sum to their age-groups.\nfollow_up <- \n  all %>% \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) %>% \n  group_by(area_type, area, groups) %>% \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) %>% \n  ungroup() %>% \n  filter(age == groups) %>% \n  filter(total != group_sum) \n\nhead(follow_up)\n\n\n# A tibble: 6 x 16\n  age    male female total area  area_type age_type\n  <fct> <int>  <int> <int> <chr> <chr>     <chr>   \n1 0-4       1      5     6 Mt. … sub-coun… age-gro…\n2 5-9       1      2     3 Mt. … sub-coun… age-gro…\n3 10-14     6      0     6 Mt. … sub-coun… age-gro…\n4 15-19     9      1    10 Mt. … sub-coun… age-gro…\n5 20-24    21      4    25 Mt. … sub-coun… age-gro…\n6 25-29    59      9    68 Mt. … sub-coun… age-gro…\n# … with 9 more variables: male_rural <int>,\n#   female_rural <int>, total_rural <int>,\n#   male_urban <int>, female_urban <int>,\n#   total_urban <int>, groups <chr>, group_sum <dbl>,\n#   difference <dbl>\n\nrm(follow_up)\n\n\n\nMt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. I can’t see it in the documentation, but it looks like they have apportioned these between various countries. It’s understandable why they’d do this and it’s probably not a big deal, so I’ll just move on.\nFinal tidying\nNow that we are confident that everything is looking good, we can just convert it to long-format so that it is easy to work with.\n\n\nall <- \n  all %>% \n  rename(male_total = male,\n         female_total = female,\n         total_total = total) %>% \n  pivot_longer(cols = c(male_total, female_total, total_total, male_rural, female_rural, total_rural, male_urban, female_urban, total_urban),\n               names_to = \"type\",\n               values_to = \"number\"\n               ) %>% \n  separate(col = type, into = c(\"gender\", \"part_of_area\"), sep = \"_\") %>% \n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nwrite_csv(all, file = \"outputs/data/cleaned_kenya_2019_census.csv\")\n\nhead(all)\n\n\n# A tibble: 6 x 7\n  area  area_type part_of_area age   age_type gender  number\n  <chr> <chr>     <chr>        <fct> <chr>    <chr>    <int>\n1 Momb… county    total        Total age-gro… male    610257\n2 Momb… county    total        Total age-gro… female  598046\n3 Momb… county    total        Total age-gro… total  1208303\n4 Momb… county    rural        Total age-gro… male        NA\n5 Momb… county    rural        Total age-gro… female      NA\n6 Momb… county    rural        Total age-gro… total       NA\n\nMake Monica’s dataset\nThe original purpose of all of this was to make a table for Monica. She needed single-year counts, by gender, for the counties.\n\n\nmonicas_dataset <- \n  all %>% \n  filter(area_type == \"county\") %>% \n  filter(part_of_area == \"total\") %>%\n  filter(age_type == \"single-year\") %>% \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n\n\n# A tibble: 6 x 4\n  area    age   gender number\n  <chr>   <fct> <chr>   <int>\n1 Mombasa 0     male    15111\n2 Mombasa 0     female  15009\n3 Mombasa 0     total   30120\n4 Mombasa 1     male    15805\n5 Mombasa 1     female  15308\n6 Mombasa 1     total   31113\n\n\n\nwrite_csv(monicas_dataset, \"outputs/data/monicas_dataset.csv\")\n\n\n\nI’ll leave the fancy stats to Monica, but I’ll just make a quick graph of Nairobi.\n\n\nmonicas_dataset %>% \n  filter(area == \"Nairobi\") %>%\n  ggplot() +\n  geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") + \n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic()+\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"Number\",\n       x = \"Age\",\n       fill = \"Gender\",\n       title = \"Distribution of age and gender in Nairobi in 2019\",\n       caption = \"Data source: 2019 Kenya Census\")\n\n\n\n\n\n\n\nFirke, Sam. 2021. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGagolewski, Marek. 2020. R Package Stringi: Character String Processing Facilities. http://www.gagolewski.com/software/stringi/.\n\n\nOoms, Jeroen. 2020. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\n\n\n",
    "preview": "posts/2020-04-14-tidying-the-2019-kenyan-census/images/2020-04-10-screenshot-of-census.png",
    "last_modified": "2021-01-30T09:10:29-05:00",
    "input_file": {},
    "preview_width": 1060,
    "preview_height": 1500
  },
  {
    "path": "posts/2020-02-11-a-review-of-forecasting-elections-with-non-representative-polls/",
    "title": "A review of 'Forecasting elections with non-representative polls'",
    "description": "This brief review of Wang, Rothschild, Goel, and Gelman, (2015) 'Forecasting elections with non-representative polls' is designed to motivate discussion during [Lauren Kennedy](https://jazzystats.com/)'s weekly MRP Discussion Group.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2020-02-11",
    "categories": [],
    "contents": "\nTable of Contents\nIntroduction\nMotivation\nData\nModel\nResults - Voter intention\nResults - Election outcomes\nConclusions\nReferences\nIntroduction\n‘Forecasting elections with non-representative polls’ by Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, published in 2015 in the International Journal of Forecasting, is a wonderful paper that should be more widely read, studied, and cited.1 It touches on many aspects from across social sciences and applied statistics, allowing readers from a variety of backgrounds a chance to better understand multi-level regression with post-stratification (MRP), and how they might contribute to its development or apply it in their own contexts. The paper clearly illustrates the power of MRP and motivates its use more widely, but is held back by not being reproducible and some unusual choices around datasets.\nIn the paper the authors use a non-representative sample of voting intention in the 2012 US presidential election from the Xbox platform, which enables people to play video games against each other online. They then combine this sample with data from exit polls, as well as a Bayesian hierarchical model, to provide estimates that are very similar to those produced from representative samples.\nThe key contributions of the paper include:\nShowing how a ‘non-traditional’ dataset can still yield meaningful results.\nTranslating daily estimates of opinion into what actually determines the election: electoral college votes.\nIllustrating the potential for MRP in other areas.\nKey weaknesses of the paper include:\na lack of reproducibility, and even a lack of transparency in some aspects;\na difficult-to-justify choice of dataset for post-stratification;\na lack of clarity around the propagation of uncertainty; and\nan inconsistent definition of what a good MRP model produces.\nIn the remainder of this review I will summarise the key aspects of the paper, attempting to link them to subsequent research where appropriate, and discussing the paper’s strengths and weaknesses, as well as raising questions that are unresolved in my mind. Based on Google Scholar, as of 11 February 2020, the paper has ‘only’ been cited around 250 times. It deserves to be much more widely read as it is a straight-forward paper that clearly illustrates the power of MRP, without hiding the potential issues.\nMotivation\n\nThe king (representative sampling) is dead, long live the king (non-representative sampling).\n\n\n\n\nFigure 1: Wang, Rothschild, Goel, and Gelman, (2015), page 1.\n\n\n\nThe paper begins with a detailed description of the role of representative sampling in modern opinion polling. It clearly illustrates why representative sampling become important, following the Literary Digest error during the 1936 US presidential election between Landon and Roosevelt. It does this with reference to two papers, both published in Public Opinion Quarterly albeit with a gap of more than 50 years, that are very interesting in their own right: Gosnell (1937) and Squire (1988) (Figure 2).\n\n\n\nFigure 2: Gosnell, 1937, on the left, and Squire, 1988 on the right.\n\n\n\nThese first few paragraphs may prompt some readers to think about the role of representative sampling in their own areas. The authors clearly establish the paper as aimed at a general audience,2 rather than solely Bayesian cult members, quantitative absolutionists, math wunderkinds3, Stan stans4 those who are already convinced of the power of MRP. The authors then give two reasons why the primacy of representative sampling may not now be absolute:\nNon-response rates are increasing for a variety of reasons including a reluctance to answer phone surveys and increased screening. The effect is possible concern around selection, but even if this selection issue does not arise, lower response rates require more post-sampling adjustment.\nOnline surveys mean large, non-representative, samples are much cheaper to collect.\nOn the basis of this the authors suggest MRP as an alternative and proceed to discuss their sample and their post-stratification dataset.\nData\nAt a minimum MRP requires two datasets. The first is the sample that is of interest, and the second is a dataset by which to adjust for some of that sample’s bias. This paper uses data from a biased, but large, sample of Xbox users about their voting intention in the 2012 US presidential election, and uses a poststratification dataset from exit polls.\nSurvey data\nKey facts about the survey data:\nData from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election.\nEach day there were three to five questions, including voter intention: ‘If the election were held today, who would you vote for?’.\nRespondents were allowed to answer at most once per day.\nFirst-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election. (That demographic data was collected before a respondent answered a poll to try to limit the amount of people switching their demographics to be in line with their intention, although it is difficult to adjust for the inverse, and increasingly respondents seem to be responding as pundits.)\nThere were 750,148 responses, 345,858 unique respondents, and over 30,000 unique respondents who completed five or more polls.5 6\nThe dataset is highly skewed (Figure 3):\n18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll.\nMen make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\n\n\n\nFigure 3: (This is Figure 1 in the paper) ‘A comparison of the demographic, partisan, and 2008 vote distributions in the Xbox dataset and the 2012 electorate (as measured by adjusted exit polls). As one might expect, the sex and age distributions exhibit considerable differences.’\n\n\n\nAge and sex are known to be strongly correlated with voting preferences, and this shows up in the raw data (Figure 4).\n\n\n\nFigure 4: (This is Figure 2 in the paper.) ‘Daily (unadjusted) Xbox estimates of the two-party Obama support during the 45 days leading up to the 2012 presidential election, which suggest a landslide victory for Mitt Romney. The dotted blue line indicates a consensus average of traditional polls (the daily aggregated polling results from Pollster.com), the horizontal dashed line at 52% indicates the actual two-party vote share obtained by Barack Obama, and the vertical dotted lines give the dates of the three presidential debates.’\n\n\n\nThere are considerable changes over time, for instance a decrease in support for Obama after the first debate (Figure 4).\nPost-stratification data\nThe survey is biased, but the question at the heart of MRP is whether it can be salvaged.\nIn order to adjust for known bias in the sample we need a dataset that we would like our dataset to ‘mimic’. This is done by constructing ‘cells’ which are defined by a combination of all of the variables (say economic, demographic, and geographic features). For instance if we were interested in explaining vote share on the basis of age-group and sex, then we would need to know the number of 18-29-year-old males, the numbers of 18-29-year-old females, the number of 30-45-year-old males, and so on. We then apply a model trained on the survey to these ‘corrected’ proportions. Hence each additional variable adds considerably to what is needed from the post-stratification dataset.7\nOnce those cells are constructed, then the model trained on the sample is applied to each cell to generate an estimate for each cell. For instance, we might be interested in the proportion of 18-29-year-old males who would vote for Obama. Those cells can then be aggregated based on their relative-weight in the post-stratification dataset. For instance, if 18-29-year-old males make up 10 per cent of the population then their support for Obama would be weighted to 10 per cent.\nThis section of the paper is especially compelling. It draws on a variety of important issues including sampling and uncertainty, to motivate the use of a hierarchical model. The specific issue in the minds of many readers who may be new to MRP but are used to analysing surveys is that the above post-stratification process could be done with weights. This requires assuming that within each cell the sampling is un-biased. That implies a need for fine cells. But in practice especially fine cells are likely to have small populations, and so small differences will have large effects. Here we can think of the kidney cancer counties example where the clusters of the high and low countries tend to be right next to each other, because a handful of cases in a small area has a big effect. We can also think of Monica Alexander’s shrinking California example (Figure 5).\n\n\n\nFigure 5: Effect of shrinking California on estimates of mortality by age.\n\n\n\nHence, the need for a hierarchical model!8 The authors mention a variety of MRP papers going back to 2004.9\nThe authors use a large number of variables to generate the cells by:\nsex (2 categories);\nrace (4 categories);\nage-group (4 categories);\neducation (4 categories);\nstate (51 categories);\nparty ID (3 categories);\nideology (3 categories); and\n2008 vote (3 categories).\nAs such they have \\(2\\times4\\times4\\times4\\times51\\times3\\times3\\times3 = 176,256\\) cells. O.M.G. The authors provide some justification for the inclusion of each of these variables, but there was scope for more given the importance of this aspect within an MRP analysis.10\nThe Current Population Survey (CPS) is the usual go-to survey in the US in terms of post-stratification data. But the authors turn away from the CPS because ‘the CPS is missing some key poststratification variables, such as party identification.’11 Instead they use exit poll data from the 2008 presidential election.\n\nExit polls are conducted outside voting stations on election day, and record the choices of exiting voters; they are generally used by researchers and news media to analyze the demographic breakdown of the vote (after a post-election adjustment that aligns the weighted responses to the reported state-by-state election results).\n\nThe exit poll that they use is made up of 101,638 respondents.12 The authors describe how this disadvantages their analysis as they have to use data that is four years out of date (it wouldn’t be appropriate to use 2012 exit polls to forecast the 2012 election!).13\nModel\nGiven the nature of the US electorate, they use a nested modelling approach: the first models whether a respondent is likely to vote for one of Obama or Romney given various information such as state, education, sex, etc:[Romney! Wow! Remember the good old days when it was the fact that the Republican candidate was going to go out of his way to appoint lots of women to positions of power was something that was held against him.][The point was raised during discussion that to a certain extent it’s not clear why they bother with this two-level approach. How large could the number of non-major party voters be? In any case, it introduces a lot, and I’m not sure the trade-off is there.] \\[\nPr\\left(Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) =\\\\\n\\mbox{logit}^{-1}\\left(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\\\\n\\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + \\alpha_{j[i]}^{\\mbox{age}}+ \\alpha_{j[i]}^{\\mbox{race}}+ \\alpha_{j[i]}^{\\mbox{party ID}}+\\\\ \\alpha_{j[i]}^{\\mbox{ideology}}+ \\alpha_{j[i]}^{\\mbox{last vote}}\n\\right)\n\\]\nThe priors on the coefficients for each variable - “var” - are given by independent distributions: \\(N(0, \\sigma^2_{var})\\)14 and the variance parameters are assigned a hyperprior distribution: \\(\\sigma^2_{var}\\sim \\mbox{inv-}\\chi^2(\\nu,\\sigma^2_0)\\).15\nHere, again, the authors go to some effort to ‘sell’ the MRP approach by making explicit the notion of sparse cells borrowing strength. The idea is that if some cell has very little information then it’s coefficients will be drawn from an average of those cells that are similar.\nAfter establishing whether a person is likely to vote for one of Obama or Romney, they use a very similar model to consider whether, given a person is voting for one of those two, a person is voting for Obama: \\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) =\\\\\n\\mbox{logit}^{-1}\\left(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\\\\n\\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + \\alpha_{j[i]}^{\\mbox{age}}+ \\alpha_{j[i]}^{\\mbox{race}}+ \\alpha_{j[i]}^{\\mbox{party ID}}+\\\\ \\alpha_{j[i]}^{\\mbox{ideology}}+ \\alpha_{j[i]}^{\\mbox{last vote}}\n\\right)\n\\]\nAll of this is run in R (R Core Team, 2019) using ‘approximate marginal maximum likelihood estimates’ via glmer() from the lme4 package (Bates, et al, 2015).16 17 18 The estimate for each day is run on the basis of that day’s results as well as the previous four, hence introducing a degree of smoothing.19\nAfter this model is trained it is applied to the post-stratification dataset.20 Estimates are made for each cell and then aggregated up to the population based on the weight of the proportion of the electorate in each cell.\nResults - Voter intention\nOverall results\nFigure 6 compares the MRP estimate (red line) with a pollster average (blue line). The vertical dotted lines are presidential debates, the horizontal dashed line is the actual outcome.\n\n\n\nFigure 6: (This is Figure 3 in the paper.) ‘National MRP-adjusted voter intent of two-party Obama support over the 45-day period, with the associated 95% confidence bands. The horizontal dashed line indicates the actual two-party Obama vote share. The three vertical dotted lines indicate the presidential debates. Compared with the raw responses in Fig. 2, the MRP-adjusted voter intent is much more reasonable, and the voter intent in the last few days is close to the actual outcome. On the other hand, the daily aggregated polling results from Pollster.com, shown by the blue dotted line, are further away from the actual vote share than the estimates generated from the Xbox data in the last few days.’\n\n\n\nWith relation to Figure 6, the authors argue:21\n\nOn the day before the election, our estimate of voter intent is off from the actual outcome (indicated by the dotted horizontal line) by a mere 0.6 percentage points.\n\nState-specific results\nThey then disaggregate the national-level vote share into states (Figure 7). This has important considerations for the issue of importance - electoral college results. This disaggregation illustrates an important feature of MRP - that state-specific estimates can be obtained by using a state-specific post-stratification dataset. The trained model remains the same, it is only the post-stratification that changes.\n\n\n\nFigure 7: (This is Figure 4 in the paper.) ‘MRP-adjusted daily voter intent for the 12 states with the most electoral votes, and the associated 95% confidence bands. The horizontal dashed lines in each panel give the actual two-party Obama vote shares in that state. The mean and median absolute errors of the last day voter intent across the 51 Electoral College races are 2.5 and 1.8 percentage points, respectively. The state-by-state daily aggregated polling results from Pollster.com, given by the dotted blue lines, are broadly consistent with the estimates from the Xbox data.’\n\n\n\nFigure 7 illustrates the twelve states with the most electoral college votes. The authors argue that although there are similar trends between the states, there are state-specific movements, which speak to a blending of national and state-level signals. Again, the blue dotted line is a pollster average, the red dotted line is the MRP estimates, the horizontal dashed line is the eventual outcome, and the vertical dotted line is the debates.\nDemographic-specific results\nSimilarly, demographic-specific estimates can be estimated by re-weighting the post-stratification dataset. Figure 8 illustrates some important demographic features.22\n\n\n\nFigure 8: (This is Figure 5 in the paper.) ‘Comparison of the two-party Obama vote share for various demographic subgroups, as estimated from the 2012 national exit poll and from the Xbox data on the day before the election.’\n\n\n\nFinally, the results can be compared with the actual results, say on the basis of the most-important two-dimensional demographic sub-groups (left-panel of Figure 9) or illustrating their size (right panel of Figure 9).23\n\n\n\nFigure 9: (This is Figure 6 in the paper.) ‘Left panel: Differences between the Xbox MRP-adjusted estimates and the exit poll estimates for the 30 largest two-dimensional demographic subgroups, ordered by the differences. Positive values indicate that the Xbox estimate is larger than the corresponding exit poll estimate. Among these 30 subgroups, the median and mean absolute differences are 1.9 and 2.2 percentage points, respectively. Right panel: Two-party Obama support, as estimated from the 2012 national exit poll and from the Xbox data on the day before the election, for various two-way interaction demographic subgroups (e.g., 65+ year-old women). The sizes of the dots are proportional to the population sizes of the corresponding subgroups.’\n\n\n\nResults - Election outcomes\nThe survey question asked ‘if the election were held today’, which has been found to be biased in various ways. Additionally, not everyone who is able to vote actually votes. As such there is a need to translate estimates of voter intentions into estimates of election outcomes. The authors describe this as ‘calibrating’ voter intent.\nTo calibrate voter intent, they get historical data from three US presidential elections (2000, 2004, and 2008) in terms of overall national and state estimates of voter intent. They then take a moving average of the poll numbers leading up to the election. The point is to try to establish a relationship between the poll estimates and the actual outcome. The moving average of voting intent, as measured by the polls, is used as an explanatory variable for the national election day vote share of the incumbent party candidate in election year \\(e\\):24 \\[y^{US}_e = a_0 + a_1x^{US}_{t,e} + a_2|x^{US}_{t,e}|x^{US}_{t,e} + a_3tx^{US}_{t,e} + \\eta(t, e).\\] Where \\(x^{US}_{t,e}\\) is the national voter intent of the incumbent party candidate \\(t\\) days before the election in year \\(e\\), and \\(\\eta\\sim N(0,\\sigma^2)\\) is the error term.25 Both \\(y^{US}_e\\) and \\(x^{US}_{t,e}\\) are offset by 0.5 so that they run from -0.5 to 0.5.26 The fully calibrated model is run with the gls() function in the R package nlme (Pinheiro, et al 2019).\nThe authors also do the same but for state-specific election outcomes.\nState-by-state outcomes\nFigure 10 shows the estimates of the calibrated model, that is, it projects Obama’s vote share in each of the twelve largest states.\n\n\n\nFigure 10: (This is Figure 7 in the paper.) ‘Projected Obama share of the two-party vote on election day for each of the 12 states with the most electoral votes, with the associated 95% confidence bands. Compared to the MRP-adjusted voter intent in Fig. 4, the projected two-party Obama support is more stable, and the North Carolina race switches direction after applying the calibration model. In addition, the confidence bands become much wider and give more reasonable state-by-state probabilities of Obama victories.’\n\n\n\nThe issue is that it is difficult to know whether this result is reasonable - what is the appropriate comparison?27 For this reason, the authors convert the vote share estimates into probabilistic forecasts and then compare them to prediction market estimates (Figure 11).\n\n\n\nFigure 11: (This is Figure 8 in the paper.) ‘Comparison between the probabilities of Obama winning the 12 largest Electoral College races based on Xbox and prediction market data. The prediction market data are the average of the raw Betfair and Intrade prices from winner-take-all markets. The three vertical lines represent the dates of the three presidential debates. The shaded halves indicate the direction in which race went.’\n\n\n\nThey find that their probabilistic estimates are consistent with prediction markets.\nImplied electoral college outcomes\nFinally, given state estimates, it is possible to provide an estimate of the aspect of presidential elections that actually matters: electoral college outcomes. The authors begin by examining the median distribution of Electoral College votes (Figure 12).\n\n\n\nFigure 12: (This is Figure 9 in the paper.) ‘Daily projections of Obama electoral votes over the 45-day period leading up to the 2012 election, with the associated 95% confidence bands. The solid line represents the median of the daily distribution. The horizontal dashed line represents the actual electoral votes, 332, that Obama captured in 2012 election. The three vertical dotted lines indicate the dates of the three presidential debates.’\n\n\n\nTheir central estimate is quite similar to the eventual outcome. But Figure 12 also makes the MRP trade-off clear, as the distribution of the estimates is very wide and covers almost all possible outcomes.28\nThey also, more clearly illustrate the distribution of potential outcomes in the electoral college (Figure 13).\n\n\n\nFigure 13: (This is Figure 10 in the paper.) ‘The projected distribution of electoral votes for Obama one day before the election. The green vertical dotted line represents 269, the minimum number of electoral votes that Obama needed for a tie. The blue vertical dashed line indicates 332, the actual number of electoral votes captured by Obama. The estimated likelihood of Obama winning the electoral vote is 88%.’\n\n\n\nThey find that ‘[w]hile Obama actually captured 332 votes, we estimate a median of 312 votes, with the most likely outcome being 303.’29\nConclusions\nThe paper closes with a discussion of various properties that one would like elections to have: ‘not only accurate, but also relevant, timely, and cost-effective’. The authors then point out how their approach satisfies these properties, especially through the use of non-representative samples.30\nThey point to the possible use of non-representative polling in local elections, and close where they began - with a recommendation that non-representative polling be considered in a new light, despite its failure 75 years ago.\nReferences\nBates, Douglas, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. doi:10.18637/jss.v067.i01.\nGosnell, H. F. (1937). How accurate were the polls? Public Opinion Quarterly, 1, 97–105.\nHanretty, C. (2019). An introduction to multilevel regression and post-stratification for estimating constituency opinion. Political Studies Review, pages 1–16.\nKastellec, J. P., Lax, J. R., and Phillips, J. (2016). Estimating State Public Opinion With Multi-Level Regression and Poststratification using R. Working Paper.\nPinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2019). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-143, <URL: https://CRAN.R-project.org/package=nlme>.\nR Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nSquire, P. (1988). Why the 1936 Literary Digest poll failed. Public Opinion Quarterly, 52, 125–133.\nWang, W., Rothschild, D., Goel, S., & Gelman, A. (2015). Forecasting elections with non-representative polls. International Journal of Forecasting, 31(3), 980-991.\nThis paper literally changed the course of my professional life. I read it in late 2015 when I was very bored by my economics PhD. I couldn’t put it down. It seems silly now, but until then I didn’t realise that statisticians wrote papers like this, and that not only could you bring together the best aspects of politics, surveys, quantitative analysis, etc but that you could also write academic papers about it! It was the combination of everything that I was interested in. Starting in early 2016 Monica Alexander and I followed this paper and Monica implemented a version of the model in JAGS for Australia, which become: Petit Poll. We gathered our own polling data by emailing friends and family and used all of this to forecast the 2016 Australian election. (We forecast the Coalition would 80-85 seats, and they eventually won 76 seats - the electoral divisions of Bass, Braddon, Lyons, and Mayo haunt my dreams.) We went on to use MRP for various other projects, improving our understanding of it and related methods. Monica was already there, but I dove further into learning R, Bayesian statistics, and machine learning, focused on applications in political science, eventually also branching out into related techniques such as text analysis. Now almost all of my work is very much statistics applied to political science, but it all started with this paper.↩\nOne of the interesting aspects around this introduction and motivation section is that there is little discussion of the usual aspects that those who are already convinced of the MRP approach get excited about. Features such as the importance of probability distributions, and the appropriate role of uncertainty, which those of us who use MRP (and related approaches) day-to-day see as motivation, are of less concern to a broader audience. By carefully constructing the introduction and motivation section, the authors likely greatly increased the number of readers who continue through the rest of the paper.↩\nThe first use of mathematical notation does not occur until the end of the third page. This also likely helps boost the readership of this paper!↩\nI stole this name from Jack Bailey. FWIW Stan existed at this point, but had not taken off to the extent it would in the following few years.↩\nIt doesn’t really matter, but why was this number rounded, while the others are exact?↩\nThe point was raised during discussion that it would have been interesting to know the number of those who changed their mind. My experience with Australian data (as well as talking to people who are much more experienced in this area than I am) suggests there would have been very few.↩\nMonica and I have submitted an application to present a paper about this topic at a super fantastic MRP-fest being organised by Lauren Kennedy and held at Columbia in April 2020. Hopefully our proposal is accepted!↩\nThe author’s describe multi-level modelling as regularised regression. I’d be keen to explore this in a little more detail.↩\nWe’ve seen in earlier papers in this reading group that we can see echoes (that’s the wrong word; what’s the antecedent of an echo?) in much earlier work including Fay and Herriot (1979) and Little (1993) and probably a bunch of other papers. The definitive MRP history paper is probably not yet written. Lauren - are you taking submissions for MRP-fest 2021 or maybe 2022?↩\nGiven how correlated and overlapping some of these variables seem (e.g. party ID, ideology, and 2008 vote), this discussion is not as extensive as I would like. As a reviewer of the paper, I would be interested in examining the performance of a ‘cut down’ version of the model. This would have also helped with some of the dataset issues raised later.↩\n1. This seems like something that could have a big effect or at least needs a lot of justification. I would have liked to have seen much more justification of this. If I were a reviewer then I would have requested they run the model using CPS and throw away whatever variables that it doesn’t have from their model. They only mention one variable, so I just can’t see why it is worth taking the hit in being so unconventional. 2. This decision also limits the capacity of others to reproduce the results. Anyone can get CPS data. But how do I get the exit poll data that they used? What exit polls did they even use? Did I miss this? The post-stratification dataset is kind of everything.↩\nI couldn’t work out which exit poll they used? I probably just missed a description of it, but even so, I also couldn’t see that they provide the data, which would have really helped improve the reproducibility of the paper.↩\nHowever this would be another benefit of using the CPS dataset, as it is run monthly.↩\nWould the authors would still use these priors or has the state-of-the-art changed?↩\nAgain, is this what we’d still do today?↩\nMaybe I missed it but they don’t provide their code. This was very annoying in 2016 when Monica and I were trying to make our own version of the model. But these days it really limits the paper. Other ‘beginner-friendly’ MRP papers, such as Kastellec et al., 2016, or Hanretty, 2019, include all of their code which make them easier to assign when teaching.↩\nWhat effect does using glmer() have on the results? I’m assuming these days this would be fit in Stan, but how easy would it be? What do we gain?↩\nIt wasn’t clear to me whether all of this can be run at once or do they need to run each separately? If the later, how is uncertainty being propagated between the two? Does it matter? I’m sure there are good answers here that I’m just not seeing, but I can’t see it.↩\nBest I can tell each of these rolling averages for a day is run independently. Why not bring it all together?↩\nSomehow you need to bring both models to the table here, but I don’t understand whether each model is applied separately in which case what is happening to the uncertainty, or if it is possible to somehow combine it all using lme4?↩\nThey show how close their results are to the actual outcome, but at the end acknowledge that this probably isn’t the best way to evaluate MRP models and discuss other aspects. What does a ‘meaningful result’ mean in the context of MRP?↩\nthe point was raised during discussion that there should probably be some measure of uncertainty around these estimates.↩\nWe’ve talked before about these types of graphs, and how they may be better with difference between ‘actual’ and ‘estimated’ on the y-axis, while ‘actual’ remains on the x-axis.↩\nI don’t understand what has happened to uncertainty. Those poll numbers would have had an uncertainty surrounding them, but I didn’t see a mention of this. Maybe they just used the central estimate, in which case this relationship may be overly precisely measured.↩\nI don’t understand what is happening in that \\(a_2\\) coefficient, or really much of any of it. Is there a subtle difference that I’m missing? It’s the same in the states model. I understand what they are trying to do, but I don’t understand how this achieves it.↩\nI don’t understand why this is done.↩\nI’m not sure that their comparison - either here or above - is appropriate at all. By what do we evaluate an MRP model? I have thoughts on this, but am keen to develop them further.↩\nAt no point is this result really all that meaningful. Is there something better that could be done here?↩\nAgain, is this really how we should be analysing the success of our models? In any case, the point was raised during discussion that claiming that the mode is informative here is a little disengenious. The ‘second mode’ (is that a thing?), anyway, there’s not much different between the probability of any of the top five more likely outcomes, but they are enormously spread out.↩\nI would have liked more discussion about these aspects by which to evaluate an MRP model.↩\n",
    "preview": "posts/2020-02-11-a-review-of-forecasting-elections-with-non-representative-polls/page_1.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1168,
    "preview_height": 1594
  },
  {
    "path": "posts/2019-12-04-getting_started_with_mrp/",
    "title": "Getting started with MRP",
    "description": "Multi-level regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. I recently ran a hands-on workshop at the ANU, aimed at interested, but not experienced, social scientists to help de-mystify MRP. The workshop aimed to give participants the ability and confidence to: 1) critically read papers that use it; and 2) apply it in their own work. Examples of how to implement MRP were illustrated in R using the brms package. The following post gives the outline of the workshop and the material and coding exercises covered.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2019-12-03",
    "categories": [],
    "contents": "\nTable of Contents\nOverview\nSchedule\nHelp with computer set-up.Computing\nGetting help\n\nIntroduction, motivation, and example\nLive-coding introductory example\nParticipants pair-code introductory example\nLive coding extended example\nParticipants pair-code extended example\nLive codingAdding layers\nGathering data\nCommunication\n\nConcluding remarks\nNext steps\nOverview\nMulti-level regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.\nThe purpose of this hands-on workshop is to de-mystify MRP and give participants the ability and confidence to: 1) critically read papers that use it; and 2) apply it in their own work. Examples of how to implement MRP will be illustrated in R using the brms package. No experience with R is required but workshop participants should bring a laptop that is: a) connected to the internet; b) has R and R Studio installed, along with the tidyverse and brms packages (if you have a hassle doing this then come early to the workshop and I can help you).\nThe GitHub repo that you should download is: https://github.com/RohanAlexander/mrp_workshop.\nSchedule\n8:45 - 9:00: (Optional) Help with computer set-up.\n9:00 - 9:15: Introduction, motivation, and example.\n9:15 - 9:25: Live-coding introductory example.\n9:25 - 9:45: Participants pair-code introductory example.\n9:45 - 9:55: Live coding extended example.\n10:00 - 10:30: Participants pair-code extended example.\n10:30 - 10:50: Live example improving the workflow: gathering data from the ABS, improving the model, and communicating results.\n10:50 - 11:00: Concluding remarks about strengths, weaknesses, and potential areas of application.\nHelp with computer set-up.\nThe primary programming language used for MRP tends to be R, but any similar language would be fine. That said, if you are already comfortable with another open source language, such as Python, then it wouldn’t hurt to learn R as well. You are welcome to use whatever language you are most comfortable with, but it will be easiest for you to be able to draw on other examples if you use R. All of the examples in this workshop are in R.\nComputing\nR can be downloaded for free from: http://cran.utstat.utoronto.ca/.\nRStudio is an interface that makes using R easier and it can be downloaded for free from: https://rstudio.com/products/rstudio/download/.\nWe will use brms later in the tutorial. In order to use this your Mac needs to have Xcode and a bunch of other things installed. To do this go to: https://github.com/rmacoslib/r-macos-rtools#how-do-i-use-the-installer and within the ‘assets’ bit of the project’s release page, download ‘macos-rtools-3.1.0.pkg’ and then install that. It’ll take a few minutes because it is downloading and setting up a bunch of things.\nGetting help\nAt some point your code won’t run or will throw an error. This is normal, and it happens to everyone. It happens to me on a daily, sometimes hourly, basis. Getting frustrated is understandable. There are a few steps that are worthwhile taking when this happens:\nIf you are having issues with a particular function then the Help file for that function can be accessed by adding a ? to the front. e.g. ‘?lm’.\nIf you’re getting an error then try googling it, (I find it can help to include the term ‘R’ and ‘MRP’ or ‘tidyverse’ or the relevant package name).\nIf your code just isn’t running, then try searching for what you are trying to do, e.g. ‘save PDF of graph in R made using ggplot’. Almost always there are relevant blog posts or Stack Overflow answers that will help.\nTry to restart R and R Studio and load everything again.\nTry to restart your computer.\nThere are a few small mistakes that I often make and may be worth checking in case you make them too:\ncheck the class e.g. class(my_dataset$its_column) to make sure that is what it should be;\nwhen you’re using ggplot make sure you use ‘+’ not ‘%>%’;\ncheck whether you are using ‘.’ when you shouldn’t be, or vice versa.\nIt’s almost always helpful to take a break and come back the next day.\nIntroduction, motivation, and example\nMulti-level regression with post-stratification (MRP) is a handy approach when dealing with survey data. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:\nIt can allow us to ‘re-weight’ in a way that includes uncertainty front-of-mind and isn’t hamstrung by small samples.\nIt can allow us to use broad surveys to speak to subsets.\nFrom a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As Lauren Kennedy points out, ‘MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we’re not sure of the limitations at the moment.’\nOne famous example is Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, 2014, ‘Forecasting elections with non-representative polls’, International Journal of Forecasting. They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election.\nKey facts about the set-up:\nData from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election.\nEach day there were three to five questions, including voter intention: “If the election were held today, who would you vote for?”\nRespondents were allowed to answer at most once per day.\nFirst-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.\nIn total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls\nYoung men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\nGiven the US electorate, they use a two-stage modelling approach. The details don’t really matter too much, and essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc: \\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}}...\n)\n\\] They run this in R using glmer() from lme4.\nHaving a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these “cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).”\nThis means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not an option for Australia in general).\nThey make state-specific estimates by post-stratifying to the features of each state. \nSimilarly, they can examine demographic-differences. \nFinally, they convert their estimates into electoral college estimates. \nLive-coding introductory example\nThe workflow that we are going to use is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data; and\napply the model to the post-stratification data.\nFirst load the packages.\n\n\n# Uncomment these (by deleting the #) if you need to install the packages\n# install.packages(\"broom\")\n# install.packages(\"here\")\n# install.packages(\"skimr\")\n# install.packages(\"tidyverse\")\n\nlibrary(broom) # Helps make the regression results tidier\nlibrary(here) # Helps make file referencing easier.\nlibrary(skimr) # Helps summarise the data\nlibrary(tidyverse) # Helps make programming with R easier\n\nThen load some sample polling data to analyse. I have generated this fictitious data so that we have some idea of what to expect from the model. The dependent variable is supports_ALP, which is a binary variable - either 0 or 1. We’ll just use two independent variables here: gender, which is either Female or Male (as that is what is available from the ABS); and age_group, which is one of four groups: ages 18 to 29, ages 30 to 44, ages 45 to 59, ages 60 plus.\n\n\nexample_poll <- read_csv(\"outputs/data/example_poll.csv\") # Here we read in a \n# CSV file and assign it to a dataset called 'example_poll'\n\nhead(example_poll) # Displays the first 10 rows\n\n# A tibble: 6 x 4\n  gender age_group  supports_ALP state\n  <chr>  <chr>             <dbl> <chr>\n1 Male   ages30to44            0 NSW  \n2 Female ages45to59            0 NSW  \n3 Female ages60plus            1 VIC  \n4 Male   ages30to44            1 QLD  \n5 Female ages30to44            1 QLD  \n6 Female ages18to29            1 VIC  \n\n# Look at some summary statistics to make sure the data seem reasonable\nsummary(example_poll) \n\n    gender           age_group          supports_ALP   \n Length:5000        Length:5000        Min.   :0.0000  \n Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Median :1.0000  \n                                       Mean   :0.5514  \n                                       3rd Qu.:1.0000  \n                                       Max.   :1.0000  \n    state          \n Length:5000       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nskimr::skim(example_poll)\n(#tab:initial_model_simulate_data)Data summary\nName\nexample_poll\nNumber of rows\n5000\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n1\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ngender\n0\n1\n4\n6\n0\n2\n0\nage_group\n0\n1\n10\n10\n0\n4\n0\nstate\n0\n1\n2\n3\n0\n8\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nsupports_ALP\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\nI generated this polling data to make both made males and older people less likely to vote for the Australian Labor Party; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset.\n\n\n# The '%>%' is called a 'pipe' and it takes whatever the output is of the \n# command before it, and pipes it to the command after it.\nexample_poll %>% # So we are taking our example_poll dataset and using it as an \n  # input to 'summarise'.\n   # summarise reduces the dimensions, so here we will get one number from a column.\n  summarise(raw_ALP_prop = sum(supports_ALP) / nrow(example_poll))\n\n# A tibble: 1 x 1\n  raw_ALP_prop\n         <dbl>\n1        0.551\n\nNow we’d like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is: \\[\n\\mbox{ALP support}_j = \\mbox{gender}_j + \\mbox{age_group}_j + \\epsilon_j.\n\\]\nThis model says that the probability that some person, \\(j\\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party.\n\n\n# Here we are running an OLS regression with supports_ALP as the dependent variable \n# and gender and age_group as the independent variables. The dataset that we are \n# using is example_poll. We are then saving that OLS regression to a variable called 'model'.\nmodel <- lm(supports_ALP ~ gender + age_group, \n            data = example_poll\n            )\n\n# broom::tidy just displays the outputs of the regression in a nice table.\nbroom::tidy(model) \n\n# A tibble: 5 x 5\n  term                estimate std.error statistic   p.value\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            0.900    0.0131      68.8 0.       \n2 genderMale            -0.205    0.0142     -14.4 2.69e- 46\n3 age_groupages30to44   -0.186    0.0176     -10.6 6.50e- 26\n4 age_groupages45to59   -0.402    0.0177     -22.7 8.29e-109\n5 age_groupages60plus   -0.585    0.0175     -33.4 5.20e-221\n\nEssentially we’ve got our inputs back. We just used regular OLS even though our dependent variable is a binary. (It’s usually fine to start with an OLS model and then iterate toward an approach that may be more appropriate such as logistic regression or whatever, but where the results are a little more difficult to interpret.) If you wanted to do that then the place to start would be glmer() from the R package lme4, and we’ll see that in the next section.\nMonica is horrified by the use of OLS here, and wants it on the record that she regrets not making not doing this part of our marriage vows.\nNow we’d like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features.\nFirst read in some real demographic data, on a seat basis, from the ABS (we’ll go into the process of getting this later).\n\n\ncensus_data <- read_csv(\"outputs/data/census_data.csv\")\nhead(census_data)\n\n# A tibble: 6 x 5\n  state gender age_group  number cell_prop_of_division_total\n  <chr> <chr>  <chr>       <dbl>                       <dbl>\n1 ACT   Female ages18to29  34683                       0.125\n2 ACT   Female ages30to44  42980                       0.155\n3 ACT   Female ages45to59  33769                       0.122\n4 ACT   Female ages60plus  30322                       0.109\n5 ACT   Male   ages18to29  34163                       0.123\n6 ACT   Male   ages30to44  41288                       0.149\n\nWe’re just going to do some rough forecasts. For each gender and age_group we want the relevant coefficient in the example_data and we can construct the estimates.\n\n\n# Here we are making predictions using our model with some new data from the \n# census, and we saving the results of those predictions by adding a new column \n# to the census_data dataset called 'estimate'.\ncensus_data$estimate <- \n  model %>% \n  predict(newdata = census_data)\n\ncensus_data %>% \n  mutate(alp_predict_prop = estimate*cell_prop_of_division_total) %>% \n  group_by(state) %>% \n  summarise(alp_predict = sum(alp_predict_prop))\n\n# A tibble: 8 x 2\n  state alp_predict\n  <chr>       <dbl>\n1 ACT         0.525\n2 NSW         0.495\n3 NT          0.541\n4 QLD         0.496\n5 SA          0.479\n6 TAS         0.464\n7 VIC         0.503\n8 WA          0.503\n\nWe now have post-stratified estimates for each division. Our model has a fair few weaknesses. For instance small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it.\nParticipants pair-code introductory example\nPlease break into pairs and with one person ‘driving’ (typing) and the other person ‘navigating’, and attempt to pair-code the introductory example.\nIf you run into issues then I am happy to help point you in the right direction. The full code of the example will be made available after the workshop, so it doesn’t matter if you’re not able to complete the example now.\nAs a reminder, our workflow is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data;\napply your model to the post-stratification data.\nGet started by opening the Rproj file from the workshop repo and opening a new R script.\nLive coding extended example\nWe’d like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are crticial, and it’s not appropriate to only report central estimates. To do this we’ll use the same broad approach as before, but just improving bits of our workflow.\nFirst load the packages.\n\n\n# Uncomment these if you need to install the packages\n# install.packages(\"broom\")\n# install.packages(\"brms\")\n# install.packages(\"here\") \n# install.packages(\"tidybayes\")\n# install.packages(\"tidyverse\") \n\nlibrary(broom)\nlibrary(brms) # Used for the modelling\nlibrary(here)\nlibrary(tidybayes) # Used to help understand the modelling estimates\nlibrary(tidyverse) \n\nAs before, read in the polling dataset.\n\n\nexample_poll <- read_csv(\"outputs/data/example_poll.csv\")\n\nhead(example_poll)\n\n# A tibble: 6 x 4\n  gender age_group  supports_ALP state\n  <chr>  <chr>             <dbl> <chr>\n1 Male   ages30to44            0 NSW  \n2 Female ages45to59            0 NSW  \n3 Female ages60plus            1 VIC  \n4 Male   ages30to44            1 QLD  \n5 Female ages30to44            1 QLD  \n6 Female ages18to29            1 VIC  \n\nNow, using the same basic model as before, but we move it to a setting that acknowledges the dependent variable as being binary, and in a Bayesian setting.\n\n\nmodel <- brm(supports_ALP ~ gender + age_group, \n             data = example_poll, \n             family = bernoulli(),\n             file = \"outputs/model/brms_model\"\n             )\n\nmodel <- read_rds(\"outputs/model/brms_model.rds\")\n\nsummary(model)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: supports_ALP ~ gender + age_group \n   Data: example_poll (Number of observations: 5000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept               2.07      0.09     1.91     2.23 1.00\ngenderMale             -1.06      0.07    -1.20    -0.91 1.00\nage_groupages30to44    -1.10      0.10    -1.29    -0.91 1.00\nage_groupages45to59    -2.04      0.10    -2.23    -1.85 1.00\nage_groupages60plus    -2.88      0.10    -3.09    -2.68 1.00\n                    Bulk_ESS Tail_ESS\nIntercept               2240     2194\ngenderMale              3403     2595\nage_groupages30to44     2483     2805\nage_groupages45to59     2521     3061\nage_groupages60plus     2517     2858\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe’ve moved to the Bernoulli distribution, so we have to do a bit more work to understand our results, but we are broadly getting back what we’d expect.\nAs before, we’d like an estimate for each state based on their demographic features and start by reading in the data.\n\n\ncensus_data <- read_csv(\"outputs/data/census_data.csv\")\nhead(census_data)\n\n# A tibble: 6 x 5\n  state gender age_group  number cell_prop_of_division_total\n  <chr> <chr>  <chr>       <dbl>                       <dbl>\n1 ACT   Female ages18to29  34683                       0.125\n2 ACT   Female ages30to44  42980                       0.155\n3 ACT   Female ages45to59  33769                       0.122\n4 ACT   Female ages60plus  30322                       0.109\n5 ACT   Male   ages18to29  34163                       0.123\n6 ACT   Male   ages30to44  41288                       0.149\n\nWe’re just going to do some rough forecasts. For each gender and age_group we want the relevant coefficient in the example_data and we can construct the estimates (this code is from Monica Alexander, https://www.monicaalexander.com/posts/2019-08-07-mrp/).\n\n\npost_stratified_estimates <- \n  model %>% \n  tidybayes::add_predicted_draws(newdata = census_data) %>% \n  rename(alp_predict = .prediction) %>% \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %>% \n  group_by(state, .draw) %>% \n  summarise(alp_predict = sum(alp_predict_prop)) %>% \n  group_by(state) %>% \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n\npost_stratified_estimates\n\n# A tibble: 8 x 4\n  state  mean lower upper\n  <chr> <dbl> <dbl> <dbl>\n1 ACT   0.526 0.243 0.791\n2 NSW   0.491 0.214 0.750\n3 NT    0.538 0.236 0.852\n4 QLD   0.493 0.215 0.762\n5 SA    0.478 0.201 0.761\n6 TAS   0.460 0.183 0.757\n7 VIC   0.501 0.224 0.766\n8 WA    0.506 0.219 0.769\n\nWe now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we’d need to get new cell counts), or adding some layers.\nParticipants pair-code extended example\nPlease break into the same pairs as before, but swap who is typing, and attempt to pair-code the extended example.\nIf you run into issues then I am happy to help point you in the right direction. The full code of the example will be made available after the workshop, so it doesn’t matter if you’re not able to complete the example now.\nAs a reminder, our workflow is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data;\napply your model to the post-stratification data.\nLive coding\nI will now briefly demonstrate some other aspects that may be useful to improve three aspects of our MRP workflow:\n(Workflow step 2) adding some more complexity to our model; and\n(Workflow step 3) gathering and preparing some data from the ABS that we could use to post-stratify on.\nWe will also add a fifth stage to our workflow: Communicating our results.\nAdding layers\nWe may like to try to add some layers to our model. For instance, we may like a different intercept for each state.\n\n\nmodel_states <- brm(supports_ALP ~ gender + age_group + (1|state), \n                    data = example_poll, \n                    family = bernoulli(),\n                    file = \"outputs/model/brms_model_states\",\n                    control = list(adapt_delta = 0.90)\n                    )\nsummary(model_states)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: supports_ALP ~ gender + age_group + (1 | state) \n   Data: example_poll (Number of observations: 5000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~state (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)     0.06      0.05     0.00     0.20 1.00     1553\n              Tail_ESS\nsd(Intercept)     2072\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept               2.07      0.09     1.90     2.26 1.00\ngenderMale             -1.06      0.08    -1.21    -0.91 1.00\nage_groupages30to44    -1.10      0.10    -1.30    -0.90 1.00\nage_groupages45to59    -2.04      0.10    -2.24    -1.84 1.00\nage_groupages60plus    -2.89      0.10    -3.10    -2.69 1.00\n                    Bulk_ESS Tail_ESS\nIntercept               1660     2273\ngenderMale              4106     2833\nage_groupages30to44     2110     2566\nage_groupages45to59     2058     2347\nage_groupages60plus     2201     2581\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nbroom::tidy(model_states, par_type = \"varying\")\n\n       term group level      estimate  std.error       lower\n1 Intercept state   ACT -0.0168699419 0.07074301 -0.14969628\n2 Intercept state   NSW  0.0017210052 0.04612268 -0.07223933\n3 Intercept state    NT  0.0153301257 0.07586938 -0.08509056\n4 Intercept state   QLD  0.0249702604 0.05321612 -0.04216737\n5 Intercept state    SA  0.0150266070 0.05864962 -0.06881398\n6 Intercept state   TAS -0.0146658602 0.07317706 -0.14542943\n7 Intercept state   VIC -0.0263030593 0.05188695 -0.12296610\n8 Intercept state    WA -0.0006364996 0.05548879 -0.09364575\n       upper\n1 0.07652361\n2 0.07734117\n3 0.14549805\n4 0.12297713\n5 0.12224758\n6 0.08379179\n7 0.04212995\n8 0.08755071\n\nbroom::tidy(model_states, par_type = \"non-varying\", robust = TRUE)\n\n                 term  estimate  std.error     lower      upper\n1           Intercept  2.069011 0.08988424  1.919983  2.2237171\n2          genderMale -1.058954 0.07408394 -1.186584 -0.9351159\n3 age_groupages30to44 -1.097002 0.10370072 -1.264609 -0.9257060\n4 age_groupages45to59 -2.038541 0.10100761 -2.205630 -1.8753367\n5 age_groupages60plus -2.884174 0.10356840 -3.063525 -2.7166738\n\nOne interesting aspect is that our multi-level approach will allow us to deal with small cell counts by borrowing information from other cells.\n\n\nexample_poll %>% \n  count(state)\n\n# A tibble: 8 x 2\n  state     n\n  <chr> <int>\n1 ACT     107\n2 NSW    1622\n3 NT       50\n4 QLD     982\n5 SA      359\n6 TAS     105\n7 VIC    1285\n8 WA      490\n\nAt the moment we have 50 respondents in the Northern Territory, 105 in Tasmania, and 107 in the ACT. Even if we were to remove most of the, say, 18 to 29 year old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents.\nGathering data\nGetting data tends to be the most troublesome aspect. I’ve found that the census is fairly useful although it can require some trade-offs (e.g. if you are doing political work then it’s not exactly the same as the electoral roll even if you restrict it to Australian citizens aged at least 18). Nonetheless, I’ve found the best way to get the sub-cell counts is to use ABS TableBuilder. There are two versions - ‘basic’ which is free, and ‘pro’, which is normally $2,510 per year, but which you can get access to if you’re associated with an Australian university.\nTableBuilder front page.Once you create an account then you can access census data for 2006, 2011, and 2016. (The ABS have relatively recently done some linking between censuses so there is actually some linked data, which is exciting).\nTableBuilder after logging in.The website is a bit cumbersome, but considering what they provide it is worth sticking with it. I usually use ‘Counting Persons, Place of Usual Residence’, but sometimes ‘Counting Persons, Place of Enumeration’ is also handy.\nTableBuilder selecting which census.We want to create a new table, and we do this by specifying the columns and rows.\nTableBuilder selecting columns.Once you have the set-up that you want then you can retrieve the data.\nTableBuilder selecting rows.You can download the data in various Excel, CSV, and other formats. If your dataset is large then you may need to submit for it to be built, which can take a day or two. Finally, if your sub-cell counts are especially small, then they will be blown around by the randomness that the ABS adds.\nTableBuilder downloading data and creating custom groups.Helpfully you can create custom groupings for geography, say to load specific electorates, and other aspects, such as age-groups. To get started with this, select ‘Custom data’.\nCommunication\nThere are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate.\n\n\npost_stratified_estimates %>% \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  ylab(\"Proportion ALP support\") + \n  xlab(\"State\") + \n  geom_point(data = example_poll %>% \n               group_by(state, supports_ALP) %>%\n               summarise(n = n()) %>% \n               group_by(state) %>% \n               mutate(prop = n/sum(n)) %>% \n               filter(supports_ALP==1), \n             aes(state, prop, color = \"Raw data\")) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank())\n\n\nSimilarly, we may like to plot the distribution of the coefficients.\n\nYou can work out which coefficients to be pass to gather_draws by using tidybayes::get_variables(model). (In this example I passed ‘b_.’, but the ones of interest to you may be different.)\n\n\nmodel %>%\n  gather_draws(`b_.*`, regex=TRUE) %>%\n  ungroup() %>%\n  mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) %>%\n  mutate(coefficient = forcats::fct_recode(coefficient,\n                                           Intercept = \"Intercept\",\n                                           `Is male` = \"genderMale\",\n                                           `Age 30-44` = \"age_groupages30to44\",\n                                           `Age 45-59` = \"age_groupages45to59\",\n                                           `Age 60+` = \"age_groupages60plus\"\n                                           )) %>% \n\n# both %>% \n  ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n  ggridges::geom_density_ridges2(aes(height = ..density..),\n                                 rel_min_height = 0.01, \n                                 stat = \"density\",\n                                 scale=1.5) +\n  xlab(\"Distribution of estimate\") +\n  ylab(\"Coefficient\") +\n  scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  theme(legend.position = \"bottom\")\n\n\nConcluding remarks\nIn general, MRP is a good way to accomplish specific aims, but it’s not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it’s a great place to start, but it’s not a panacea.\nThere’s not a lot of work that’s been done using Australian data, so there’s plenty of scope for exciting work. I look forward to seeing what you do with it!\nNext steps\nThere are a lot of resources out there that would make great next steps. I recommend having a look at the following resources to see which speaks best to your interests and background.\nAlexander, Monica, 2019, ‘Analyzing name changes after marriage using a non-representative survey’, available at: https://www.monicaalexander.com/posts/2019-08-07-mrp/.\nKennedy, Lauren, and Jonah Gabry, 2019, ‘MRP with rstanarm’, available at: https://mc-stan.org/rstanarm/articles/mrp.html.\nKennedy, Lauren, and Andrew Gelman, 2019, ‘Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample’, available at: https://arxiv.org/abs/1906.11323.\nKastellec, Jonathan, Jeffrey Lax, and Justin Phillips, 2016, ‘Estimating State Public Opinion With Multi-Level Regression and Poststratification using R’, available at: https://scholar.princeton.edu/sites/default/files/jkastellec/files/mrp_primer.pdf.\nHanretty, Chris, 2019, ‘An introduction to multilevel regression and post-stratification for estimating constituency opinion’, available at: https://journals.sagepub.com/doi/abs/10.1177/1478929919864773.\nDownes, Marnie, Lyle Gurrin, Dallas English, Jane Pirkis, Dianne Currier, Matthew Spittal, and John Carlin, 2018, ‘Multilevel Regression and Poststratification: A Modeling Approach to Estimating Population Quantities From Highly Selected Survey Samples’, available at: https://www.ncbi.nlm.nih.gov/pubmed/29635276.\nJackman, Simon, Shaun Ratcliff, and Luke Mansillo, 2019, ‘Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications’, available at: https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf\n(Self-promotion alert) Alexander, Rohan, Patrick Dumont, and Patrick Leslie, 2019, ‘Forecasting Multi-District Election’, available at: https://github.com/RohanAlexander/ForecastingMultiDistrictElections.\nIf you don’t have survey data, then there is some individual-level data available on the Australian Data Archive: https://ada.edu.au. You will need to request access to the datasets, but they are very keen for people to use their data and will help you through the process if needed.\n\n\n",
    "preview": "posts/2019-12-04-getting_started_with_mrp/getting-started-with-mrp_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-01-03-gathering-and-analysing-text-data/",
    "title": "Gathering and analysing text data",
    "description": "Text modelling is an exciting area of research. But many guides assume that you already have a nice dataset. Similarly, web scraping is an exciting way to get information, but not many explanations go on to explain what you could do with it. This post attempts to go from scraping text from a website through to modelling the topics. It's not meant to be an exhaustive post, but should hopefully provide enough that you can get started with your own project and know where to go for more information.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2019-01-03",
    "categories": [],
    "contents": "\nTable of Contents\nIntroduction\nGathering data\nAnalysing data\nIntroduction\nText modelling is an exciting area of research. But many guides assume that you already have a nice dataset. Similarly, web scraping is an exciting way to get information, but not many explanations go on to explain what you could do with it. This post attempts to go from scraping text from a website through to modelling the topics. It’s not meant to be an exhaustive post, but should hopefully provide enough that you can get started with your own project and know where to go for more information.\nThe example that I’m going to use is getting data from the minutes of the RBA board meeting.\nGathering data\nThe first step is to get some data. I’m going to use the rvest package to do the web scraping. When you are scraping data you should try to be polite - slow down your requests as much as possible, avoid times you know they’ll have a lot of traffic, and check if the website has an API or a robots.txt file (usually access that at domain.com/robots.txt) that provides guidance.\n\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# Read in the list of the website addresses\ndata_to_scrape <- read_csv(\"inputs/addresses.csv\") # Just a list of the URLs \n# and dates for each minutes.\naddress_to_visit <- data_to_scrape$address\nsave_name <- data_to_scrape$save_name\n\n# Create the function that will visit address_to_visit and save to save_name files\nvisit_address_and_save_content <-\n  function(name_of_address_to_visit,\n           name_of_file_to_save_as) {\n    # The function takes two inputs\n    read_html(name_of_address_to_visit) %>% # Go to the website and read the html\n      html_node(\"#content\") %>% # Find the content part\n      html_text() %>% # Extract the text of the content part\n      write_lines(name_of_file_to_save_as) # Save as a text file\n    print(paste(\"Done with\", name_of_address_to_visit, \"at\", Sys.time()))  \n    # Helpful so that you know progress when running it on all the records\n    Sys.sleep(sample(30:60, 1)) # Space out each request by somewhere between \n    # 30 and 60 seconds each so that we don't overwhelm their server\n  }\n\n# If there is an error then ignore it and move to the next one\nvisit_address_and_save_content <-\n  safely(visit_address_and_save_content)\n\n# Walk through the addresses and apply the function to each\nwalk2(address_to_visit,\n      save_name,\n      ~ visit_address_and_save_content(.x, .y)) \n\nThe CSV with the addresses and save names that we use looks something like this:\n\naddress\nsave_name\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-11-06.html\ninputs/minutes/2018-11-06.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-10-02.html\ninputs/minutes/2018-10-02.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-09-04.html\ninputs/minutes/2018-09-04.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-08-07.html\ninputs/minutes/2018-08-07.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-07-03.html\ninputs/minutes/2018-07-03.txt\n\nAnalysing data\nIn this example we’ll use a whole bunch of packages so that you can see what’s available. In general probably stringr, quanteda and stm are the workhorse packages with others used as needed.\n\n\n#### Workspace set-up ####\n# install.packages(\"broom\")\nlibrary(broom) # Used to clean up results\n# install.packages(\"devtools\")\nlibrary(devtools)\n# devtools::install_github(\"DavisVaughan/furrr\")\nlibrary(furrr) # Used to do parallel processing with the topic models\nplan(multiprocess)\n# install.packages(\"quanteda\")\nlibrary(quanteda) # Used for data cleaning\n# install.packages(\"readtext\")\nlibrary(readtext) # Used to read in the txt files that were scraped\n# install.packages(\"stm\")\nlibrary(stm) # Used for more interesting topic models\n# install.packages(\"tictoc\")\nlibrary(tictoc) # Used for timing\n# install.packages(\"tidytext\")\nlibrary(tidytext)\n# install.packages(\"tidyverse\")\nlibrary(tidyverse) # Used for everything\n# install.packages('topicmodels')\nlibrary(topicmodels) # Used to make basic topic models\n\n# Read in the text that we scraped earlier\ntext <- readtext::readtext(\"inputs/minutes/*.txt\") # readtext makes this easy, \n# but could also use the usual base approach of listing files that end in txt etc.\n\nIn general you’ll often need to do a lot of cleaning before you can do the stats bit and get results. Here, I’ll just show two example steps. I’ve found that cleaning the dataset seems to take about 80 per cent of the time.\n\n\n#### Clean data ####\n# Do some basic cleaning - remove puncuation and change everything to lower case\ntext$text <- str_to_lower(text$text)\ntext$text <- str_replace_all(text$text, \"[:punct:]\", \" \")\n\nNow that we have a plausibly clean dataset (of course you’d want to come back and clean it more if you were actually interested in analysing the RBA minutes), we can try a topic model. Topic models are essentially just summaries. Instead of a document becoming a collection of words, they become a collection of topics with some probability associated with each topic.\n\n\n#### First topic modelling ####\n# Convert the corpus to a form that the topic model can work with\nrba_minutes <- quanteda::corpus(text) %>% # Minimum viable conversion\n  quanteda::dfm(remove_punct = TRUE, remove = stopwords('en')) %>% # Get rid of\n  # punctuation (in case you didn't already do that) and stop words - check \n  # those stop words assumptions\n  quanteda::dfm_trim(min_termfreq = 2, # Remove any word that doesn't occur at \n                     # least twice\n           min_docfreq = 2) # Get rid of any word that isn't in at least two documents\n\n# Run the topic model with 10 topics\ndtm <- quanteda::convert(rba_minutes, to = \"topicmodels\") # Getting the dfm \n# into a form that topicmodels can deal with\nlda_topics <- topicmodels::LDA(dtm, k = 10) # The k is the number of topics - \n# this decision has a big impact\n\n# Have a look at the terms\nterms(lda_topics, 10) # Top 10 words for each topic. Topics are just \n# probability distributions over words so you should look at different numbers of words\n\nLooking at the words in the topics, it seems as though “per” and “cent” are being treated as separate words. The RBA is proud that it separates “per” and “cent”, and if you’re a grad there that’ll stick with you for a while (see earlier paragraphs), but for our purposes they are one word and we need to combine them.\n\n\n#### Clean data ####\n# Let's deal with the first issue first.\ntext$text <- stringr::str_replace_all(text$text, \"per cent\", \"per_cent\")\ntext$text <- stringr::str_replace_all(text$text, \"per cent\", \"per_cent\")\n\n# You could run the topic model again if you wanted.\n\nRight, that issue of per cent has been fixed, but what if there are combinations of words like this that don’t show up very high in the topics? To identify these we need to construct n-grams. Earlier with ‘per’ ‘cent’, we generated a 2-gram. Quanteda and the tidyverse makes it easy to identify popular n-grams (if your dataset is large then I’d work with a sample of it because these can get a little unwieldy, and we only really care about the popular ones anyway). Our text is in sentences, paragraphs, etc, and we first need to break it down into tokens (essentially separate words). There’s a wonderful set of tutorials put together by the quanteda team here: https://tutorials.quanteda.io and the code for this section is from: https://tutorials.quanteda.io/basic-operations/tokens/tokens_ngrams/.\n\n\n#### Adjusting for common co-location ####\ntoks <- tokens(text$text)\n# First generate 2-grams\nngrams <- tokens_ngrams(toks, n = 2:4)\n# Somewhat annoyingly for our purposes (although understandably given the broader \n# picture) quanteda puts tokens into its own class, so we need ot convert in \n# order to use the usual tidyverse tools that we may be more familiar with.\n# As a side note, I often find it worthwhile to checking class in R when there's \n# an issue because often that's part of the issue, in this case: class(ngrams).\n# The tokens class seems to just be a list, so we can unlist it and then put it \n# into a more-friendly tibble.\nngram_counts <- tibble(ngrams = unlist(ngrams)) %>% \n  count(ngrams, sort = TRUE)\n\n# We can identify a bunch of obvious replacements. If we start getting a long \n# list then we can create a file that holds the replacement.\ntext$text <- stringr::str_replace_all(text$text, \"assistant governor\", \"assistant_governor\")\ntext$text <- stringr::str_replace_all(text$text, \"reserve bank board\", \"reserve_bank_board\")\ntext$text <- stringr::str_replace_all(text$text, \"unemployment rate\", \"unemployment_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"national accounts\", \"national_accounts\")\ntext$text <- stringr::str_replace_all(text$text, \"australian dollar\", \"australian_dollar\")\ntext$text <- stringr::str_replace_all(text$text, \"monetary policy\", \"monetary_policy\")\ntext$text <- stringr::str_replace_all(text$text, \"united states\", \"united_states\")\ntext$text <- stringr::str_replace_all(text$text, \"exchange rate\", \"exchange_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"glenn stevens\", \"glenn_stevens\")\ntext$text <- stringr::str_replace_all(text$text, \"reserve bank\", \"reserve_bank\")\ntext$text <- stringr::str_replace_all(text$text, \"cash rate\", \"cash_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"us dollar\", \"us_dollar\")\ntext$text <- stringr::str_replace_all(text$text, \"iron ore\", \"iron_ore\")\n\nrm(toks, ngrams, ngram_counts)\n\nTake a look at the topics again. Notice that ‘growth’ is in essentially every topic. So is ‘members’ and a couple of others. It’s not that growth isn’t important (insert standard economist joke here), but the fact that ‘members’ shows up suggests that these may just be due to the way that language is used at the RBA, rather than communicating topics. If you read these minutes, you’ll know that the RBA starts a LOT of sentences with ‘Members noted…’. What does this mean for our purposes? Essentially, if you look at each topic by itself they seem ‘coherent’, but taken as a group it seems as though the topics are too similar. Another way to say that is that the words lack ‘exclusivity’. This is a common tradeoff, and our results suggest that it may be worthwhile for us to reduce some of the coherence in order to increase the exclusivity. At this point, we’ll use a different package for creating topic models - the STM package - because it has a bunch of nice features that you might like to take advantage of in future work.\n\n\n#### Introducing STM and quanteda ####\nrba_minutes <- quanteda::corpus(text) %>% # Minimum viable conversion\n  quanteda::dfm(remove_punct = TRUE, \n                 remove_numbers = TRUE,\n                 remove = stopwords('en')) %>% # Get rid of punctuation (in \n  # case you didn't already do that) and stop words - check those stop words assumptions\n  quanteda::dfm_trim(min_termfreq = 2, # Remove any word that doesn't occur at least twice\n                     min_docfreq = 0.05, # Get rid of any word that isn't in at \n                     # least 5 per cent of documents\n                     max_docfreq = 0.90, # Get rid of any word that is in at \n                     # least 90 per cent of documents\n                     docfreq_type = \"prop\" # Above we specified percentages - you \n                     # could specify counts or ranks\n                     ) \n\n# We can run the topic model using STM\ntopics_stm <- stm(rba_minutes, K = 10)\n# Looking at the results you can see that the results are fairly similar to \n# those that we got from the topicmodels package, which is what we want. \nlabelTopics(topics_stm)\nrm(topics_stm)\n# If we were interested in the results then we might like to pre-process the text \n# a little more, for instance removing the names of months.\n\nOther than pre-processing decisions, the other major determininat of the outputs of topic models is the number of topics specified. There are a bunch of diagnostic tests that have been developed to help with this decision and we can use some nice code from Julia Silge (https://juliasilge.com/blog/evaluating-stm/) to try a bunch of different values for the number of topics.\n\n\n#### Deciding on the number of topics ####\ntic(\"With parallel\") # This allows us to time the code\nmany_models <- data_frame(K = seq(5, 20, by = 5)) %>% # Here we're running four \n  # topic models: 5 topics, 10 topics, 15 topics and 20 topics\n  mutate(topic_model = future_map(K, ~stm(rba_minutes, \n                                          K = .,\n                                          verbose = FALSE)))\ntoc()\n\n# You can also try setting K to zero within STM and seeing the number of topics \n# that it recommends: e,g, choose_topic_num_for_me <- stm(rba_minutes, K = 0, verbose = FALSE)\n\n# We want to compare those models with different numbers of topics using various diagnostics.\nheldout <- make.heldout(rba_minutes) # First create a test/training set\n\nk_result <- many_models %>%\n  mutate(exclusivity = map(topic_model, exclusivity), # How unique are words to the topics\n         semantic_coherence = map(topic_model, semanticCoherence, rba_minutes), # How \n         # much the topics tend to be coherent if we look at them (usually a \n         # tradeoff with exclusivity)\n         eval_heldout = map(topic_model, eval.heldout, heldout$missing),\n         residual = map(topic_model, checkResiduals, rba_minutes),\n         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),\n         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),\n         lbound = bound + lfact,\n         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))\n\nPut these diagnostics into a nice summary graph (again code is Julia’s originally).\n\n\nk_result %>%\n  transmute(K,\n            `Lower bound` = lbound,\n            Residuals = map_dbl(residual, \"dispersion\"),\n            `Semantic coherence` = map_dbl(semantic_coherence, mean),\n            `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %>%\n  gather(Metric, Value, -K) %>%\n  ggplot(aes(K, Value, color = Metric)) +\n  geom_line(show.legend = FALSE) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(x = \"K (number of topics)\",\n       y = NULL,\n       title = \"Model diagnostics by number of topics\") +\n  theme_minimal()\n\n\nIn general we are looking for the max/min of parabolas, so our results suggest we may be best with some more topics (go to Julia’s post for to see another example: https://juliasilge.com/blog/evaluating-stm/.\n\n\n# Have a look at that exclusivity to coherence tradeoff\nk_result %>%\n  select(K, exclusivity, semantic_coherence) %>%\n  unnest() %>%\n  mutate(K = as.factor(K)) %>%\n  ggplot(aes(semantic_coherence, exclusivity)) +\n  geom_point() +\n  facet_wrap(vars(K)) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\") +\n  theme_minimal()\n\n\nAlthough you’d probably want more, let’s just choose 10 topics for now. What we’re most interested in is getting the betas and gammas so that we can do our usual analysis.\n\n\ntopic_model <- k_result %>% \n  filter(K == 10) %>% \n  pull(topic_model) %>% \n  .[[1]]\n\n# Grab the betas - these are the probability of each term in each topic\ntd_beta <- broom::tidy(topic_model, \n                       matrix = \"beta\")\n\n# Grab the gammas - these are the probability of each word in each topic\ntd_gamma <- tidy(topic_model, \n                 matrix = \"gamma\",\n                 document_names = rownames(rba_minutes))\n\nFrom here you could look at how the gammas and betas evolve or change using a statistical model. Or even sometimes just looking at them is interesting. Julia Silge has a bunch of code that makes very nice graphs and tables. One of the advantages of the STM package is that it makes it easier to include specific types of additional information. For instance, we know that over our time period there have been two governors: GRS and Phil Lowe. We could associate each date with who the governor is and then allow that to affect the prevalence of certain topics.\nYou can grab the files and folder set up from GitHub if you’d like.\n\n\n",
    "preview": "posts/2019-01-03-gathering-and-analysing-text-data/gathering-and-analysing-text-data_files/figure-html5/diagnosticmeasuregraphs-1.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-08-13-cleaning-hansard/",
    "title": "Cleaning Hansard: The pay's not great but the work is hard",
    "description": "Cleaning the Australian Hansard is mind-numbing, annoying and time-consuming, but someone has to do it.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-08-13",
    "categories": [],
    "contents": "\nThanks to Monica for the title.\nAfter getting an email along the lines of ‘hey aren’t you doing something with Australia’s Hansard?’ I realised that I’ve been a bit remiss about sharing my Australian Hansard progress. This is the first of a series of posts to fix that.\n{{< tweet 926509282874585089 >}}\nI haven’t written about what I’ve been doing with Australian Hansard to this point because: 1) my knowledge of the political science literature is piecemeal and I’m sure someone must have already done all this and I just can’t find it; and 2) my coding knowledge is also piecemeal and there’s no doubt a million ways to better do what I’ve done so far. If anyone has advice on either aspect (or anything really!) I’d be keen to hear it - please email.\nBest I can tell, Australian Hansard is a treasure-trove of data and it’s hard to believe it hasn’t been more analysed. I’m probably missing a whole bunch of literature (insert standard joke about economists here) but so far I can really only find a handful of papers using Australia’s Hansard.1 There’s plenty of work using the parliamentary records of other Commonwealth countries such as Canada2, NZ3 and the UK4. I think that in Australia it’s really only Patrick Leslie who may be using it at the moment (big thank you to Jill Sheppard for pointing this out), but I’ll update this if I find others.\nThe good news is that Australia’s Hansard has been digitised and is available on the parliament’s website, so a figurative pseudo-Manhattan-project isn’t required (cf. what was needed in Canada, see: https://www.lipad.ca/). If you just want short, specific, sections then the situation is fine - for pre-1981 go to Tim Sherratt’s brilliant Historic Hansard website (http://historichansard.net); for 1981 to 2006 just use the parliament’s website; and for 2006 onward just use Open Australia. The bad news is that Hansard isn’t really available as a nice corpus for larger scale analysis. Making this nice corpus has been keeping me busy, and will be the subject of this series of posts.\nHelpfully, various people/organisations have gone to the Hansard website to get the XML files they provide and made them available as an easy download (note that these tend to have been posted as they were provided by the parliament, so they’re full of typos, transcription errors, and a bunch of other mistakes):\n1901-1980 Hansard is available as XML from Tim Sherratt’s Historic Hansard Github - https://github.com/wragge/hansard-xml.\n1998-2014 Hansard is available from Andrew Turpin’s website at University of Melbourne (https://people.eng.unimelb.edu.au/aturpin/qtCorpus/index.html).\n2006-current is available from Open Australia’s website (http://data.openaustralia.org.au/origxml/).\nThese helpful people/organisations were able to get those dates (1901-1980 and 1998-current) because the Hansard provides the XML for those years on their website. The problem is the 1980s and the early/mid 1990s because they don’t have the XML available on the website (and from emails with them - it seems as though they simply don’t have it) - the only choice seems to be either to scrape it manually from the Hansard website or to grab all the PDFs, convert them, and then fix the mistakes. I’ve started on the second option - unsure how wise it is but I don’t know of any alternative.\nFuture posts:\nWho has been elected to parliament?\nWhat divisions are relevant?\nWhat parties are relevant?\nHansard pre-1981\nHansard post-1980\nFor instance: Turpin ‘An Attempt to Measure the Quality of Questions in Question Time of the Australian Federal Parliament’.↩\nFor instance: Beelen, Thijm, Cochrane, Halvemaan, Hirst, Kimmins, Lijbrink, Marx, Naderi, Rheault, Polyanovsky, Whyte ‘Digitization of the Canadian Parliamentary Debates’.↩\nFor instance: Curran, Higham, Ortiz, Vasques Filho ‘Look Who’s Talking: Bipartite Networks as Representations of a Topic Model of New Zealand Parliamentary Speeches’.↩\nFor instance: Abercrombie, Batista-Navarro ‘Aye or No? Speech-level Sentiment Analysis of Hansard UK Parliamentary Debate Transcripts’↩\n",
    "preview": {},
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-28-the-sql-is-never-as-good-as-the-original/",
    "title": "The SQL Is Never As Good As The Original",
    "description": "SQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-07-28",
    "categories": [],
    "contents": "\nThanks to Monica for the title.\nIntroduction\nSQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post.\n\nSQL is fairly straightforward if you’ve used mutate, filter and join in the R tidyverse as the concepts (and sometimes even the verb) are the same. In that case, half the battle is getting used to the terminology, and the other half is getting on top of the order of operations because SQL can be a tad pedantic.\nSQL (“see-quell” or “S.Q.L.” - both camps seem fairly insistent on their way…) is used with relational databases. A relational database is just a collection of at least one table, and a table is just some data organized into rows and columns. If there’s more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or don’t, but always end a SQL command in a semicolon;\nCreating a table\nCreate an empty table of three columns of type: int, text, int:\n\n\nCREATE TABLE table_name (\n  column1 INTEGER,\n  column2 TEXT,\n  column3 INTEGER\n);\n\nAdd a row of data:\n\n\nINSERT INTO table_name (column1, column2, column3)\n  VALUES (1234, 'Gough Menzies', 32);\n\nAdd a column:\n\n\nALTER TABLE table_name\n  ADD COLUMN column4 TEXT;\n\nViewing the data\nSee one column (similar to R’s select):\n\n\nSELECT column2\n  FROM table_name;\n\nSee two columns:\n\n\nSELECT column1, column2\n  FROM table_name;\n\nSee all columns:\n\n\nSELECT *\n  FROM table_name;\n\nSee unique rows in a column (similar to R’s distinct):\n\n\nSELECT DISTINCT column2\n  FROM table_name;\n\nSee the rows that match a criteria (similar idea to R’s which or filter):\n\n\nSELECT *\n  FROM table_name\n    WHERE column3 > 30;\n\nAll the usual operators are fine with WHERE: =, !=, >, <, >=, <=. Just make sure the condition evaluates to true/false.\nSee the rows that are pretty close to a criteria:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '_ough Menzies';\n\nThe _ above is a wildcard that matches to any character e.g. ‘Cough Menzies’ would be matched here, as would ‘Gough Menzies’. LIKE is not case-sensitive: ‘Gough Menzies’ and ‘gough menzies’ would both match here.\nUse % as an anchor to matches pieces:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '%Menzies';\n\nThat matches anything ending with ‘Menzies’, so ‘Cough Menzies’, ‘Gough Menzies’, ‘Sir Menzies’ etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match ‘Sir Menzies Jr’ whereas %Menzies would not.\nThis is wild: NULL values (!) (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 IS NOT NULL;\n\nThis too is wild: There’s an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric! The following looks for text that starts with a letter between A and M (not including M) so would match ‘Gough Menzies’, but not ‘Sir Gough Menzies’!\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M';\n\nIf you look for a numeric (as opposed to text) then BETWEEN is inclusive.\nCombine conditions with AND (both must be true to be returned) or OR (at least one must be true):\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M'\n    AND column3 = 32;\n\nYou can order the result:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3;\n\nAscending is the default, add DESC for alternative:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC;\n\nRestrict the return to a certain number of values by adding LIMIT at the end:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC\n    LIMIT 1;\n\n(This doesn’t work all the time - only certain SQL databases.)\nModifying data and using logic\nEdit a value:\n\n\nUPDATE table_name\n  SET column3 = 33\n    WHERE column1 = 1234;\n\nImplement if/else logic:\n\n\nSELECT *,\n  CASE\n    WHEN column2 = 'Gough Whitlam' THEN 'Labor'\n    WHEN column2 = 'Robert Menzies' THEN 'Liberal'\n    ELSE 'Who knows'\n  END AS 'Party'\n  FROM table_name;\n\nThis returns a column called ‘Party’ that looks at the name of the person to return a party.\nDelete some rows:\n\n\nDELETE FROM table_name\n  WHERE column3 IS NULL;\n\nAdd an alias to a column name (this just shows in the output):\n\n\nSELECT column2 AS 'Names'\n  FROM table_name;\n\nSummarising data\nWe can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of summarise in R. COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *:\n\n\nSELECT COUNT(*)\n  FROM table_name;\n\nSimilarly, pass a column to SUM, MAX, MIN, and AVG:\n\n\nSELECT SUM(column1)\n  FROM table_name;\n\nROUND takes a column and an integer to specify how many decimal places:\n\n\nSELECT ROUND(column1, 0)\n  FROM table_name;\n\nSELECT and GROUP BY is similar to group_by in R:\n\n\nSELECT column3, COUNT(*)\n  FROM table_name\n    GROUP BY column3;\n\nYou can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.\nHAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.\nCombining data\nCombine two tables using JOIN or LEFT JOIN:\n\n\nSELECT *\n  FROM table1_name\n  JOIN table2_name\n    ON table1_name.colum1 = table2_name.column1;\n\nBe careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.\nUNION is the equivalent of cbind if the tables are already fairly similar.\n\n\n",
    "preview": {},
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-29-getting_started_with_topic_modelling-theory/",
    "title": "Topic Modelling - Theory",
    "description": "Each statement in Hansard needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, Jordan, @Blei2003latent, as implemented by the R package 'topicmodels' by Grun @Grun2011.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\nThese are notes that I put together in the process of trying to understand how topic modelling works with a view to applying it to Australia’s Hansard. There are undoubtedly mistakes and aspects that are unclear. Please get in touch if you have any suggestions.\nOverview\nEach statement in Hansard needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by the R package ‘topicmodels’ by Grün and Hornik (2011).\nThe key assumption behind the LDA method is that each statement, ‘a document’, in Hansard is made by a speaker who decides the topics they would like to talk about in that document, and then chooses words, ‘terms’, that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in Hansard group themselves to define topics.\nDocument generation process\nAs applied to Hansard, the LDA method considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the speaker then chooses appropriate words to use for each of those topics.\nMore generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure 1).\n\n\n\nFigure 1: Probability distributions over topics\n\n\n\nSimilarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure 2).\n\n\n\nFigure 2: Probability distributions over terms\n\n\n\nFollowing Blei and Lafferty (2009), Blei (2012) and Griffiths and Steyvers (2004), the process by which a document is generated is more formally considered to be:\nThere are \\(1, 2, \\dots, k, \\dots, K\\) topics and the vocabulary consists of \\(1, 2, \\dots, V\\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \\(k\\)th topic is \\(\\beta_k\\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \\(0<\\eta<1\\) is used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).1 Strictly, \\(\\eta\\) is actually a vector of hyperparameters, one for each \\(K\\), but in practice they all tend to be the same value.\nDecide the topics that each document will cover by randomly drawing distributions over the \\(K\\) topics for each of the \\(1, 2, \\dots, d, \\dots, D\\) documents. The topic distributions for the \\(d\\)th document are \\(\\theta_d\\), and \\(\\theta_{d,k}\\) is the topic distribution for topic \\(k\\) in document \\(d\\). Again, the Dirichlet distribution with the hyperparameter \\(0<\\alpha<1\\) is used here because usually a document would only cover a handful of topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). Again, strictly \\(\\alpha\\) is vector of length \\(K\\) of hyperparameters, but in practice each is usually the same value.\nIf there are \\(1, 2, \\dots, n, \\dots, N\\) terms in the \\(d\\)th document, then to choose the \\(n\\)th term, \\(w_{d, n}\\):\nRandomly choose a topic for that term \\(n\\), in that document \\(d\\), \\(z_{d,n}\\), from the multinomial distribution over topics in that document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose a term from the relevant multinomial distribution over the terms for that topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\n\nGiven this set-up, the joint distribution for the variables is (Blei (2012), p.6): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{i=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]\nBased on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \\(\\beta_{1:K}\\) and \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). This is intractable directly, but can be approximated (Griffiths and Steyvers (2004) and Blei (2012)).\nAnalysis process\nAfter the documents are created, they are all that we have to analyse. The term usage in each document, \\(w_{1:D, 1:N}\\), is observed, but the topics are hidden, or ‘latent’. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures 1 or 2. In a sense we are trying to reverse the document generation process – we have the terms and we would like to discover the topics.\nIf the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (Blei (2012), p.7): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]\nThe initial practical step when implementing LDA given a corpus of documents is to remove ‘stop words’. These are words that are common, but that don’t typically help to define topics. There is a general list of stop words such as: “a”; “a’s”; “able”; “about”; “above”… An additional list of words that are commonly found in Hansard, but likely don’t help define topics is added to the general list. These additions include words such as: “act”; “amendment”; “amount”; “australia”; “australian”; “bill”… A full list will be available in a follow up post going through an example with R code. We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.\nAfter the dataset is ready, the R package ‘topicmodels’ by Grün and Hornik (2011) can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following Steyvers and Griffiths (2006) and Darling (2011), the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \\(\\alpha = \\frac{50}{K}\\) and \\(\\eta = 0.1\\) (Steyvers and Griffiths (2006) recommends \\(\\eta = 0.01\\)), where \\(\\alpha\\) refers to the distribution over topics and \\(\\eta\\) refers to the distribution over terms (Grün and Hornik (2011), p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik (2011), p.6): \\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-i}+K\\alpha} \\] where \\(z'_{d, n}\\) refers to all other topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) is a count of how many other times that any term has been assigned to topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\) in that particular document; and \\(\\lambda'^{(d)}_{-i}\\) is a count of how many other times that term has been assigned in that document. Once \\(z_{d,n}\\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.\nThis conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths (2006)). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.\nWarnings and extensions\nThe choice of the number of topics, k, affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well.\nOne weakness of the LDA method is that it considers a ‘bag of words’ where the order of those words does not matter (Blei (2012)). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking.\nReferences\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84.\n\n\nBlei, David M, and John D Lafferty. 2009. “Topic Models.” In Text Mining, 101–24. Chapman; Hall/CRC.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nDarling, William M. 2011. “A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 642–47.\n\n\nGriffiths, Thomas, and Mark Steyvers. 2004. “Finding Scientific Topics.” PNAS 101: 5228–35.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic Models.” In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch.\n\n\nThe Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \\(\\eta=1\\), it is equivalent to a uniform distribution. If \\(\\eta<1\\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \\(\\eta\\) decreases. A hyperparameter is a parameter of a prior distribution.↩\n",
    "preview": "posts/2018-06-29-getting_started_with_topic_modelling-theory/topic_modelling-theory_files/figure-html5/topicsoverdocuments-1.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-09-14-getting-started-with-latex/",
    "title": "Getting started with LaTeX",
    "description": "LaTeX makes it easier to produce papers that look great, but it can be overwhelming at the start. These notes help you get up-and-running with LaTeX.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\nThank you to Janet Bradly and Maria Racionero for their support of this workshop. These notes are based on ones prepared by Zac Cranko and I for a presentation in 2015 and those that I put together for a 2016 presentation. Zac’s work is used with permission.\nIntroduction\nLaTeX makes it easier to produce papers that look great, but it can be overwhelming at the start. These notes help you get up-and-running with LaTeX.\nBy the end you will have created an example paper and slides that include a title, author, affiliation, abstract, sections, tables, figures and references that looks like this:\n\n\n\nIt is best to type everything out yourself, but you can download the tex file from here: https://github.com/RohanAlexander/blogdown_website/blob/master/static/img/paper.tex\nWriting papers\nHello world!\nYou can treat LaTeX as a markup language. You mark your text with commands depending on how you want it to look, and then compile it to produce an output such as a PDF. Get started by downloading LaTeX, for free, from: https://www.latex-project.org/get/.\nUse the default settings. You will likely end up installing a bunch of programs. (There is now a smaller version of MacTeX available, but for now it is probably best to stick with the original version for now.)\nLet’s make a document. Open ‘TeXShop’ if you’re using a Mac, and ‘TeXStudio’ if you’re using a PC.\nIn TeXShop/TeXStudio type:\n\n\n\nThen click ‘Typeset’. A bunch of miscellaneous files are produced in the process of compiling. Don’t worry about these – all they do is make your directory ugly. The two important files are the tex file, which contains your markup, and the pdf file, which contains your output.\nSquare brackets are optional in LaTeX, but it is worth specifying the paper size and the font for the documentclass command. If you wanted to specify A4 paper and 12pt text by default then our ‘Hello world!’ becomes:\n\n\n\nThe type of document that you want goes in the braces. We used ‘article’, which is good for producing papers. There are many classes of document, including ‘book’, ‘letter’, and ‘beamer’ which is for presentations, but ‘article’ is probably the one that you will most commonly use.\nPackages\nPackages expand the basics of LaTeX. There are a few that you should load every time because they’re often used when writing economics papers:\namsmath;\namsthm;\namssymb; and\ngraphicx.\nLoad these packages by adding the following after documentclass[a4paper, 12pt]{article}, but before begin document:\n\n\n\nThen click ‘Typeset’. Nothing should change in the output, we are just typesetting it to make sure that we have not introduced an error. The first three packages help when writing maths. The fourth helps with including graphs.\nEverything before begin document is called the preamble, and everything after it is called the content.\nTitle, author, and affiliation\nYou can add a title, author and affiliation to your paper by adding the following to the content:\n\n\n\nClick ‘Typeset’ and a title should be added to your paper.\nAdd an affiliation by changing the author markup to:\n\n\n\nThe textit command makes the text italic. You don’t need to make the affiliation italic, but many people seem to. You could make your text bold using textbf.\nFinally, add an acknowledgements section by changing the author markup to:\n\n\n\nIf you don’t include date{A specific date} then LaTeX will add today’s date. If you don’t want the date then add date{} instead.\nAbstract\nIf you want an abstract, then LaTeX will do the formatting for you if you use the abstract tags. Add this markup just below maketitle:\n\n\n\nTo summarise what your tex file should look like to this point:\n\n\n\nSections\nThe main section commands are: section, subsection, and subsubsection. These produce headings of decreasing importance and are numbered automatically. That can be turned off using an asterisk, for instance: section*.\nFor now let’s add numbered introduction, model, and data sections to our document by adding the following markup after the abstract:\n\n\n\nMaths\nNow let’s add some maths into the model section of our paper. Type the following under the model section:\n\n\n\nWhen you compile your tex file you should get this equation:\n\\[ \\hat{\\alpha} = \\frac{\\sum^J_{j=1}\\beta_j}{\\int^{\\infty}_{0}f(k)} \\]\nWe will now go through the pieces of this.\nGreek, limits, infinity, and integrals\nWe invoked ‘maths mode’ by using double dollar signs. That put the maths that you write on its own line. If you wanted to have your maths content without breaking the line, such as \\(x = 5\\), then just use single dollar signs.\nWithin maths mode you can get many greek letters by backslash followed by their name. The examples above were alpha and beta.\nLimits, infinity and integrals are built into LaTeX math mode, and can be access with the command lim, infty, and int. You can use accents and underbars if you need to denote what the limit refers to or upper and lower bounds. For instance, add the following markup underneath the formula:\n\n\n\nWhen you compile this it should looks like:\nAnd when \\(\\lim_{k\\rightarrow0}f(k)/k = \\infty\\) it is the case that \\(\\int^{\\infty}_{0}f(k) = 100\\).\nProbability, expectations, real numbers, integrals\nThe \\(\\mathbb{P}\\), \\(\\mathbb{E}\\), and \\(\\mathbb{R}\\) that you may be used to seeing to denote probability, expectation, and the real numbers are made by a call to mathbb within maths mode. For instance add the following to your paper:\n\n\n\nWhen you compile it should look like this:\nOften we care about probability, \\(\\mathbb{P}\\), because of expectations, \\(\\mathbb{E}\\), over real numbers \\(\\mathbb{R}\\).\nYou can also make a call to mathcal, for instance in naming sets \\(\\mathcal{A}\\), \\(\\mathcal{B}\\), \\(\\mathcal{C}\\) or for a nice Lagrangian \\(\\mathcal{L}\\).\nFractions\nFractions are built into math mode using frac{}{} and you can nest them if you need to. For instance add the following to your paper:\n\n\n\nThis should compile to:\nIt can be surprising when you first learn that \\(\\frac{\\frac{x}{y}}{y} = \\frac{x}{y^2}\\).\nBe careful when using brackets and fractions because sometimes the sizes need to be aligned. You can do it manually, but alternative specify left and right, for instance, compare with and without:\n\n\n\nwhich compiles to:\n\\[\\left(\\frac{\\frac{x}{y}}{y}\\right) = (\\frac{\\frac{x}{y}}{y})\\].\nTheorems, definitions and proofs\nTheorems and proofs draw on the amsthm package that was loaded earlier. You need to declare the name that you’ll use to refer to it in the preamble. After that you can call a theorem, proposition, description, whatever it was you defined, throughout the document.\nFirst, add this to the preamble:\n\n\n\nThen add this to the content:\n\n\n\nIn this case, I defined a theorem and LaTeX will print Theorem when compiled You could add another for propositions, etc.\nProofs are similar, but don’t need to be defined in the preamble:\n\n\n\nYour tex file should now look like this:\n\n\n\nText\nParagraphs\nLaTeX was designed for maths, but it does text well too. To start a new paragraph, just leave a blank line in your editor, LaTeX will take care of spacing. For instance add the following to your introduction:\n\n\n\nThere are a few aspects to be aware of:\nTo get ‘a quote’, you need to use the key next to the ‘1’ on your keyboard for the opening mark and then the normal quotation mark for the end mark.\nYou can makes words italic using textit{some italic words}, or make them bold using textbf{some bold words}.\nBecause the dollar sign is used to invoke maths mode, if you want to refer to prices, say $4, then you need to use a slash before the dollar sign. This is the same for the per cent symbol, %.\nSome people prefer different formatting on the paragraphs. Although it can cause some issues, you can change this by adding to the preamble:\n\n\n\nLists\nThere are two main types of lists: itemize and enumerate.\nAdd the following to your data section:\n\n\n\nTables\nTables are often annoying in LaTeX. Fortunately, many programs will automatically format their table outputs with LaTeX markup for your to copy-and-paste into your tex file, and there are websites that can help.\nSimple tables are not a problem. For instance, add the following to your data section:\n\n\n\nBut it gets complicated. If you commonly use tables then it is easier to get your statistics program to output tables that have been formatted for LaTeX, or use an online table generator, such as http://www.tablesgenerator.com/.\nIn R, there is a package ‘Huxtable’.\nGraphs and pictures\nTo include graphs or pictures in your document, add the file to the same folder that your tex file is in. From there you can add it. Many adjustments are possible in terms of size and layout. Fortunately, all the labelling is done for us. For instance, download the following image into the folder where your tex file is saved: https://github.com/RohanAlexander/blogdown_website/blob/master/static/img/me.png\nThen add this markup into your data section:\n\n\n\nYour tex file should look like this:\n\n\n\nReferences\nLaTeX uses Bibtex for references. To use this open a new file in TeXShop/TeXStudio and add the following:\n\n\n\nSave it as first_bibliography.bib in the same folder as your tex file. Then add the following at the end of the paragraphs in your Introduction:\n\n\n\nAnd add the following at the end of your document, on the line above end{document}.\n\n\n\nThen typeset as BibTeX, (you’ll only need to do this once each time you update your bib file), and then typeset as normal with LaTeX.\nYour tex file should look like this:\n\n\n\nSlides\nMaking slides is similar to writing a paper in that the markup can be the same. But you need to specify when the content of a slide should start and stop and also what the title should be.\nTo get started, open a new file in TeXShop/TeXStudio and add the following:\n\n\n\nYou’ll notice that the only difference is that the document class has been changed to beamer.\nCopy the title, author and date, etc markup from your paper, and then paste it between begin{frame} and end{frame}. So your file should look like this:\n\n\n\nWhen you save and compile this you should get slides.\nYou can add a content slide by adding the following after that first slide:\n\n\n\nI’ve chosen to include a list, but you could include paragraphs, or images or tables using the same markup that you have in your paper.\nMisc\nLearning more and getting help\nCheck that there is an end for every begin, and similarly that all braces that are opened are also closed.\nCompile frequently so that you have a better idea of where the error is.\nStack Overflow is helpful if you want specific changes or features.\nThe wikibooks guide - https://en.wikibooks.org/wiki/LaTeX - is an excellent resource to improve your knowledge.\nShareLaTeX\nThese days I more commonly use LaTeX in the cloud instead of on my local computer, via: https://www.sharelatex.com/. The advantage is that it brings google-docs-style collaboration tools.\n\n\n",
    "preview": "posts/2017-09-14-getting-started-with-latex/images/LaTeX_first_example.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1014,
    "preview_height": 1448
  },
  {
    "path": "posts/2017-08-11-reproducing-a-grattan-map/",
    "title": "Reproducing a Grattan Institute map",
    "description": "Blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-08-11",
    "categories": [],
    "contents": "\nThank you to Monica for her helpful edits.\nGeoffrey Liu found an error in how I deal with the postcodes data that I haven’t fixed yet.\nIntroduction\nThe Grattan Institute is an Australian think tank that produces reports about public policy. Last week they released ‘Regional patterns of Australia’s economy and population’. That report looks into the differences between geographic areas across various economic and demographic variables. It includes interactive maps made using Carto. The Grattan Institute also released the datasets that underpin the report’s maps and graphs.\nI was interested to see if I could reproduce one of their interactive maps - Annual income growth per person (FY04 - FY15) - in an hour using R and the Leaflet package. A day later I found the underlying dataset did not correspond with the map that the Grattan Institute published. But following the methodology in the report I was able to create a dataset that seems pretty close to the values illustrated on their map.\nThe final map that I produced is below (it’ll take about 5 seconds to load) and this note records what I did to produce it in R using the Leaflet package.\n\n\n\n\nWorkspace\nThe first step was to set-up the workspace. Mostly this just meant loading packages. The tidyverse and magrittr packages help with general data manipulation tasks; zoo helps with the rolling average needed later; and leaflet, rgdal, and rmapshaper are specific to mapping.\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(zoo)\nlibrary(leaflet)\nlibrary(rgdal)\nlibrary(rmapshaper)\n\nData\nI decided to reproduce Figure 2.4 of the report. This shows the average annual growth in real taxable income per tax filer between the financial years 2003/04 and 2014/15 for the 2011 SA3 areas. To do this I needed the incomes data that the Grattan Institute mapped and the geographic data that defines the areas.\nIncomes\nThe Grattan Institute released the datasets that they said had been mapped. It was straightforward to download this file and export the relevant sheet as a csv file. The Excel file is available at https://grattan.edu.au/report/regional-patterns-of-australias-economy-and-population/ (in the left panel).\n\n\n# Import the income data that has been taken from the Grattan data download, sheet 'Figure 2.4' (which was saved as a csv).\nincomes <- read_csv(\"data/890-Regional-patterns-chart-data.csv\")\n# Remove some debris columns\nincomes <- incomes %>%\n  select(\"SA3\", \"Growth_mean_income_FY04_FY15\")\n\nGeographies\nThe other dataset that I needed was the geographies that the Grattan Institute had used. I initially wasted a lot of time using the 2016 SA3 release. Eventually I realised they were using the SA3 release from 2011. This is available from the ABS website at: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202011?OpenDocument and I used the zipped file: ‘Statistical Area Level 3 (SA3) ASGS Ed 2011 Digital Boundaries in ESRI Shapefile Format’.\n\n\n# 2011 SA3 boundaries\nold_boundaries <- readOGR(dsn = \"data/1270055001_sa3_2011_aust_shape\", layer = \"SA3_2011_AUST\")\n# Add the income data into the boundaries dataset\nold_boundaries <- merge(old_boundaries, incomes, by.x = \"SA3_NAME11\", by.y = \"SA3\")\n\nComparing the incomes data with the 2011 geographies data indicates the incomes data is missing two SA3 areas: ‘Illawarra Catchment Reserve’ and ‘Blue Mountains - South’. I also found these areas had been left as NA in the Grattan Institute’s interactive map. This made me confident that the Grattan Institute was using the 2011 boundaries (I may have missed it but I don’t think this was documented).\nMap\nMaking the map was not complicated once the pieces were in place. I called Leaflet and specified a black and white base map. After that I adjusted the default view and then added the patchwork quilt that shows the incomes dataset by SA3 area.\nI didn’t spend too much time on the colours because the inferno palette got fairly close. I just copied their published bins to reproduce the breaks that the Grattan Institute used. Box 1 in Appendix A of the Grattan Institute’s report specifies how they came up with these bins. Give my purposes I didn’t worry too much about this.\n\n\n# Set the color scheme \npal <- colorBin(\n  palette = \"inferno\",\n  domain = old_boundaries$Growth_mean_income_FY04_FY15,\n  bins = c(0, 0.005, 0.012, 0.015, 0.017, 0.019, 0.022, 0.029, 0.07),\n  reverse = TRUE\n  )\n\n# Make the map\nAustralia_incomes_map <- \n  leaflet() %>%\n  # Base map\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %>% \n  setView(lng = 133.7751, lat = -25.2744, zoom = 4) %>% # Specify where the map is initially focused\n  addPolygons(data = old_boundaries, \n              color = pal(old_boundaries$Growth_mean_income_FY04_FY15), \n              weight = 1, \n              stroke = FALSE,\n              smoothFactor = 0.5,\n              fillOpacity = 0.8, \n              label = paste(\"Area name (SA3):\", as.character(old_boundaries$SA3_NAME11), \n                            \"Mean annual growth:\", as.character(old_boundaries$Growth_mean_income_FY04_FY15)),\n              highlightOptions = highlightOptions(color = \"#666\", weight = 2, bringToFront = FALSE)) %>% \n  addLegend(\"bottomright\", pal = pal, values = old_boundaries$Growth_mean_income_FY04_FY15,\n            title = \"Mean annual income growth (FY04 - FY15)\",\n            opacity = 0.4\n  )\n\n# Call the map\nAustralia_incomes_map\n\nThere were many small changes that could be made to better reproduce the Grattan Institute’s map, but at this stage I realised there was something going on with the data. I decided to spend more time with that than tweaking the remaining aspects.\nComparison\nIssues\nComparing screenshots of the maps shows some of the issues:\n\n\n\nFigure 1: Grattan’s map\n\n\n\n\n\n\nFigure 2: My map\n\n\n\nFor instance there are areas where my map has a lot more variation such as:\nthe northern SA3 areas of Queensland; and\nthe SA3 areas in the south west of Western Australia.\nAnd there are also some areas where the colours are fairly different (notwithstanding the fact that I didn’t match theirs exactly), such as:\nwest of Melbourne where my values are a lot higher.\nTriage\nI checked that I was using the dataset that I had meant to use. While I was checking this dataset (which is the one that the Grattan Institute makes available) I noticed that the numbers in the Grattan Institute’s dataset were not always the ones that were being mapped. This was easy to see because they included a static version of the map next to the dataset, so I was confident it was meant to be the same.\nI couldn’t work out how to download the actual dataset underlying the interactive Carto map. But I was able to check some on an area-by-area basis because the value was displayed on mouse-hover. I found that the figure that was displayed did line up with the colour of the area, but not the dataset that they offered as underpinning the map.\nThe following table summarises some of the SA3 areas in north Queensland.\nSA3 Area\nGrattan Carto value\nGrattan data value\nCairns - South\n1.73\n2.0\nCharters Towers - Ayr - Ingham\n2.35\n1.6\nFar North\n2.66\n3.0\nOutback - North\n2.33\n1.7\nPort Douglas - Daintree\n1.73\n1.4\nTablelands (East) - Kuranda\n2.23\n1.6\nThe key issue was whether it was the Carto map or the dataset that was wrong.\nDown the rabbit hole\nBy this stage it was after dinner, and I’d had a glass or three of wine. But the only way to work out whether it was the dataset or the map that was wrong was to recreate the dataset myself. To do this I needed:\nincomes data for financial years 2003-04 and 2014-15;\nan inflation rate over this time to make the 2003-04 data real; and\na correspondence from postcodes to the 2011 SA3 areas.\nWithout their code it would be hard to be certain, but the Grattan Institute provided enough information in the report that I was confident I could get reasonably close to what they’d done.\nIncomes\nThe incomes data is from Table 8 of the ATO’s 2015 tax stats which is available here: https://data.gov.au/dataset/taxation-statistics-2014-15/resource/02e58971-ddee-4f77-af15-5c45de569ed6\n\n\n# Import the tax data\ntax_data <- read_csv(\"data/taxstats2015individual08medianaveragetaxableincomestateterritorypostcode.csv\", skip = 2, col_names = FALSE)\n# Remove the debris rows\ntax_data <- tax_data[2:2254,]\n# Grattan says we only need postcode, the 2003/04 data and the 2014/15 data, so drop the rest of the variables\ntax_data <- tax_data %>%\n  select(X2:X5, X11:X13)\n# Fix the column names\ntax_data <- rename(tax_data, \"postcode\" = X2, \"population_0304\" = X3, \"median_inc_0304\" = X4, \"ave_inc_0304\" = X5, \"population_1415\" = X11, \"median_inc_1415\" = X12, \"ave_inc_1415\" = X13)\n# Finally, change the classes to numeric (need to remove the comma first)\ntax_data$population_0304 <- sub(\",\", \"\", tax_data$population_0304)\ntax_data$median_inc_0304 <- sub(\",\", \"\", tax_data$median_inc_0304)\ntax_data$ave_inc_0304 <- sub(\",\", \"\", tax_data$ave_inc_0304)\ntax_data$population_1415 <- sub(\",\", \"\", tax_data$population_1415)\ntax_data$median_inc_1415 <- sub(\",\", \"\", tax_data$median_inc_1415)\ntax_data$ave_inc_1415 <- sub(\",\", \"\", tax_data$ave_inc_1415)\ntax_data$postcode <- as.numeric(tax_data$postcode)\ntax_data$population_0304 <- as.numeric(tax_data$population_0304)\ntax_data$median_inc_0304 <- as.numeric(tax_data$median_inc_0304)\ntax_data$ave_inc_0304 <- as.numeric(tax_data$ave_inc_0304)\ntax_data$population_1415 <- as.numeric(tax_data$population_1415)\ntax_data$median_inc_1415 <- as.numeric(tax_data$median_inc_1415)\ntax_data$ave_inc_1415 <- as.numeric(tax_data$ave_inc_1415)\n\nInflation\nI needed to change the financial year 2003-04 incomes into 2014-15 dollars. In the Grattan Institute’s report (p. 34) they say:\n\nNominal income for the 2003-04 financial year was adjusted to real 2014-15 dollars, using a yearly average of ABS quarterly data on the Consumer Price Index, starting from the 2003 September quarter.\n\nI downloaded the inflation data that they specified from: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/6401.0Mar%202017?OpenDocument (it is Series A2325846C which is in Tables 1 and 2 of the release). I wasn’t exactly sure how the Grattan Institute constructed its measure, but I decided to just go with the RBA formula (http://www.rba.gov.au/calculator/) which is: (Average of the four quarters in the final year / average of the four quarters in the first year - 1) *100, although I didn’t need to worry about removing one or the multiplying.\n\n\n# Import the inflation data\ninflation_data <- read_csv(\"data/640101.csv\", skip = 10, col_names = FALSE)\n# Grab the series they use: A2325846C\ninflation_data <- inflation_data %>%\n  select(X1, X10)\n# Fix the column names\ninflation_data <- rename(inflation_data, \n                   \"quarter\" = X1,\n                   \"index_value\" = X10\n)\n# To use the RBA formula we need the average index number over the four quarters\ninflation_data <- inflation_data %>%\n  mutate(\n    average_index_over_year = rollmean(index_value, 4, align = 'right', fill = NA)\n  )\n# Now we just use (final_year_ave / first_year_ave) (don't need to bother with removing the 1 or multiplying by 100 because we are just going to multiply the first year data up to get in terms of final year)\ninflation_rate <- \n  ((inflation_data$average_index_over_year[inflation_data$quarter == \"Jun-2015\"] / inflation_data$average_index_over_year[inflation_data$quarter == \"Jun-2004\"]))\n\nThe inflation adjustment turned out to be about 1.34. I also worked out the annual inflation rates and then chained them, and also tried just using the start and end quarter index numbers (not averaged). They came to similar values, so even though I wasn’t exactly sure what the Grattan Institute had done, I was confident that small differences wouldn’t matter too much.\nGrowth\nI then needed to create the annual growth rate that the Grattan Institute used (p. 34):\n\nGrowth in taxable income is calculated as a compound annual growth rate from the 2003-04 financial year to the 2014-15 financial year.\n\n\n\n## Data manipulation - create the new interest variable that is of interest - 11 years?\ntax_data <- tax_data %>%\n  mutate(\n    real_ave_inc_0304 = ave_inc_0304 * inflation_rate,\n    real_median_inc_0304 = median_inc_0304 * inflation_rate,\n    annual_ave_growth = (ave_inc_1415/real_ave_inc_0304)^(1/11),\n    annual_median_growth = (median_inc_1415/real_median_inc_0304)^(1/11)\n  )\n\nCorrespondence\nThe incomes data that the ATO makes available is on a postcode basis. I needed to convert this into 2011 SA3 levels and the ABS makes a correspondence for this purpose. This is available at: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.006July%202011?OpenDocument where the correct table is the zipped file ‘Postcode 2011 to Statistical Area Level 3 2011’.\n\n\n# Import the correspondence\ncorrespondence <- read_csv(\"data/1270055006_CG_POSTCODE_2011_SA3_2011.csv\", skip = 7, col_names = FALSE)\ncorrespondence <- correspondence %>%\n  select(X2:X6)\n# Fix the column names\ncorrespondence <- rename(correspondence, \n                   \"postcode\" = X2,\n                   \"SA3_CODE_2011\" = X3,\n                   \"SA3_NAME_2011\" = X4,\n                   \"ratio\" = X5,\n                   \"percentage\" = X6\n)\n\ncorrespondence <- merge(correspondence, tax_data, by.x = \"postcode\", by.y = \"postcode\")\nmatch\n\ncorrespondence <- correspondence %>%\n  mutate(\n    contribution_ave = ratio * annual_ave_growth,\n    contribution_median = ratio * annual_median_growth\n  )\n\nsa3_data <- correspondence %>%\n  group_by(SA3_NAME_2011) %>%\n  summarise(\n    ave_growth = weighted.mean(annual_ave_growth, ratio, na.rm = T),\n    median_growth = weighted.mean(annual_median_growth, ratio, na.rm = T)\n  )\n\nPutting it all together\nThis result of all this is that I created a dataset that seems pretty much the same as the one that the Grattan Institute mapped, but not the one that they released.\n\n\n#load(\"sa3_data.Rda\")\n#head(sa3_data, n = 20)\n\nMy dataset can be downloaded as a csv here: https://github.com/RohanAlexander/blogdown_website/blob/master/content/post/sa3_data.csv\nThe values underlying the Grattan Carto map and the values that I generated are the same for the five 2011 SA3 areas in the earlier table:\nSA3 Area\nGrattan Carto value\nGrattan data value\nMy value\nCairns - South\n1.73\n2.0\n1.73\nCharters Towers - Ayr - Ingham\n2.35\n1.6\n2.35\nFar North\n2.66\n3.0\n2.66\nOutback - North\n2.33\n1.7\n2.33\nPort Douglas - Daintree\n1.73\n1.4\n1.73\nTablelands (East) - Kuranda\n2.23\n1.6\n2.23\nAfter having compared my values with theirs I think that in their Excel dataset the Grattan Institute ordered the columns for the SA3 areas and the incomes data separately instead of together.\nReconciliation\nI’ve reached out to the Grattan Institute and will update this post based on what they say.\nConclusion\nThe Grattan Institute is probably Australia’s most important think tank in terms of not being overly associated with one side of politics but still making a contribution to thinking on policy. It is good that they are being more open about their datasets, but it would be much better if they made their code available. Using tools like Leaflet and ggmap instead of Carto would help with this.\nIt was fun to spend the day in the data-analyst’s version of a treasure chase. But hopefully the next time I decide to reproduce a Grattan Institute map common sense prevails before I go too far down the rabbit hole.\nI regret nothing :)Disclosures: In the interest of transparency I’ll point out that I applied unsuccessfully for a job at the Grattan Institute about five years ago. One friend works as a research assistant for them on a casual basis; and there are a few friends-of-friends who work there full time, but I haven’t talked about this post with any of them.\n\n\n",
    "preview": "posts/2017-08-11-reproducing-a-grattan-map/images/grattan.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 1888,
    "preview_height": 1336
  },
  {
    "path": "posts/2017-07-21-getting-started-with-blogdown/",
    "title": "Getting started with Blogdown",
    "description": "Blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-07-21",
    "categories": [],
    "contents": "\nThank you to Minhee Chae and Peter Gibbard for helpful comments.\nIntroduction\nBlogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.\nBlogdown lets you freely and quickly get a website up-and-running. It is easy to add content from time-to-time. It integrates with R Markdown which lets you easily share your work. And the separation of content and styling allows you to relatively quickly change your website’s design.\nThat said, using Blogdown is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customise many aspects of your website, or need everything to be ‘just so’ then Blogdown may not be for you.\nBlogdown is still under active development and various aspects may break in future releases. That said, the investment of time required to set up a Blogdown website is unlikely to be wasted. Even if Blogdown were shuttered tomorrow most of the content could be repurposed for a regular Hugo website.\nA Blogdown user-guide is being written by Yihui Xie, Amber Thomas, and Alison Presmanes Hill. The current draft can be viewed here: https://bookdown.org/yihui/blogdown/. Alison Presmanes Hill also has a very helpful post on getting started: https://apreshill.rbind.io/post/up-and-running-with-blogdown/.\nThis post is a simplified version of those two resources. It sticks to the basics and doesn’t require much decision-making. The purpose is to allow someone without much experience to use Blogdown to get a website up-and-running. Head to those two resources once you’ve got a website working and want to dive a bit deeper.\nSoftware installation\nSoftware\nTo use Blogdown you need R and R Studio.\nTo download R go to https://cran.csiro.au/ and download the version of R for your operating system. Follow the instructions to install R.\nTo download R Studio go to https://www.rstudio.com/products/rstudio/download/, scroll down to ‘Installers for Supported Platform’ and download the version of R Studio for your operating system (likely one of the first two links: Windows or Mac). Follow the instructions to install R Studio.\nPackages\nYou need to install the following packages: devtools, blogdown. To do this open R Studio and type the following into the console, hitting enter at the end of each line to run the command:\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rstudio/blogdown\")\n\nFolder management\nThis section follows Alison Presmanes Hill’s post closely. Go there for more information.\nGitHub\nIt will be easier to put your website on the internet if you have a GitHub account. To create a GitHub account, go to https://github.com/ and sign-up for a free account. This restricts you to making everything public, but as we are using GitHub for a public website that’s fine. Once you have an account, create a new repository by clicking on the plus and call it ‘my_website’.\n\n\n\nDon’t worry about including a readme or gitignore. Once you get to the ‘Quick setup’ page, copy the website address.\n\n\n\nTerminal\nOpen Terminal (either cmd + space then search for ‘Terminal’ or find it in your Applications). Use a combination of typing ls followed by ‘return’ and typing cd followed by ‘return’, to navigate to your ‘Documents’ folder. This is where your website will live for now.\nType git clone and paste the address you copied earlier and follow by ‘return’. This links that folder to your GitHub account.\nWebsite building\nCreation\nOpen R Studio and install Hugo via the blogdown package with the following code:\n\n\nblogdown::install_hugo()\n\nIn R Studio create a new project in the folder that you just created ‘my_website’. To do this click on: File -> New Project -> Existing Directory. Then navigate to the folder ‘my_website’. This will open a new R Studio session. Creating a project just adds a .proj file in the folder that makes it easier to come back to your website later.\nUsing that new R Studio session create your website with the following code:\n\n\nblogdown::new_site(theme = \"gcushen/hugo-academic\", theme_example = TRUE)\n\nThis will:\ndownload files into your ‘my_website’ folder;\nopen a R Markdown file that you can close for now; and\nbegin serving the site in your R Studio viewer.\nThe console and viewer of your R Studio session should look like this:\n\n\n\nInitial editing\nAt this point, the default website is being ‘served’ locally. This means that changes you make will be reflected in the website that you see in your R Studio Viewer. To see the website in a web browser click the ‘show in new window’ button on the top left of the Viewer. This is circled in the above image. That will open the website using the address that the R Studio also tells you.\nHeadshot\nThe first change to make is to update the headshot. In your folder, go to my_website -> static -> img. Replace ‘portrait.jpg’ with your own square headshot jpg. If you do this correctly then when you go back to your website the image will have updated.\nPersonal details, contacts, and main menu\nTo update the biography and other details in that first pane, go to File -> Open File in the R Studio menu and open config.toml which is in my_website -> config.toml. This file will either open in a text editor or in R Studio – it doesn’t matter which. When you save the file the changes will be reflected in the website.\nSearch for ‘title’ or go to line 2. It should say:\n\n\n'title = \"Academic\"'\n\nChange that to:\n\n\n'title = \"Your Name\"'\n\nSearch for ‘[params]’ or go to line 21. There you can update parameters such as name, role, and contact details. If you don’t want a particular parameter to show up on your website then set it equal to \"\". (An example of this is on line 33.)\nOnce you’ve updated these parameters, search for ‘[[params.social]]’ or go to line 126. There you can update your contact details, such as email, twitter, etc. Just delete or comment out the full four lines if you don’t want a particular contact type displayed on your website. You can always add more later.\nFinally, search for ‘[[menu.main]]’ or go to line 152. There you can change the menu items that are displayed on the top right of your website. For instance if you don’t want a blog then delete or comment out the four lines:\n\n\n[[menu.main]]\n  name = \"Posts\"\n  url = \"#posts\"\n  weight = 3\n\nIf you want to change the order of the items then change the ‘weight’. Ascending values from left to right.\nBiography\nIn your folder, go to my_website -> content -> home -> about.md. That should open in R Studio or your text editor. Any changes that you save should immediately show up in your website.\nSearch for ‘# List your academic interests.’ or go to line 12. There you can change your academic interests. If you don’t want this to show up on your website then you can just delete or comment out lines 12-18.\nSearch for ‘# List your qualifications (such as academic degrees).’ or go to line 20. There you can change your academic qualification. If you don’t want this to show up on your website then you can just delete or comment out these lines.\nThe ‘year’ is a numeric field. If you’d prefer to include duration (e.g. 2013 – 2017), then replace the ‘2012’ with ‘“2013 – 2017”’ (the \"\" are important). Or similarly, if you are expecting a degree then you could replace the ‘year’ with ‘“Expected month year”’.\nSearch for ‘# Biography’ or go to line 43. There you can add a brief biography.\nTeaching\nMost of the other files in my_website -> content -> home just display content from elsewhere. This is because of the setup of the website. The exception is teaching.md. Open that and edit everything after line 15.\nPublications\nIn your folder, go to my_website -> content -> publication. There are two default publications added there. You can edit those and then copy them to add extra publications.\nPosts\nIf you want a blog in your website then the content is saved in: my_website -> content -> post. If you don’t want a blog then just delete this folder and comment out the posts menu item from my_website -> config.toml file so it doesn’t show up in the menu.\nOnce your website is working, if you want a new blog post, then you can simply use the R Studio menu bar: Tools -> Addins -> New Post.\nEtc\nGo through the different parts and change it as you need.\nSubsequent editing\nTo come back to editing your website once you’ve closed R Studio, go to the ‘my_website’ folder and then double-click on the Rproj file, ‘blogdown_test.Rproj’. That will open a new instance of R Studio.\nFrom there you can type ‘blogdown:::serve_site()’ into the console to serve your site and then continue editing, or you could use the R Studio menu bar: Tools -> Addins -> Serve Site.\nMaking your website public\nCommit\nSo far everything has happened on your own computer. The first step to making your website public is to commit these changes to GitHub. To do this open Terminal again and as before use cd and ls to navigate to ‘my_website’.\nOnce there, type each of the following lines (adding your own description) and follow each by ‘return’\n\n\ngit add -A\ngit commit -m \"DESCRIBE THE CHANGE YOU ARE ADDING\"\ngit push\n\n(You may be asked for your GitHub password. Terminal is a bit tricky to type passwords into because you don’t know how many characters you’ve typed, but have a go and follow it by ‘return’.)\nyour_domain.netlify.com\nThere are many ways to make your website public, but the best at the moment is to use Netlify. I don’t have anything to change from the instructions of Alison Presmanes Hill and you can follow those here: https://apreshill.rbind.io/post/up-and-running-with-blogdown/#deploy-in-netlify.\n\n\n",
    "preview": "posts/2017-07-21-getting-started-with-blogdown/images/blogdown_github_1.png",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {},
    "preview_width": 2152,
    "preview_height": 1142
  },
  {
    "path": "posts/2017-07-18-mapping-the-2016-australian-election-polling-place-results/",
    "title": "Mapping the 2016 Australian Election Polling Place Results",
    "description": "The note that follows introduces Australia's political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-07-18",
    "categories": [],
    "contents": "\n\n\n\n\nThe note that follows introduces Australia’s political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map.\nAustralia’s political system\nIn 2016 Australia’s federal government was determined by the outcomes of elections in 150 divisions which each elected one member to the lower house. The Liberal/National Coalition won 76 seats which allowed it to form a majority government; while the Labor party won 69 seats to form the Opposition; the Greens and the Nick Xenophon Team each won one seat; and there were two Independent members (Andrew Wilkie and Cathy McGowan).\nVotes are cast at polling places in each division. In general voters can go to any polling place within their registered division, but some polling places that are close to a boundary will allow voting from there and some major polling places (such as the city hall of a state capital) will allow voting in any division.\nAlthough there are some exceptions divisions are generally constructed so that they each have roughly the same number of people. However this is not the case for polling places – some are much larger than others. Nonetheless it is interesting to see the geographic distribution of which party received the most first-preference votes in each polling place, especially in the context of which party won the division.\nPolling place data\nThe main packages for the data manipulation are the tidyverse and magrittr. leaflet allows the creation of interactive maps, ggmap creates static maps, and rgdal assists with dealing with geographic data. rmapshaper is used to reduce the size of the shapefile of division boundaries so that it is faster to load.\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(rgdal)\nlibrary(rmapshaper)\n\nThe polling place results can be downloaded by state from the AEC website at http://results.aec.gov.au/20499/Website/HouseDownloadsMenu-20499-Csv.htm. There the AEC also makes available a dataset that contains geocodes for each of the polling places. The separate datasets for each state need to be merged, and then each polling place needs to be geocoded. Finally some minor changes are needed to make the party names easier to follow.\n\n\n#### Read in the polling place datasets (are state specific), and the geocodes for each polling place. Then put it all together to have one geocoded polling place dataset for all of Australia: Australia_booths. Finally, create a dataset that is filtered so that it just shows the winner of each booth: Australia_booths_winner. ####\n# Data importing\ngeocodes <- read_csv(\"data/GeneralPollingPlacesDownload-20499.csv\", skip = 1)\nNSW_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NSW.csv\", skip = 1)\nQLD_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-QLD.csv\", skip = 1)\nVIC_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-VIC.csv\", skip = 1)\nACT_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-ACT.csv\", skip = 1)\nTAS_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-TAS.csv\", skip = 1)\nSA_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-SA.csv\", skip = 1)\nWA_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-WA.csv\", skip = 1)\nNT_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NT.csv\", skip = 1)\n# Merge\nAustralia_booths <- rbind(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# Add the geocodes\nAustralia_booths <- Australia_booths %>% \n  left_join(geocodes)\n# Clean up\nrm(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# If you need it use this to get a list of the parties, ordered by the number of first-preference votes\n# first_votes <- Australia_booths %>%\n#   group_by(PartyNm) %>%\n#   summarise(votes = sum(OrdinaryVotes, na.rm = TRUE)) %>%\n#   arrange(desc(votes))\n# Combine some parties that are separate, but equivalent: Australian Labor Party & Australian Labor Party (Northern Territory) Branch & Labor, Country Liberals (NT) & Liberal, The Greens & The Greens (WA).\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"Australian Labor Party (Northern Territory) Branch\" = \"Australian Labor Party\", \"Labor\" = \"Australian Labor Party\")\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"Country Liberals (NT)\" = \"Liberal/LNP\", \"Liberal National Party of Queensland\" = \"Liberal/LNP\", \"Liberal\" = \"Liberal/LNP\")\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"The Greens (WA)\" = \"The Greens\")\n# Create an indicator for who won the polling place then filter on that\nAustralia_booths_winner <- Australia_booths %>% \n  group_by(PollingPlaceID) %>% \n  mutate(polling_place_winner = ifelse(max(OrdinaryVotes) == OrdinaryVotes, max(OrdinaryVotes), 0)) %>%\n  filter(polling_place_winner >= 1)\n#table(Australia_booths_winner$PartyNm)\n# There are three parties that only win one booth, so combine all those into 'Other'\nAustralia_booths_winner$PartyNm <- recode(Australia_booths_winner$PartyNm, \"Australian Recreational Fishers Party\" = \"Other\", \"Christian Democratic Party (Fred Nile Group)\" = \"Other\", \"Derryn Hinch's Justice Party\" = \"Other\")\n\nDivision data\nThe divisions can be coloured based on which party won overall. The map of the boundaries for each division can be downloaded from the AEC website here: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm. The shapefile doesn’t have winner of each division so this needs to be merged into it. It is important to put the shapefile dataset first when merging. Finally, the shapefile is quite a large file and this can be reduced for faster loading.\n\n\n#### Read in the shapefiles (maps) that show each of the boundaries of the divisions (electorates) then add the data to say who won that division. Result is a spatial dataframe called boundaries. ####\n# Overall winner for each division, which will be used to color the division\nDivision_winner <- read_csv(\"data/HouseMembersElectedDownload-20499.csv\", skip = 1)\n# The boundaries of the divisions (downloaded from: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm)\nboundaries <- readOGR(dsn = \"data/national-midmif-09052016/COM_ELB.TAB\", layer = \"COM_ELB\")\n# Fix a couple - Mcmillan and Mcpherson - that have capitalisation issues\nboundaries$Elect_div <- recode(boundaries$Elect_div, \"Mcmillan\" = \"McMillan\", \"Mcpherson\" = \"McPherson\")\n# Add the overall division winner dataset into the boundaries dataset (thanks to http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS2_MergingSpatialData_part1_Joins.html)\nboundaries <- merge(boundaries, Division_winner, by.x = \"Elect_div\", by.y = \"DivisionNm\")\n# Simplify and reduce the size of the shapefile so that it loads better\nobject.size(boundaries)\nboundaries <- rmapshaper::ms_simplify(boundaries)\nobject.size(boundaries)\n# Clean up\nrm(Division_winner)\n\nThen colours need to be associated with each party.\n\n\n#### Specify the colour schemes that will be used. ####\n# Set the color scheme for the booth coloring\n# pal <- colorFactor(\n#   palette = \"Dark2\", \n#   domain = unique(Australia_booths$PartyNm)\npal <- colorFactor(palette = c(\"#c04745\", \"#616161\", \"black\", \"purple4\", \"#4776be\", \"#ff5800\", \"cyan1\", \"yellow\", \"#a8c832\", \"brown4\"), \n                          domain = c(\"Australian Labor Party\", \"Independent\", \"Informal\", \"Katter's Australian Party\", \"Liberal/LNP\", \"Nick Xenophon Team\", \"Other\", \"Pauline Hanson's One Nation\", \"The Greens\", \"The Nationals\"))\n# Set the color scheme for the division coloring\npall <- colorFactor(palette = c(\"#c04745\", \"#616161\", \"purple4\", \"#4776be\", \"#4776be\", \"#ff5800\", \"#a8c832\", \"brown4\"), \n                   domain = c(\"Australian Labor Party\", \"Independent\", \"Katter's Australian Party\", \"Liberal\", \"Liberal National Party\", \"Nick Xenophon Team\", \"The Greens\", \"The Nationals\"))\n\nInteractive map\nFinally, the map can be produced:\n\n\n#### Pull it all together to make the map ####\n# Make the map\nAustralia_map <- \n  leaflet() %>%\n  # Base groups\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %>% # Add a black and white alternative\n  setView(lng = 133.7751, lat = -25.2744, zoom = 4) %>% # Specify where the map is initially focused\n  addPolygons(data = boundaries, \n              color = \"#444444\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillColor = pall(boundaries$PartyNm),\n              highlightOptions = highlightOptions(color = \"#666\", weight = 2, bringToFront = FALSE)) %>% # Add the plot of the divisions, coloured by which party won it\n  addCircles(\n    data = Australia_booths_winner,\n    lng = Australia_booths_winner$Longitude, \n    lat = Australia_booths_winner$Latitude, \n    popup = paste(\"<b>Division:<\/b>\", as.character(Australia_booths_winner$DivisionNm), \"<br>\",\n                  \"<b>Polling place:<\/b>\", as.character(Australia_booths_winner$PollingPlaceNm), \"<br>\",\n                  \"<b>Address:<\/b>\", as.character(Australia_booths_winner$PremisesAddress1), \"<br>\",\n                  \"<b>Party with most first-pref votes:<\/b>\", as.character(Australia_booths_winner$PartyNm), \"<br>\",\n                  \"<b>First-pref votes:<\/b>\", as.character(Australia_booths_winner$OrdinaryVotes), \"<br>\"),\n    label = ~as.character(Australia_booths_winner$DivisionNm),\n    #clusterOptions = markerClusterOptions(),\n    color = pal(Australia_booths_winner$PartyNm),\n    fillOpacity = 0.5) %>% # Plot the booths, coloured by which party got the most first-preferences.\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"Toner Lite\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>%\n  addLegend(\"bottomright\", pal = pal, values = Australia_booths_winner$PartyNm,\n            title = \"Which party won\",\n            #labFormat = labelFormat(prefix = \"$\"),\n            opacity = 1\n  )\n# Call the map\nAustralia_map\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-15-greitens-reports-for-duty/",
    "title": "Greitens Reports For Duty",
    "description": "Eric Greitens may be the Republican Übermensch. Rhodes Scholar, Navy SEAL officer, husband and father. He's now the Republican candidate in the Missouri gubernatorial election. And one suspects that being a governor could just be a step for Greitens. While 2016 will always be the year that US politics descended to the gutter, it could also be the year that the next Republican president begins his political career.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-10-15",
    "categories": [],
    "contents": "\n(Comment 27 March 2019. This has aged horribly! Not only was it written with a presumption that Clinton was going to win, but even though Greitens won he flamed out within 18 months having been accused of some very odd behaviour. Nonetheless, the visit to Missouri was fun, and I enjoyed putting this piece together.)\nEric Greitens may be the Republican Übermensch. Rhodes Scholar, Navy SEAL officer, husband and father. He’s now the Republican candidate in the Missouri gubernatorial election. And one suspects that being a governor could just be a step for Greitens. While 2016 will always be the year that US politics descended to the gutter, it could also be the year that the next Republican president begins his political career.\n\n\n\nIt is easy to imagine Eric Greitens as a senior at Duke University as he entered the boxing ring for his Golden Gloves bout. He may liked to have chewed on his mouth guard, and thrown a few jabs. But instead of touching gloves with an opponent to start a fight, Greitens entered the ring and was declared the winner. Greitens became the Golden Gloves Novice Champion without throwing a punch in anger – an opponent didn’t even show up.\nIn some ways, it’s unfair to mention this story. Greitens doesn’t hide the fact that he won unopposed – it was a key chapter in the memoir that acted as his political job application. But it is important to recognize how straightforward Greitens makes high achievement seem. It is easy to wonder if Greitens has ever felt close to failing.\n\n\n\nAt a recent rally of about 50 people in Des Peres on the outskirts of St Louis, Missouri, Greitens entered stage right. Greitens has lashed his political persona to his time as a Navy SEAL and styles himself as a ‘conservative outsider’. He started off g-droppin’ but then forgot himself and hints of his education could be heard. Greitens spoke like a head boy eager to impress – no fillers such as ‘um’ or ‘uh’, with plenty of appropriate gestures, and useful timing and cadence changes.\nHe’s still a little too polished to pull off George W. Bush’s everyman. He’s not yet comfortable enough in the weeds to give speeches about complicated ideas like Bill Clinton. And Greitens is not yet Obama in terms of ability to inspire. But Greitens is much better than many politicians at this stage of their career. It is important to recognize that events like this are the extent of many aspiring politician’s careers. And if he loses this race, Greitens surely has a plan for a political career that will be anything but average. Nonetheless, from his stump speech it is clear that if elected Greitens would like to be evaluated on jobs and education.\n\n\n\nIf you had to balance the US population like a plate based on where people live then the center would lie in Missouri. It is also in the middle of many measures of jobs and education, such as the unemployment rate and the percentage of the population with at least a high school education. Between 1904 and 2004 whichever candidate won Missouri also won the overall election with the exception of 1956. But, in 2008 and 2012 the state voted Republican even though Obama won overall.\nEconomically, Missouri did not have many of the financial sector jobs that were directly affected by the financial crisis of 2007-08. Nonetheless the state was hit hard. According to data from the St Louis Federal Reserve in May 2007 Missouri’s unemployment rate was 5.0 per cent, while the US unemployment rate was 4.4 per cent. A little over two years later, in December 2009, Missouri’s unemployment rate had reached 9.8 per cent while that of the broader US economy was 9.9 per cent. Since that time, Missouri’s unemployment rate reduced to 4.2 per cent in February 2016. But in contrast to the broader US economy during the past six months it has crept up. The September 2016 measure puts Missouri’s unemployment rate at 5.2 per cent, while that of the broader US economy was 4.9 per cent.\nGreitens didn’t mention many specific economic plans, but as it is only a few hours south of Chicago the location of St Louis, Missouri’s second-largest city, brings many economic opportunities. For instance St Louis could piggyback on the large number of tech firms that are starting in Chicago. If he wins, Greitens could convert some of the old St Louis warehouses into tech-friendly offices and offer any business coming out of a Chicago incubator subsidized rent if they, say, take on at least one intern from a Missouri college. Sure, most of the businesses will not survive, but those that will may stay in St Louis if, as Governor, Greitens ensures they plant roots.\nOn education Greitens gave the Republican rank-and-file what they wanted. The only point of agreement among warring Republican factions this presidential cycle seems to be an agreement that Common Core is terrible. It is worth remembering that Common Core is essentially just a set of education standards established by the federal government, but with considerable implementation leeway at a state level. Nonetheless Republicans from Jeb Bush to Donald Trump oppose it, and so does Greitens. Greitens also brings unions into the education debate, insisting that teachers should not be forced to join a union as a condition of employment.\nWhile neither of these issues are likely to really help Missouri improve its educational outcomes, they do suggest that Greitens has at least some understanding of the importance of give-and-take in politics. And there are measures, such as increased support for vocational training and improved teacher training and support that he could easily tweak if he wins.\nGiven Missouri’s circumstances there is plenty of opportunity for improvement and Greitens isn’t short on ambition. Reading his first book, ‘The Heart and the Fist’, published in 2011, makes it clear that running for office has always been his plan. And the website ‘ericgreitensforpresident.com’ was registered at least as early as 21 July 2009, so it’s clear what his goal is. Greitens makes life-defining achievements seem easy, but the swiftboating of John Kerry in 2004 showed how easy it can be to muddy a political opponent.\nGreitens has already picked up a couple of bad habits. At the rally he referenced his humanitarian work in Kosovo. But in his memoir, the humanitarian work is described as happening in Croatia. While there are undoubtedly nuances due to historical and cultural factors that make the issue complicated, it is also the case that the largest population of Kosovars outside of the country lives in St Louis. This difference has already been noted by at least one local newspaper and while it’s not damning, Greitens should be more careful.\n\n\n\nGreitens also seems to have an unexpected aversion to openness, such as not releasing his tax returns. This is odd given that for so much of his life his primary income would likely have come from the Navy and then a non-profit that he founded. He’d have some income from public speaking fees, but it’s hard to see why these would be embarrassing.\nThere’s also a lot that needs to be improved in his campaign. For instance, there was no staffer at the door to the rally gathering the contact details of attendees. And there were no merchandise sales either at the rally or on Greitens’ campaign website.\n\n\n\nWhether he wins or loses the Missouri gubernatorial race on November 8, this won’t be last we hear of Eric Greitens. In four years, he could run again for Governor or even against the Democratic Senator from Missouri, Claire McCaskill. The Republican 2020 cohort is firming with Senator Tom Cotton, Speaker Paul Ryan, and Governors John Kasich and Mike Pence already positioning themselves.\nGreitens is not perfect. But those who serve in the military are among the best of us. And Rhodes Scholars are among the cleverest. His success as a politician will be worth watching.\n\n\n",
    "preview": "posts/2016-10-15-greitens-reports-for-duty/images/StLouis_Greitens_1.jpg",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-08-professional-amateurs/",
    "title": "Professional Amateurs",
    "description": "Despite many unforced errors Hillary Clinton has won the Democratic nomination and polls suggest she will beat Donald Trump. But her campaign continues to make unforced errors. There was plenty of evidence of an amateur nature to what should be a professional campaign at a recent rally for Clinton’s running mate, Tim Kaine, in Grand Rapids, Michigan.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-09-08",
    "categories": [],
    "contents": "\nDespite many unforced errors Hillary Clinton has won the Democratic nomination and polls suggest she will beat Donald Trump. But her campaign continues to make unforced errors. There was plenty of evidence of an amateur nature to what should be a professional campaign at a recent rally for Clinton’s running mate, Tim Kaine, in Grand Rapids, Michigan.\n\n\n\nThe wait for Tim Kaine was around three hours. That’s not unusual. But it meant the campaign had three hours in which no one was able to walk away or claim they had somewhere else to be. They had three hours to turn supporters into advocates. They blew it.\nWe wrote our contact details on a piece of paper while we waited in line. Presumably a volunteer’s time was later spent transferring the scrawls into a computer. Why not have us enter our contact details ourselves into a tablet? It’s the work of just a few hours to make an app to do that.\nLocal campaign volunteers spent a lot of time leading chants while we waited in line. Americans seem to like this, and enthusiasm is important - but it should be captured and used in ways that get more votes. Everyone that I talked to was going to vote for Clinton, but what about their friends, families and neighbors? Many in the line complained that there was no easy way to volunteer. They were so fired up against Trump that they wanted to do something right away. Why did the Clinton campaign not capture this enthusiasm by using the time spent in the line to organize? It would have been easy to get those supporters to commit to doorknock their neighborhood.\n\n\n\nTo actually get into the event you had to hand-over the piece of paper with your contact details. Why did the campaign not send an SMS or email that could be shown to gain entry, thereby checking that the information was accurate? Even better, the local campaign could have made an organizing app, had people download it and then sent an entrance ticket using that. Sure, some people don’t carry phones or aren’t comfortable with technology, but it’s easy enough to use pens and paper for them.\nFinally, there no merchandise sales at the door. T-shirts are an easy source of $10 - $20 per person, and buttons an easy source of $5. With about 500 people at the event, local campaigns could have easily raised at least a few thousand dollars.\n\n\n\nClinton admits that she is not a good retail politician. And that’s fine. It may make it more difficult to get elected, but in many ways it could be an advantage when it comes to actually being president. However, the Clinton campaign didn’t seem able to even do the basic task that wins elections - organizing supporters to talk to their friends, family and others in their community.\nTrump is an extraordinary factor pushing supporters toward Clinton. But her campaign needs to establish a network of committed supporters that it can lean on in 2020 when she will ask for a fourth Democratic term. Clinton has made many unforced errors and gotten away with it. But there is no good reason that her campaign should not be better at organizing. Although it looks as though she will beat Trump, she’s risking 2020 if her campaign doesn’t stop making these sorts of basic errors.\n\n\n\n\n\n",
    "preview": "posts/2016-09-08-professional-amateurs/images/ProfessionalAmateurs-WelcomeSign.jpg",
    "last_modified": "2020-12-19T13:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-17-trump-revisited/",
    "title": "Trump, Revisited",
    "description": "Donald Trump is an improved politician, but it's unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-07-17",
    "categories": [],
    "contents": "\n(Comment 4 July 2017: Like much of my writing about Trump, the opinions here haven’t aged too well. Nonetheless, I learnt a lot from writing this and later from thinking about why I went wrong.)\nA few notes and photos from a Trump rally in Indiana earlier this week. The focus is on whether Trump ‘could’ win the election in November. In the interest of transparency, it’s worth acknowledging that I didn’t think Trump could win the Republican nomination. Thanks to Monica for helpful edits.\n\n\n\nDonald Trump is an improved politician, but it’s unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election.\nAt a recent rally in Westfield, Indiana, Trump was comparatively structured and measured. There was less name-calling and little of the ludicrous hypotheticals that characterized a January rally in Iowa.1 2 Some things haven’t changed: the crowd is still overwhelmingly white; ‘Storm Trumpers’ in ill-fitting suits are still on patrol; and Trump still lies.3 But he is no longer politically inexperienced, and neither were the Trump supporters that I talked to.\n\n\n\nThe Republican Governor of Indiana, Mike Pence, introduced Trump at the rally. Pence was later announced as Trump’s running mate. If Trump is to win the election then he needs to easily win states such as Indiana, where Republicans have only lost once in the past fifty years.4 Even though the outcome in Indiana should not be in doubt, Hoosier Republicans are important. Trump needs fired-up volunteers to travel to neighboring Ohio, a crucial swing state, and Trump needs money. But mostly, Trump needs friends.\nDespite Trump’s improvement as a politician, winning a presidential election usually requires constructing coalitions. Often this is the work of a lifetime. For instance, Richard Ben Cramer describes how, beginning in his 20s, George H. W. Bush built a Christmas card list. By the time he was Vice President 30,000 ‘friends’ received an annual Christmas card from him.5 The Clintons have been building coalitions since their 20s too. Part of Trump’s appeal is that he has only been a politician for a year, but his campaign is inefficient without coalitions.\n\n\n\nThe most precious resource in any election is a candidate’s time. The US presidential election magnifies this because of the scrutiny, the electoral college, and the size of the country. Yes, a candidate needs to raise money, motivate supporters, and convince undecided voters. But to stand a chance of winning, a candidate usually also needs coalitions that can do all this for them. Without these, there is more pressure on Trump.\nTrump has also only recently put modern campaign essentials in place. For instance, just a few months ago Trump described the use of data in politics as ‘overrated’.6 But he seems to have changed his mind: Trump collected and verified the phone numbers of those who attended the rally in Indiana. Texts and phone calls will be critical to the effort of getting his supporters to turn out to vote in November. Trump is also now sending emails but it takes time to build a high-quality list.\n\n\n\nThe Trump supporters that I spoke to were unfailingly polite. They conscientiously thanked the many law enforcement personnel, and there were many military veterans in the audience. No one supported every aspect of Trump’s platform, but this is not unusual in political campaigns. There was some anti-Muslim sentiment, and a few conspiracy theories. Political correctness was a recurrent issue, as was declining US influence in the world. Although Trump uses ‘The Wall’ as a call-and-response device (Trump: ‘Who’s going to pay for the wall?’ Crowd: ‘Mexico’), anti-Mexican sentiment seemed to be driven more by a perceived willingness of Hispanics to work for low wages than racism.\nThe Republicans I talked to were united only in being against Clinton; but similarly many Democrats seem united only in wanting to stop Trump. How such a campaign translates into votes is unclear. While there don’t seem to be many undecided voters, there are many dejected ones. It will be interesting to see turnout estimates in swing states. Would you stand in line to vote against, rather than for, a candidate?\n\n\n\nIn just one year Trump has changed US politics. He is quickly improving as a politician, but remains divisive. Is Trump the moment, or just of the moment? Although he probably does not have enough time to do the work that would allow him to win in November,7 the impact of his campaign will be felt for many years.\nGo to https://www.rohanalexander.com/2016/01/14/notes-and-photos-from-iowa/ for that write-up.↩\nThe speech can be viewed here: https://youtu.be/ewMhP-V1ed8, as at 15 July 2016.↩\nSee, for instance, Politifact (http://www.politifact.com/personalities/donald-trump/) which awarded Trump PolitiFact’s 2015 Lie of the Year and rules 58 per cent of his statements as either ‘False’ or ‘Pants on Fire’.↩\nSee the entry for Indiana here: https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state, as at 15 uly 2016.↩\nSee Richard Ben Cramer’s ‘What It Takes’, page 153 of the Vintage; Reprint edition (June 1, 1993).↩\nSee: http://bigstory.ap.org/article/6d588a38061c4657a557d1dde86782ec/trumps-questioning-value-data-worries-republicans, accessed 15 July 2016.↩\nIt is estimated that Trump currently has a 20-40 per cent chance of winning the election. For instance, see the Five Thirty Eight election forecast (http://projects.fivethirtyeight.com/2016-election-forecast/), as at 15 July 2016 or the New York Times summary of election polls (http://www.nytimes.com/interactive/2016/us/elections/polls.html), again as at 15 July 2016.↩\n",
    "preview": "posts/2016-07-17-trump-revisited/images/2016_07_17_Children.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-03-broader-thinking-needed-on-the-australian-budget/",
    "title": "Broader Thinking Needed on the Australian Budget",
    "description": "The Treasury Secretary, and many others, bemoan the wasted years of the mining boom. Most agree that Australia should have more to show for what was the most significant boom since Federation. But the boom is over. And a fixation on budget surpluses means that we are missing an opportunity to make up for it. Australia’s credit rating is a strength that we should take advantage of. The Commonwealth should be borrowing to fund infrastructure investment.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-02-03",
    "categories": [],
    "contents": "\nThe Treasury Secretary, and many others, bemoan the wasted years of the mining boom. Most agree that Australia should have more to show for what was the most significant boom since Federation. But the boom is over. And a fixation on budget surpluses means that we are missing an opportunity to make up for it. Australia’s credit rating is a strength that we should take advantage of. The Commonwealth should be borrowing to fund infrastructure investment.\n\n\n\nDespite what many think, ‘deficit’ is not a dirty word, and like many other aspects of life, context matters. For instance, students are rarely chastised for taking a HELP loan. This is because that debt is being used to buy an education, which helps in the long term. It’s the same for the economy.\nIn a recent speech to the Sydney Institute the Treasury Secretary said that the Federal Government must exercise ‘expenditure restraint (that) will allow resources that would otherwise go to interest payments to be allocated to other priorities…’.1 But why should we lash expenditure restraint to interest payments? The Secretary explains that his rationale for this comes from the lessons of the late ’80s and early ’90s. But interest rates are no longer at such high levels. If the costs of borrowing have changed, then should we still dismiss its benefits?\nAdditionally, it is not clear why it should be the case, as the Secretary claims, that if the Commonwealth achieves surpluses then the states would ‘…run small(er) overall deficits that they can use to finance productive infrastructure investment’. Even if state governments were to identify and fund the best investments for their state, shouldn’t the Commonwealth be concerned about what would be best for Australia overall?\nThe Secretary implies that interest payments are wasted money and so we should do whatever it takes to reduce this. But to focus only on the cost of borrowing means to miss out on the benefits. For instance, many Australians choose to incur the cost of making interest payments so that they can enjoy the benefits of owning a house. Why should the Commonwealth be any different? If the Commonwealth were to issue bonds to fund projects in the public interest, it would be following in the footsteps of many governments since Federation.2\nThe current low interest rate environment is handy because it allows us to cheaply borrow money. This is especially true of the Commonwealth, which could issue bonds for a low cost at the moment. Even if there were no obvious efficient infrastructure projects available right now, locking in the money at these levels would be no bad thing even if it takes a year or two to identify appropriate projects.\nTaking on debt is actually what many large corporations are doing at the moment. For instance, Apple recently issued bonds and has now raised more than $55 billion since 2013.3 And Visa raised $16 billion via bond sales before Christmas.4 For what it is worth, the Commonwealth’s credit rating is better than either of those businesses.\nInterest payments are only inherently a waste of money if the principal is wasted. But if it is being used for national infrastructure projects then the question is more nuanced. The circumstances that the Australian economy faces are different to those of the early ’90s, and our policy responses and public debate should remain up-to-date.\nhttps://treasury.gov.au/speech/the-australian-budget-some-context/↩\nhttp://www.rba.gov.au/publications/rdp/2012/pdf/rdp2012-09.pdf.↩\nhttp://www.bloomberg.com/news/articles/2015-09-10/apple-said-to-market-euro-bonds-adding-to-53-billion-debt-binge↩\nhttp://www.bloomberg.com/news/articles/2015-12-09/visa-said-to-start-marketing-bonds-backing-visa-europe-takeover↩\n",
    "preview": "posts/2016-02-03-broader-thinking-needed-on-the-australian-budget/images/DSC_0599.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-14-notes-and-photos-from-iowa/",
    "title": "Notes and Photos From Iowa",
    "description": "Bernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-01-14",
    "categories": [],
    "contents": "\nSincere thanks to Bec, Callam, Monica, and Owen for reading and improving these notes.\nBernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm.\n\n\n\nFigure 1: Photo of Bernie Sanders and audience at Perry, Iowa, by Monica Alexander.\n\n\n\nSo much calm, in fact, that some audience members who walked into the auditorium undecided, walked out excitedly supporting Sanders’ ‘socialist’ revolution. His revolution includes, amongst other features, universal medical coverage, paid maternity leave and a $15 hourly minimum wage; hardly revolutionary notions in many developed countries. For instance, few politicians that are against these policies get elected in Australia. However, Sanders will test whether a politician who supports them can get elected in the US.\nMuch has been made of Sanders’ recent polling. But the Iowa Caucasus, especially for Democrats, are a test not just of support, but enthusiasm and strength of will. Participants do not vote, they caucus. This means they gather with their neighbours and publicly indicate their support of a candidate - sometimes by raising a hand, sometimes by moving to a side of a room. Supporters must resist peer pressure, and continue supporting their candidate even as others try to sway them.\nIn addition to popularity, winning in Iowa requires organisation and attention to detail. This is why the operatives of Secretary Clinton, who is 68, remain content despite the polls. Caucusing is onerous, especially for those with children or without transport. A campaign is only as good as the number of supporters that it can get to turn up on the night. Clinton operatives are quick to mention their advantage in terms of this ‘ground game’.\nClinton’s ground game advantage is partly due to experience, but it is also due to money. Sanders unexpectedly raised $33 million in the final quarter of 2015, which compares favourably with Clinton’s $37 million, but Clinton has other sources of financial support. Sanders’ supporters should hope that the evident lack of preparation at the event in Perry, Iowa, itself (for instance, Sanders had to repeatedly ask for water as none had been left on the podium, nearly losing his voice on occasion) are not indicative of broader organisational oversights.\n\n\n\nFigure 2: Photo of Trump protesters.\n\n\n\nDonald Trump is different. More than a thousand people watched Trump, who is 69, speak in Cedar Falls, Iowa. Almost all were white. What he lacks in substance or structure, Trump makes up for with self-confidence. For 40 minutes he verbally picked at this and that, discussing polls, as well as goading opponents via ‘hypotheticals’ and name-calling. The audience was allowed no questions.\n\n\n\nFigure 3: Photo of Donald Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nTrump does not campaign in poetry (although he did literally recite song lyrics) and he seems unlikely to govern in prose. His operatives were clad in ill-fitting suits and shiny leather shoes that looked newly purchased. These ‘Storm Trumpers’ were unfailingly polite, but nonetheless menacing. The loudspeaker request, moments before Trump spoke, to not physically harm protesters was chilling rather than reassuring, not least since it was followed by laughter from the crowd. The comfort of knowing that the Secret Service would have confiscated any knives or guns at the door was relative rather than absolute. [Edit 26/1/16: Trump has since asserted that he could shoot somebody and not lose any voters. You get the uneasy sense that it would be better for his theory to remain untested, for fear that he may onto something.]\n\n\n\nFigure 4: Photo of Donald Trump audience at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nTrump’s political inexperience seems matched by that of his supporters. His warm-up acts (one of whose qualification, as she explained, was being a runner-up on The Apprentice) spent considerable time explaining the importance of turning out to caucus on 1 February. Unlike the Democrats, Republican caucus-goers do not have to be as resistant to peer pressure - secret ballots are possible. But much of the crowd seemed new to the political process, and getting each of them to turn out, and in some cases register as Republican, may be too much to expect. If so, then it is likely that Senator Ted Cruz, a 45-year-old conservative Republican from Texas, will prevail.\n\n\n\nFigure 5: Photo of Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nFor all their differences, it is the same anger that propels Sanders and Trump toward the top of the polls. Neither candidate is an establishment member of their respective parties – Sanders only joined earlier this year despite having generally voted with the Democrats as a senator, and Trump appears to swear allegiance only to himself. It is the feeling of being let down by the status quo, of the system needing a catalyst for something more, that drives their popularity.\nWhile Sanders’ policies may not make much difference for today’s caucus-goers, he speaks to their concern that their children’s lives may not be better than their own. His is an appeal for hope. Trump’s appeal is to those who feel they are worse off now than they were in the past. He gives them someone to blame, and provides solutions such as tariffs and walls, that some see as plausible. Perhaps for Sanders the glass is half-full; for Trump, half-empty?\nThe economic reality is that feeling worse off is reasonable for many Americans. After accounting for inflation the 2014 measure (which is the most recent one) of American household median income is lower than it was in 1997. And, as both Trump and Sanders accurately explained to their audiences, the 5 per cent unemployment rate that President Obama appeals to as a measure of his success is artificially low because some have given up looking for work.\nThose suffering most from the success of Sanders and Trump are candidates such as John Ellis Bush. Jeb, 62, seems to have carefully studied the art of coming across as a nice guy, albeit one who is a little annoyed about having to speak to about 200 people in Coralville, Iowa. Being a Bush comes with baggage and expectations, but it does have its advantages, such as immaculate event advance work and plenty of press.\n\n\n\nFigure 6: Photo of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\nJeb emphasised his commander-in-chief credentials. His older brother’s war in Iraq may have ensured that the age of aggressive American imperialism is over for now, but Jeb was still introduced by a retired Admiral who spoke of Jeb’s leadership fighting hurricanes in Florida and neighbouring states.\n\n\n\nFigure 7: Photo of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\nThe crowd was lively and Jeb was frequently interrupted by cheers. He spoke without notes for about 30 minutes to a crowd surrounding him on all four sides. Maybe this is a plan to seem more approachable, or to encourage audience participation? An hour of questions, courtesy of roaming microphones, followed. Jeb worked hard to be nice, but it was apparent that he was, indeed, working at it, because he occasionally lapsed and gently made fun of a questioner.\nFor all the well-acknowledged flaws of presidential systems in general, and the US one specifically, it has much to be proud of. Every candidate for their party’s nomination will visit Iowa at some stage this week, many visiting multiple towns in a day. The scrutiny is intense. Iowa is a state of 3 million people, divided into 99 counties, and by 1 February, many candidates will have visited every county.\n\n\n\nFigure 8: Photo in Iowa, by Monica Alexander.\n\n\n\nIf one were starting from scratch, the Iowa Caucuses would probably not be the way to go. Iowa is not representative of the rest of the United States, and its outsized electoral importance skews national policy. But in their own way, the Iowa Caucuses are valuable. In general, the nature of the candidates seems to come through in the events because they are so intimate. Given that so much of leadership consists of reacting to unexpected events, and the difficulty that politicians tend to have implementing their desired policies, perhaps this is the most important consideration. It may be that, to rework Churchill’s aphorism, the Iowa Caucuses are the worst way to select presidential candidates, apart from all the others.\n\n\n\nFigure 9: Photo in Iowa, by Monica Alexander.\n\n\n\nThere are many aspects of the United States that should not be considered, let alone encouraged, in Australia. But the rigorous examination of candidates facilitated by the Iowa Caucuses is something Australia should emulate.\n\n\n",
    "preview": "posts/2016-01-14-notes-and-photos-from-iowa/images/2016_01_11_Bernie.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-31-prepare-for-future-economic-crises-now/",
    "title": "Prepare For Future Economic Crises Now",
    "description": "Few policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2015-08-31",
    "categories": [],
    "contents": "\nOriginally published in the Canberra Times.\nFew policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change.\n\n\n\nFigure 1: Always better to buy a helmet before you fall off the bike.\n\n\n\nAfter the collapse of the US housing market, Lehman Brothers’ bankruptcy became an economic Rorschach test. Policymakers at the Reserve Bank of Australia and the Australian Treasury saw it as evidence that some banks cannot be allowed to fail. In response, the RBA reduced the cash rate and the Treasury implemented a stimulus package.\nWith the benefit of hindsight, some (such as Nobel Laureate Paul Krugman) argue that we should have anticipated the collapse. This is more than just wishful thinking. We now know there were those (for instance Raghuram Rajan, now the Governor of the Reserve Bank of India) whose warnings were more or less spot-on. Unfortunately, they were mostly ignored. It just seemed too unlikely that trouble in the US housing market could cause widespread recessions.\nIn an effort to prevent a repeat of the financial crisis, policymakers at the RBA, Treasury and elsewhere have taken steps to mitigate the weaknesses of the financial sector. And they have studied how those weaknesses could affect the broader economy. But there is a danger that they will overlook economic threats that do not originate in the financial sector. Like a general who trains troops to fight past wars, we may again be caught unprepared.\nThe financial crisis taught us that even infallible elements of an economy can fail. But because such failure is difficult to imagine ex ante, there appears to have been little attempt to act on this lesson. One way would be to practise dealing with unfamiliar – and less obvious – economic threats.\nThe process should begin by systematically examining the economy for weaknesses beyond the financial sector. Policymakers need to identify which elements of the economy pose systemic risks. We now know about the financial sector and there are many initiatives to deal with its risks, but we need to search for others.\nPolicymakers need to stress-test elements of the economy that appear robust and unfaltering, not just the financial sector. And they should conduct scenario analysis to practise responding if those elements were to fail. Just as the possibility that the US sub-prime housing market could cause recessions was almost unfathomable in 2006, the cause of the next economic crisis could seem unlikely to us now. Policymakers need to search broadly for possibilities.\nOf course most of the scenarios that are considered will never occur, and policymakers should not be given carte blanche to make legislative changes. What is needed is less heavy-handed but more useful: policymakers need to publicly consider and practise reacting to, unexpected events. This makes it more likely that they will react well when an unexpected crisis hits.\nThe financial crisis made it clear that unexpected elements of an economy can fail. Policymakers should be better prepared for this. They must fix the weaknesses identified by the financial crisis and deal with current threats. But importantly, policymakers must also give consideration to more obscure risks. It is only through this effort that we can hope to build an economy that is more resilient to future crises.\n\n\n",
    "preview": "posts/2015-08-31-prepare-for-future-economic-crises-now/images/DSCF1945.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-24-in-appreciation-of-ronald-coase/",
    "title": "In Appreciation of Ronald Coase",
    "description": "Ronald Coase, one of the most influential economists of the twentieth century, passed away in 2013 aged 102. Reading his papers today, I wonder whether he'd have become an economist if he were making that decision now.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-12-24",
    "categories": [],
    "contents": "\nRonald Coase, one of the most influential economists of the twentieth century, passed away in 2013 aged 102. Reading his papers today, I wonder whether he’d have become an economist if he were making that decision now.\n\n\n\nFigure 1: Coase’s article about lighthouses is brilliant. Which economist could write a paper like that today? Photo by Monica Alexander.\n\n\n\nCoase began his career by publishing ‘The Nature of the Firm’ in 1937. That article reported the results of Coase’s year-long study of American firms while he was an undergraduate at the London School of Economics (LSE). Until that article was published it was not clear why firms should do better when they are free to interact in a market, yet most individuals give up that freedom by tying themselves to a firm. Coase explained that it was due to transaction costs.\nAt the time the article was published Coase was teaching at the LSE, but this was soon interrupted by World War II. As he describes, during the war Coase ‘entered government service doing statistical work, first at the Forestry Commission and then at the Central Statistical Office, Offices of the War Cabinet’. He returned to the LSE after the war.\nIn 1951 he moved to the US taking a position first at the University of Buffalo then at the University of Virginia. He studied issues such as the allocation of radio frequency spectrum, which lead to the publication of ‘The Problem of Social Cost’ in 1960.\nThat article was concerned with externalities, that is, when an individual’s decision affects others. Think of factory that pollutes a river thereby affecting the livelihood of downstream fishermen. Although our inclination may be to shut down the factory, Coase points out that perhaps it would be better for the factory to continue to pollute if it also compensates the fishermen. For Coase, it was important to compare the costs with the benefits.\nThis article became one of the most cited articles in economics, and has been particularly influential in law. For instance, Michael Kirby describes a case where a decision would traditionally have been made by reference to cases ‘…decided long ago and in another country’. In contrast, the opinion of the NSW Court of Appeal, written by Kirby, referenced the costs and benefits of various options.\nIn 1964 Coase moved to the University of Chicago, alongside economists such as Gary Becker, Milton Friedman, and George Stigler. Coase’s influence was recognised in 1991 when he was awarded the Nobel Prize in Economics (now in 2014, formally the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel). He remained active at an advanced age, publishing a book about the rise of China when he was 101.\nCoase was an economist in the style of Adam Smith and John Maynard Keynes. He used mathematics sparingly in publications, and he ensured that his analysis was grounded in reality. Unfortunately these traditions are now largely lost to the profession. How many undergraduates take a year to study how businesses actually work, as Coase did?\nIn 2012 he published an article bemoaning how isolated economics was from ‘the ordinary business of life’. He believed that the changed nature of economics had undermined the usefulness of the subject and argued the need for it to be ‘firmly grounded in systematic empirical investigation of the working of the economy’.\nCoase’s work reminds us that economics does not have to be about manipulating abstractions on a whiteboard. His passing was a loss, not only to his friends and family, but also to the entire economics profession.\n\n\n",
    "preview": "posts/2014-12-24-in-appreciation-of-ronald-coase/images/IMG_0145.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-24-i-ll-have-what-they-re-having/",
    "title": "I'll Have What They're Having",
    "description": "Some accuse the Rich White Males from San Francisco's Bay Area of only making products for other Rich White Males. But that neglects the fact that what they want is sometimes also what the rest of us want.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-12-17",
    "categories": [],
    "contents": "\nSome accuse the Rich White Males from San Francisco’s Bay Area of only making products for other Rich White Males. But that neglects the fact that what they want is sometimes also what the rest of us want.\n\n\n\nFigure 1: If you’re rich even the San Francisco Bay can be your playground.\n\n\n\nFor instance, in the case of Facebook, Rich White Males wanted an easy way to communicate with their friends and family. Facebook is a success because others want this too. In the case of Uber, a taxi alternative, Rich White Males wanted frictionless transport. It turns out that everyone else (other than taxi medallion owners) wants this too.\nBloomThat, a Bay Area flower delivery business, is on the face of it, one of the newest solutions to Rich White Male problems.\nAlthough I’m neither rich nor white, being a twenty-eight year old male, I’ve been using flower delivery services for about a decade, and it’s often an awful experience. Their websites are too complicated, and have too many garish options. Their prices are too high. And delivery is ‘next day’, when I usually want flowers delivered right away.\nBloomThat claims to solve these problems. Each day they offer a small number of seasonal options that look as though Apple’s Jony Ive designed them. The small selection not only keeps it simple, but also reduces wastage. And they promise delivery within 90 minutes.\nCritics would say this is perhaps the definitive Rich White Male product. And although, like most Bay Area tech businesses, BloomThat’s founding team and initial investors (which includes Ashton Kutcher – lucky Mila Kunis!) are all male, I’m not sure that should be damning in this case.\nFor one thing, according to the founders, BloomThat’s numbers indicate the majority of their customers are female.\nBut more importantly, as a customer, the thrilling aspect of BloomThat isn’t the flowers, it’s how easy the business makes it for you to delight another person.\nTo my mind, BloomThat has attracted millions of dollars of funding for one reason. Investors think that if they can do this with flowers, then they can do it with other products. BloomThat hasn’t yet expanded beyond the Bay Area (although they’ll soon be in LA) but they’re already trying partnerships with complimentary products, such as donuts and SoulCycle.\nThese days many products arrive within a couple of days of being ordered. That used to be delightful, but the Rich White Males are tired of it. They want their products immediately. And they’d better enjoy the ordering process as well. By investing in businesses like BloomThat, they’re betting that applies to you too.\n\n\n",
    "preview": "posts/2014-12-24-i-ll-have-what-they-re-having/images/DSCF3336.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-19-the-human-equation-an-interview-with-george-zachary/",
    "title": "The Human Equation: An Interview With George Zachary",
    "description": "In an age when you can buy data-driven refrigerators and Moneyball is nominated for Academy Awards you may be surprised to hear there are investors who describe themselves as gut-driven. Frankly I thought such people would be too embarrassed to be out in public. Then I heard George Zachary talk.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-03-19",
    "categories": [],
    "contents": "\n\n“Two bicyclists start twenty miles apart and head toward each other, each going at 10 mph. At the same time a fly that travels at 15 mph starts from the front wheel of the southbound bicycle and flies to the front wheel of the northbound one, then turns around and flies to the front wheel of the southbound one again, and continues in this manner till he is crushed between the two front wheels. What total distance did the fly cover?” One way to find the answer is to calculate the distance the fly covers on the first leg of the trip, then on the second, then on the third, etc., and, finally, to sum the infinite series. The quick way is to observe that the bicycles meet exactly one hour after their start, so that the fly had just an hour for his travels; the answer must therefore be 15 miles. When the question was put to John von Neumann, he solved it in an instant, and thereby disappointed the questioner: “Oh, you must have heard the trick before!” “What trick?” asked von Neumann, “All I did was sum the infinite series.”\n\n\nPaul Halmos on John von Neumann\n\nIn an age when you can buy data-driven refrigerators and Moneyball is nominated for Academy Awards you may be surprised to hear there are investors who describe themselves as gut-driven. Frankly I thought such people would be too embarrassed to be out in public. Then I heard George Zachary talk.\n\n\n\nFigure 1: Some people can just see the answer, the rest of us have to work at it.\n\n\n\nZachary is a self-described ‘gut-driven investor’. But I didn’t storm out of the talk muttering about negative-alpha because Zachary is a venture capitalist at Charles River Ventures (CRV). And he has an impressive record, mostly notably being the first institutional investor at Twitter and Yammer. Of course, this could be down to chance, but what are the chances of it happening twice? So I was curious to understand what made Zachary tick.\nZachary is a first-generation American, whose parents were born in Greece. He made his initial money in the early ‘90s as a founder of Shutterfly, which is now a public company. For the past 18 years he’s been an investor, and has invested over $160 million. He’s funded 27 businesses at a ’venture level’ (multiple millions of dollars) and focuses on early stage businesses. He’s also funded 100-150 businesses at a ‘seed level’ (hundreds of thousands to a few million dollars). This is from a pool of 35,000 businesses that he’s looked at.\nCRV is a collection of partners. A business that wants funding from CRV needs one of the partners to champion them. All businesses are put to a vote, and if the champion cannot get the other partners excited then it’s unlikely the business will be funded. This means that usually a successful product pitch will not only be clear and concise, but also work just as well when it’s not delivered by the founders. Zachary’s one exception was Twitter, ‘who had the worst presentation ever’.\nSuccessful businesses, Zachary believes, have ‘founders that are maniacal, bipolar and slightly crazy’. Their need to win permeates every aspect of their life and although this drives them, it also means they are difficult to work with. Zachary’s one exception was Twitter, ‘who had three co-founders, none of whom wanted to confront each other’.\nZachary believes that founders must have a clear vision of what they are trying to achieve. This means that the best pitches cause a binary reaction: love or hate. This is because if the audience shares the founder’s vision then they will love the product, but otherwise they will hate it. A product that causes reactions such as ‘oh cool’ or ‘nice’ is unlikely to get VC funding at CRV – they prefer a low probability of an enormous success over a high probability of moderate success. As you probably expect by now, Zachary’s exception was Twitter, where he said the founders basically didn’t understand why anyone would tweet for the first couple of years.\nVision is even more important than a clear path to monetization to Zachary. When he evaluates businesses he asks himself whether people love the product and whether the founders are able to develop the product to take advantage of this. It’s only then that he asks whether it seems that they can monetize. And a lack of an obvious plan to monetize isn’t a deal-breaker.\nThe implication of all this, which Zachary was very clear about, is that successful businesses don’t need VC funding. Equally, a failure to get VC funding doesn’t mean the business is necessarily a failure. Here capital is an accelerator, and a successful business will not be dependent on VC funding.\nThroughout the talk Zachary continually referred to the fact that he’s a gut-instinct investor. But given his talk I’m not so sure it’s true. For instance, during Q&A he was pitched to by a person whose business is Uber for motorcycles. So the guy pitches that, just that, no other information, with an accent so thick you can barely make out what he’s saying, and then asks Zachary whether he thinks it’ll be a billion dollar business! Straight away Zachary says ‘no’. I’m expecting it to be over, but then Zachary continues for five minutes explaining why he thinks that way, applying some of the simple rules that he’d discussed earlier and references to data that he knew off-hand, and that I later verified to be correct.\nZachary’s gut just seemed to be able to do the calculations that data-driven investors require computers to do. So is he really gut-instinct investor? By now it was getting late and Zachary needed to get home to put his kids to sleep. But he left us all with his email address, and a plea to get in contact, ‘but not all tonight, otherwise I won’t be able to respond to you all’. So he mustn’t be a computer after all.\nSan Francisco\n\n\n",
    "preview": "posts/2014-03-19-the-human-equation-an-interview-with-george-zachary/images/IMG_0277.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-28-final-shot-at-ending-industrial-disputes/",
    "title": "Final Shot At Ending Industrial Disputes",
    "description": "The industrial relations disputes that culminated in the shutdown of Qantas last year took more than nine months to make it through arbitration. Such lengthy delays have left many wondering if changes should be made to the processes that underpin this form of dispute resolution. The use of ‘final offer’ arbitration could be the small change that has a big effect, saving conflicted parties time, money and reputation.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Ben O'Neill",
        "url": {}
      }
    ],
    "date": "2012-11-28",
    "categories": [],
    "contents": "\nWritten with Ben O’Neill and originally published in the Canberra Times.\nThe industrial relations disputes that culminated in the shutdown of Qantas last year took more than nine months to make it through arbitration. Such lengthy delays have left many wondering if changes should be made to the processes that underpin this form of dispute resolution. The use of ‘final offer’ arbitration could be the small change that has a big effect, saving conflicted parties time, money and reputation.\n\n\n\nFigure 1: Qantas negotiates with a lot of other parties. Final offer arbitration may have helped reduce the length of the shutdown. Photo by Monica Alexander.\n\n\n\nArbitration is an alternative to a formal litigation. Unless the parties agree otherwise, arbitration is conducted as an adversarial process, but it has looser rules of evidence than standard litigation. An arbitrator, often an experienced lawyer or a highly regarded expert in the field of the dispute, guides the process and makes a binding determination if the parties fail to agree on a solution.\nDespite its benefits, the traditional arbitration process encourages par- ties to begin with an extreme position. This is because the parties know that during the process, they are likely to compromise from that position, and unlike in a formal litigation, it is unlikely they will suffer cost penalties for making spurious claims or arguments.\nUltimately the arbitrator is required to make a determination according to law, but finding the proper legal solution is made more difficult when the parties have an incentive to exaggerate their true position. As an analogy, imagine a real-estate agent who inflates the asking price of a house, knowing that people will try to talk the price down, and a buyer who feigns low interest in the house in order to bargain more effectively — both have an incentive to exaggerate their true position.\nFinal offer arbitration removes this perverse incentive problem. In FOA, the two parties submit their position on the proper legal outcome to the arbitrator and these positions are revealed simultaneously to both parties. A negotiation period ensues, and if the parties cannot agree to an outcome, the arbitrator makes a determination.\nHowever, the arbitrator must choose one of the submitted positions in its entirety — there is no alternative outcome allowed.\nAlthough this may seem restrictive, or even unfair, it creates strong in- centives for the parties to be reasonable in their submission to the arbitrator. The more extreme their submitted position (relative to the arbitrator’s view of the proper legal outcome), the less likely it is to be chosen. Moreover, if the parties are being reasonable in their submission, it is more likely that they will be able to find common ground in negotiation. Final offer ar- bitration encourages the parties to make a bona fide attempt to strive for impartiality in their own claim.\nNonetheless, FOA is not without its weaknesses. For instance, it assumes sufficient mastery of the law and the facts to prepare a ‘winning’ final offer. As such, it is best used in situations where the parties, their lawyers and their experts are experienced and understand the possible outcomes. This would usually be the case in commercial disputes and some labour disputes.\nThe ‘final offer’ restriction on the arbitrator’s decision means that, if a settlement is not reached, there will be a total acceptance of one party’s claim. This might lead some to worry about the arbitrator’s impartiality, though this is a consideration in any form of arbitration. In any case, this matter is already well provided for in law. For instance, arbitrators are under a legal obligation to be impartial and fair, and to resolve the dispute according to law. Additionally, parties can challenge the nomination of an arbitrator and apply to court to set aside an award on the basis of arbitrator misconduct.\nArbitration mechanisms in Australia are provided for under contract law and in various commercial arbitration acts. These could easily facilitate the use of FOA. Ultimately, the form of arbitration is best selected by the parties prior to a dispute, but it is not unthinkable that final offer arbitration could be useful during dispute resolution. It could save time and money, and could also foster more fruitful negotiation. If Qantas and their workers can achieve the miracle of human flight, efficient dispute resolution might also be possible.\n\n\n",
    "preview": "posts/2012-11-28-final-shot-at-ending-industrial-disputes/images/DSC3132.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-05-17-tournaments-could-drive-r-d-effort/",
    "title": "Tournaments Could Drive R&D Effort",
    "description": "The government should use tournaments to stimulate research and development in Australia. They have been largely overlooked since the Cutler Review of Innovation, but when structured properly, they encourage out-standing achievement and promote creative destruction.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Andrew Barnes",
        "url": {}
      }
    ],
    "date": "2011-05-17",
    "categories": [],
    "contents": "\nWritten with Andrew Barnes and originally published in the Australian Financial Review.\nThe government should use tournaments to stimulate research and development in Australia. They have been largely overlooked since the Cutler Review of Innovation, but when structured properly, they encourage out-standing achievement and promote creative destruction.\n\n\n\nFigure 1: Tournaments bring out the best of us in sport, why not also R&D? Photo by Monica Alexander.\n\n\n\nCreative destruction is a phrase made famous by the 20th-century economist Joseph Schumpeter. It describes the process whereby companies, seeking profit, innovate so successfully that their new product destroys the demand for existing products.\nIt is not hard to find examples, just consider how CDs replaced cassettes, and faxes have been made almost extinct by email. Although harsh, creative destruction is necessary for economic progress, modern society would be unimaginable without it.\nA lesson that economists are now learning is that environments in which innovative ideas happen can be deliberately created. Authors such as Cass Sunstein and Richard Thaler suggest governments are most likely to be successful when they point markets in a desired direction and let them find their way.\nOne way to do this, common overseas but underused in Australia, is the use of tournaments. All the government does is put up a prize for a specific achievement, and then wait to crown the winner. There is little scope for bias because the criteria must be both publicly available and clearly defined to allow firms to do the necessary research and development.\nTournaments are effective because companies tend to spend more on the required R&D than the prizemoney that is offered. They do this because the rewards of winning are not just the proffered prize money; companies also get prestige, attention, and press coverage, all of which means successful solutions flourish.\nWith US$10 million in prizemoney, and the thrill of competition to entice firms, the Ansari X Prize proved that it was possible for a private company to fly into space. Though the cost to develop a vehicle capable of winning was many times the prizemoney, the tournament was a triumph and a catalyst for rapid innovation.\nAustralia already has one well-known tournament, the World Solar Challenge, a solar car race from Darwin to Adelaide. There should be more. The level of innovation is great that in recent years a new section has been set-up with more onerous design restrictions.\nTo enjoy strong and sustainable economic growth Australia must be at the forefront of innovation. Our companies must be responsible for creative destruction. Tournaments would not only appeal to our competitive natures, but also be good for the economy.\n\n\n",
    "preview": "posts/2011-05-17-tournaments-could-drive-r-d-effort/images/DSCF4403.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2009-05-25-plastic-policies/",
    "title": "Plastic Policies",
    "description": "There is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Flavio Menezes",
        "url": {}
      }
    ],
    "date": "2009-05-25",
    "categories": [],
    "contents": "\nWritten with Professor Flavio Menezes and originally published at Australian Policy Online.\nThere is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument.\nIn South Australia, one of the reasons for a ban rather than a tax was the belief that a tax would impose an additional cost on households. The implied corollary is that a ban on plastic bags would not impose these costs, and thus would presumably be paid for entirely by retailers. This claim is incorrect: not only does a ban on plastic bags impose costs on households, but they are greater than those imposed by a tax.\nTo understand the impact of a tax on plastic bag consumption it is necessary to distinguish between the initial and final ‘incidence’ of the tax — in other words, between who is legally responsible for paying a tax, and whom the burden of paying the tax would be passed on to. It is the final incidence that matters from a policy perspective. Economic theory suggests that except under very special circumstances, final and initial incidences do not coincide. Indeed, evidence from Germany, Ireland and Switzerland suggest that plastic bag consumption decreases as a result of a tax; and that it is both retailers and households who bear the burden of paying the tax, regardless of the initial incidence.\nUnder a ban on plastic bags, retailers must switch to other, more ex- pensive, options. Again the distinction between initial and final incidence is important. Although retailers would be legally responsible for providing the more expensive bags required to replace plastic bags, at least some of this additional cost would be passed onto households through higher prices or an explicit charge. Thus, regardless of whether a tax or a ban is used, households bear an additional cost.\nSo is a tax or a ban the more appropriate instrument? A ban reduces the number of plastic bags to zero, whatever the cost. Bans on smoking in workplaces, for example, are considered desirable because the cost of even a small amount of smoking in the workplace is believed to be unacceptable. For this reason, society is prepared to pay any price to ensure a non-smoking work environment. Is the same true for plastic bags?\nIn contrast, under a tax the reduction in the number of plastic bags is uncertain, but the additional cost imposed on households is known: it is the size of the tax. A tax more explicitly considers the cost of reduction. It allows households a choice that takes into account the cost and the ben- efit and results in plastic bag usage in cases where the benefit of doing so outweighs the cost.\nWhen the benefit of each additional unit of plastic bag reduction is small compared to the cost a tax is the better option; for should this cost be higher than anticipated, a tax rather than a ban would cost society less. And given that a ban supposes the cost of abatement to be entirely outweighed by the benefit for every level of abatement it is, in effect, not possible that a government advocating a ban has underestimated it.\nAlthough a number of factors must be considered when deciding between a tax or a ban, a ban on plastic bags is unlikely to be the best option for minimising the financial impact on households.\n\n\n",
    "preview": {},
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2008-05-08-home-buyers-will-be-hurt-by-aid-plan/",
    "title": "Home Buyers Will Be Hurt by Aid Plan",
    "description": "The Government's First Home Saver Accounts policy, announced during the last federal election, is a promise that should never have been made. It will place upward pressure upon inflation and increase the potential for domestic financial turbulence. It will be the individuals that the policy is designed to help (young Australians with a still-insecure financial foundation) that will be most hurt in the long-run.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2008-05-08",
    "categories": [],
    "contents": "\nOriginally published in the Australian Financial Review.\nThe Government’s First Home Saver Accounts policy, announced during the last federal election, is a promise that should never have been made. It will place upward pressure upon inflation and increase the potential for domestic financial turbulence. It will be the individuals that the policy is designed to help (young Australians with a still-insecure financial foundation) that will be most hurt in the long-run.\n\n\n\nFigure 1: A view of the Harbour apparently adds c.$100,000 to the price of an apartment in Sydney.\n\n\n\nThere are four broad areas of concern. Firstly, although the policy appears economically sound, closer inspection shows that it amounts to government encouragement of irresponsible mortgage practices (remember that irresponsible mortgage practices contributed to the sub-prime meltdown in the US). Specifically, it provides a tax concession to allow individuals to qualify for a loan but does not provide similar help to make repayments. The effect will be similar to that of the substantial decrease in the US interest rate during the early 2000s – families will be able to take out home loans which they could not ordinarily afford. These are the American families experiencing the greatest hardship from the sub-prime meltdown and similarly, it will be the Australian families taking advantage of this policy that will suffer when forced to service debt larger than they can reasonably repay.\nSecondly, even if the Australians helped by this policy are able to afford the repayments given the current domestic economic position it is irresponsible to expect these conditions to continue indefinitely. The inability of individuals to meet repayments under changed conditions would lead to significant foreclosure activity – an outcome not desired by any Australian.\nThirdly, the policy increases the exposure of young Australians to inter- est rates (and thereby international economic conditions). Increased inter- national money market links are not beneficial to most Australians in the short-term given current international money market conditions and especially not the group targeted by the policy.\nFinally, as the policy is available to all first home buyers it does not increase the relative ability of any targeted individual to purchase a house from the normal first home market i.e. when bidding against other first home buyers – a zero-sum game. However, the increase in house prices to be expected as a result of the game engenders domestic inflation pressure.\nGiven domestic economic conditions, responsible policy ought to encourage saving. In order to avoid the above concerns, such savings must not be tied to the residential property market and thus policy needs to encourage broader economy-wide saving. Its nature would be such that some of the increased savings would be invested in projects increasing productive capacity – important given current capacity constraints.\nThe First Home Saver Accounts policy amounts to encouragement of irresponsible mortgage practices and even if it does increase the ability of young Australians to purchase their first home (which it will not) it is likely that they would be placed under substantial financial pressure during a domestic slowdown. The economic conditions we currently enjoy represent a rare opportunity to increase the long term savings rate – a change which will fight against inflation and policy must reflect this.\n\n\n",
    "preview": "posts/2008-05-08-home-buyers-will-be-hurt-by-aid-plan/images/DSCF0213.jpg",
    "last_modified": "2020-12-19T13:27:53-05:00",
    "input_file": {}
  }
]
